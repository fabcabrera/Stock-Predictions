{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515730c1-45ed-49e9-86b8-dbd07f2dbd0e",
   "metadata": {},
   "source": [
    "# CNN Model using full Ford data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "23263a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, InputLayer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, model_selection\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb74ca8-4ea0-46a4-afa0-b61173feef12",
   "metadata": {},
   "source": [
    "## Part 1: Clean and Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3e760be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9fb5de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ford</th>\n",
       "      <th>F-150</th>\n",
       "      <th>Ford Bronco_x</th>\n",
       "      <th>Ford Mustang_x</th>\n",
       "      <th>Ford Stock</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Ford Motor Company</th>\n",
       "      <th>Ford Mustang_y</th>\n",
       "      <th>Ford F Series</th>\n",
       "      <th>Ford Bronco_y</th>\n",
       "      <th>Lincoln Navigator</th>\n",
       "      <th>Lincoln Aviator</th>\n",
       "      <th>Ford GT</th>\n",
       "      <th>dow_open</th>\n",
       "      <th>dow_high</th>\n",
       "      <th>dow_low</th>\n",
       "      <th>dow_close</th>\n",
       "      <th>dow_vol</th>\n",
       "      <th>nas_open</th>\n",
       "      <th>nas_high</th>\n",
       "      <th>nas_low</th>\n",
       "      <th>nas_close</th>\n",
       "      <th>nas_vol</th>\n",
       "      <th>Wiki_total</th>\n",
       "      <th>Google_total</th>\n",
       "      <th>Stock_total</th>\n",
       "      <th>Nas_total</th>\n",
       "      <th>Dow_total</th>\n",
       "      <th>Wiki_Moment_1</th>\n",
       "      <th>Wiki_Moment_2</th>\n",
       "      <th>Wiki_Moment_1_s</th>\n",
       "      <th>Wiki_Moment_2_s</th>\n",
       "      <th>Wiki_MAvg</th>\n",
       "      <th>Wiki_MAvg_s</th>\n",
       "      <th>Wiki_Disparity</th>\n",
       "      <th>Wiki_Disparity_s</th>\n",
       "      <th>Wiki_ROC</th>\n",
       "      <th>Wiki_ROC_s</th>\n",
       "      <th>Wiki_Rocp</th>\n",
       "      <th>Wiki_EMA</th>\n",
       "      <th>Wiki_diff</th>\n",
       "      <th>Wiki_gain</th>\n",
       "      <th>Wiki_loss</th>\n",
       "      <th>Wiki_avg_gain</th>\n",
       "      <th>Wiki_avg_loss</th>\n",
       "      <th>Wiki_rs</th>\n",
       "      <th>Wiki_RSI</th>\n",
       "      <th>Wiki_Move</th>\n",
       "      <th>Wiki_MAvg_Move</th>\n",
       "      <th>Wiki_MAvg_s_Move</th>\n",
       "      <th>Wiki_EMA_Move</th>\n",
       "      <th>Wiki_Disparity_Move</th>\n",
       "      <th>Wiki_Disparity_s_Move</th>\n",
       "      <th>Wiki_RSI_Move</th>\n",
       "      <th>Google_Moment_1</th>\n",
       "      <th>Google_Moment_2</th>\n",
       "      <th>Google_Moment_1_s</th>\n",
       "      <th>Google_Moment_2_s</th>\n",
       "      <th>Google_MAvg</th>\n",
       "      <th>Google_MAvg_s</th>\n",
       "      <th>Google_Disparity</th>\n",
       "      <th>Google_Disparity_s</th>\n",
       "      <th>Google_ROC</th>\n",
       "      <th>Google_ROC_s</th>\n",
       "      <th>Google_Rocp</th>\n",
       "      <th>Google_EMA</th>\n",
       "      <th>Google_diff</th>\n",
       "      <th>Google_gain</th>\n",
       "      <th>Google_loss</th>\n",
       "      <th>Google_avg_gain</th>\n",
       "      <th>Google_avg_loss</th>\n",
       "      <th>Google_rs</th>\n",
       "      <th>Google_RSI</th>\n",
       "      <th>Google_Move</th>\n",
       "      <th>Google_MAvg_Move</th>\n",
       "      <th>Google_MAvg_s_Move</th>\n",
       "      <th>Google_EMA_Move</th>\n",
       "      <th>Google_Disparity_Move</th>\n",
       "      <th>Google_Disparity_s_Move</th>\n",
       "      <th>Google_RSI_Move</th>\n",
       "      <th>Stock_Moment_1</th>\n",
       "      <th>Stock_Moment_2</th>\n",
       "      <th>Stock_Moment_1_s</th>\n",
       "      <th>Stock_Moment_2_s</th>\n",
       "      <th>Stock_MAvg</th>\n",
       "      <th>Stock_MAvg_s</th>\n",
       "      <th>Stock_Disparity</th>\n",
       "      <th>Stock_Disparity_s</th>\n",
       "      <th>Stock_ROC</th>\n",
       "      <th>Stock_ROC_s</th>\n",
       "      <th>Stock_Rocp</th>\n",
       "      <th>Stock_EMA</th>\n",
       "      <th>Stock_diff</th>\n",
       "      <th>Stock_gain</th>\n",
       "      <th>Stock_loss</th>\n",
       "      <th>Stock_avg_gain</th>\n",
       "      <th>Stock_avg_loss</th>\n",
       "      <th>Stock_rs</th>\n",
       "      <th>Stock_RSI</th>\n",
       "      <th>Stock_Move</th>\n",
       "      <th>Stock_MAvg_Move</th>\n",
       "      <th>Stock_MAvg_s_Move</th>\n",
       "      <th>Stock_EMA_Move</th>\n",
       "      <th>Stock_Disparity_Move</th>\n",
       "      <th>Stock_Disparity_s_Move</th>\n",
       "      <th>Stock_RSI_Move</th>\n",
       "      <th>Nas_Moment_1</th>\n",
       "      <th>Nas_Moment_2</th>\n",
       "      <th>Nas_Moment_1_s</th>\n",
       "      <th>Nas_Moment_2_s</th>\n",
       "      <th>Nas_MAvg</th>\n",
       "      <th>Nas_MAvg_s</th>\n",
       "      <th>Nas_Disparity</th>\n",
       "      <th>Nas_Disparity_s</th>\n",
       "      <th>Nas_ROC</th>\n",
       "      <th>Nas_ROC_s</th>\n",
       "      <th>Nas_Rocp</th>\n",
       "      <th>Nas_EMA</th>\n",
       "      <th>Nas_diff</th>\n",
       "      <th>Nas_gain</th>\n",
       "      <th>Nas_loss</th>\n",
       "      <th>Nas_avg_gain</th>\n",
       "      <th>Nas_avg_loss</th>\n",
       "      <th>Nas_rs</th>\n",
       "      <th>Nas_RSI</th>\n",
       "      <th>Nas_Move</th>\n",
       "      <th>Nas_MAvg_Move</th>\n",
       "      <th>Nas_MAvg_s_Move</th>\n",
       "      <th>Nas_EMA_Move</th>\n",
       "      <th>Nas_Disparity_Move</th>\n",
       "      <th>Nas_Disparity_s_Move</th>\n",
       "      <th>Nas_RSI_Move</th>\n",
       "      <th>Dow_Moment_1</th>\n",
       "      <th>Dow_Moment_2</th>\n",
       "      <th>Dow_Moment_1_s</th>\n",
       "      <th>Dow_Moment_2_s</th>\n",
       "      <th>Dow_MAvg</th>\n",
       "      <th>Dow_MAvg_s</th>\n",
       "      <th>Dow_Disparity</th>\n",
       "      <th>Dow_Disparity_s</th>\n",
       "      <th>Dow_ROC</th>\n",
       "      <th>Dow_ROC_s</th>\n",
       "      <th>Dow_Rocp</th>\n",
       "      <th>Dow_EMA</th>\n",
       "      <th>Dow_diff</th>\n",
       "      <th>Dow_gain</th>\n",
       "      <th>Dow_loss</th>\n",
       "      <th>Dow_avg_gain</th>\n",
       "      <th>Dow_avg_loss</th>\n",
       "      <th>Dow_rs</th>\n",
       "      <th>Dow_RSI</th>\n",
       "      <th>Dow_Move</th>\n",
       "      <th>Dow_MAvg_Move</th>\n",
       "      <th>Dow_MAvg_s_Move</th>\n",
       "      <th>Dow_EMA_Move</th>\n",
       "      <th>Dow_Disparity_Move</th>\n",
       "      <th>Dow_Disparity_s_Move</th>\n",
       "      <th>Dow_RSI_Move</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ford  F-150  Ford Bronco_x  Ford Mustang_x  Ford Stock  Open  High  Low  \\\n",
       "0     0      0              0               0           0     6     6    6   \n",
       "\n",
       "   Close  Volume  Dividends  Stock Splits  Ford Motor Company  Ford Mustang_y  \\\n",
       "0      6       6          6             6                   0               0   \n",
       "\n",
       "   Ford F Series  Ford Bronco_y  Lincoln Navigator  Lincoln Aviator  Ford GT  \\\n",
       "0              0              0                  0                0        0   \n",
       "\n",
       "   dow_open  dow_high  dow_low  dow_close  dow_vol  nas_open  nas_high  \\\n",
       "0         4         4        4          4        4         4         4   \n",
       "\n",
       "   nas_low  nas_close  nas_vol  Wiki_total  Google_total  Stock_total  \\\n",
       "0        4          4        4           0             0            6   \n",
       "\n",
       "   Nas_total  Dow_total  Wiki_Moment_1  Wiki_Moment_2  Wiki_Moment_1_s  \\\n",
       "0          4          4              0              0                0   \n",
       "\n",
       "   Wiki_Moment_2_s  Wiki_MAvg  Wiki_MAvg_s  Wiki_Disparity  Wiki_Disparity_s  \\\n",
       "0                0          0            0               0                 0   \n",
       "\n",
       "   Wiki_ROC  Wiki_ROC_s  Wiki_Rocp  Wiki_EMA  Wiki_diff  Wiki_gain  Wiki_loss  \\\n",
       "0         0           0          0         0          0          0          0   \n",
       "\n",
       "   Wiki_avg_gain  Wiki_avg_loss  Wiki_rs  Wiki_RSI  Wiki_Move  Wiki_MAvg_Move  \\\n",
       "0              0              0        0         0          0               0   \n",
       "\n",
       "   Wiki_MAvg_s_Move  Wiki_EMA_Move  Wiki_Disparity_Move  \\\n",
       "0                 0              0                    0   \n",
       "\n",
       "   Wiki_Disparity_s_Move  Wiki_RSI_Move  Google_Moment_1  Google_Moment_2  \\\n",
       "0                      0              0                0                0   \n",
       "\n",
       "   Google_Moment_1_s  Google_Moment_2_s  Google_MAvg  Google_MAvg_s  \\\n",
       "0                  0                  0            0              0   \n",
       "\n",
       "   Google_Disparity  Google_Disparity_s  Google_ROC  Google_ROC_s  \\\n",
       "0                 0                   0           0             0   \n",
       "\n",
       "   Google_Rocp  Google_EMA  Google_diff  Google_gain  Google_loss  \\\n",
       "0            0           0            0            0            0   \n",
       "\n",
       "   Google_avg_gain  Google_avg_loss  Google_rs  Google_RSI  Google_Move  \\\n",
       "0                0                0          0           0            0   \n",
       "\n",
       "   Google_MAvg_Move  Google_MAvg_s_Move  Google_EMA_Move  \\\n",
       "0                 0                   0                0   \n",
       "\n",
       "   Google_Disparity_Move  Google_Disparity_s_Move  Google_RSI_Move  \\\n",
       "0                      0                        0                0   \n",
       "\n",
       "   Stock_Moment_1  Stock_Moment_2  Stock_Moment_1_s  Stock_Moment_2_s  \\\n",
       "0              11              11                11                11   \n",
       "\n",
       "   Stock_MAvg  Stock_MAvg_s  Stock_Disparity  Stock_Disparity_s  Stock_ROC  \\\n",
       "0           0             0                6                  6         11   \n",
       "\n",
       "   Stock_ROC_s  Stock_Rocp  Stock_EMA  Stock_diff  Stock_gain  Stock_loss  \\\n",
       "0           11          11          6          11          11          11   \n",
       "\n",
       "   Stock_avg_gain  Stock_avg_loss  Stock_rs  Stock_RSI  Stock_Move  \\\n",
       "0              76              76        76         76           0   \n",
       "\n",
       "   Stock_MAvg_Move  Stock_MAvg_s_Move  Stock_EMA_Move  Stock_Disparity_Move  \\\n",
       "0                0                  0               0                     0   \n",
       "\n",
       "   Stock_Disparity_s_Move  Stock_RSI_Move  Nas_Moment_1  Nas_Moment_2  \\\n",
       "0                       0               0             7             7   \n",
       "\n",
       "   Nas_Moment_1_s  Nas_Moment_2_s  Nas_MAvg  Nas_MAvg_s  Nas_Disparity  \\\n",
       "0               7               7         0           0              4   \n",
       "\n",
       "   Nas_Disparity_s  Nas_ROC  Nas_ROC_s  Nas_Rocp  Nas_EMA  Nas_diff  Nas_gain  \\\n",
       "0                4        7          7         7        4         7         7   \n",
       "\n",
       "   Nas_loss  Nas_avg_gain  Nas_avg_loss  Nas_rs  Nas_RSI  Nas_Move  \\\n",
       "0         7            46            46      46       46         0   \n",
       "\n",
       "   Nas_MAvg_Move  Nas_MAvg_s_Move  Nas_EMA_Move  Nas_Disparity_Move  \\\n",
       "0              0                0             0                   0   \n",
       "\n",
       "   Nas_Disparity_s_Move  Nas_RSI_Move  Dow_Moment_1  Dow_Moment_2  \\\n",
       "0                     0             0             7             7   \n",
       "\n",
       "   Dow_Moment_1_s  Dow_Moment_2_s  Dow_MAvg  Dow_MAvg_s  Dow_Disparity  \\\n",
       "0               7               7         0           0              4   \n",
       "\n",
       "   Dow_Disparity_s  Dow_ROC  Dow_ROC_s  Dow_Rocp  Dow_EMA  Dow_diff  Dow_gain  \\\n",
       "0                4        7          7         7        4         7         7   \n",
       "\n",
       "   Dow_loss  Dow_avg_gain  Dow_avg_loss  Dow_rs  Dow_RSI  Dow_Move  \\\n",
       "0         7            46            46      46       46         0   \n",
       "\n",
       "   Dow_MAvg_Move  Dow_MAvg_s_Move  Dow_EMA_Move  Dow_Disparity_Move  \\\n",
       "0              0                0             0                   0   \n",
       "\n",
       "   Dow_Disparity_s_Move  Dow_RSI_Move  target_1  target_2  target_3  target_4  \\\n",
       "0                     0             0         0         0         0         0   \n",
       "\n",
       "   target_5  \n",
       "0         0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ford = pd.read_csv(\"Ford_Cleaned_Date.csv\")\n",
    "Ford.date = pd.to_datetime(Ford.date)\n",
    "Ford = Ford.set_index(\"date\")\n",
    "Ford = Ford.iloc[14:, :] # to remove first 14 days that include NaNs due to some calculations\n",
    "pd.DataFrame(Ford.isna().sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8736c8",
   "metadata": {},
   "source": [
    "We see from above that some varaibles contain a lot of NaN's so they either might not be useful, or they're going to lead us into eliminating a lot of data points.\n",
    "\n",
    "To Solve NaN problem we will create two initial data sets, one removing high NaN values, one not, and see which produced better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "997c5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ford = Ford.drop([['Ford', 'Ford_Bronco_x', 'Ford_Stock', 'F-150', 'Ford_Bronco_y', 'Ford Motor Company', 'Ford F Series', 'Lincoln Navigator', 'Lincoln Aviator']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "74c58691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((709, 169), (768, 134))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High NaN varaibles included will be Ford_0\n",
    "Ford_0 = Ford.dropna()\n",
    "Ford_0 = Ford_0[~(Ford_0.isin([np.inf, -np.inf]).any(axis=1))] # to remove inf\n",
    "\n",
    "# Ford_1 will remove the high NaN columns\n",
    "Ford_1 = Ford[Ford.columns.drop(list(Ford.filter(regex='gain')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='loss')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='RSI')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='_rs')))]\n",
    "Ford_1 = Ford_1.dropna()\n",
    "Ford_1 = Ford_1[~(Ford_1.isin([np.inf, -np.inf]).any(axis=1))]\n",
    "\n",
    "Ford_0.shape,Ford_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8a68ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Chosen is target_3\n",
    "Ford_0 = Ford_0.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "Ford_1 = Ford_1.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "\n",
    "target_3_0 = Ford_0[\"target_3\"]\n",
    "target_3_1 = Ford_0[\"target_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "262d44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training sets \n",
    "column_indices = {name: i for i, name in enumerate(Ford_1.columns)}\n",
    "\n",
    "n = len(Ford_0)\n",
    "train_f0 = Ford_0[0:int(n*0.7)]\n",
    "val_f0 = Ford_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0 = Ford_0[int(n*0.9):]\n",
    "\n",
    "train_f0t = target_3_0[0:int(n*0.7)]\n",
    "val_f0t = target_3_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0t = target_3_0[int(n*0.9):]\n",
    "\n",
    "# now with Ford_1\n",
    "n = len(Ford_1)\n",
    "train_f1 = Ford_1[0:int(n*0.7)]\n",
    "val_f1 = Ford_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1 = Ford_1[int(n*0.9):]\n",
    "\n",
    "train_f1t = target_3_1[0:int(n*0.7)]\n",
    "val_f1t = target_3_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1t = target_3_1[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e5e31954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preporocessing and standardizing the data\n",
    "Mscaler = MinMaxScaler() # keeps binarys at zero and 1 :)\n",
    "\n",
    "train_f0 = pd.DataFrame(Mscaler.fit_transform(train_f0), columns = Ford_0.columns)\n",
    "val_f0 = pd.DataFrame(Mscaler.fit_transform(val_f0), columns = Ford_0.columns)\n",
    "test_f0 = pd.DataFrame(Mscaler.fit_transform(test_f0), columns = Ford_0.columns)\n",
    "\n",
    "train_f1 = pd.DataFrame(Mscaler.fit_transform(train_f1), columns = Ford_1.columns)\n",
    "val_f1 = pd.DataFrame(Mscaler.fit_transform(val_f1), columns = Ford_1.columns)\n",
    "test_f1 = pd.DataFrame(Mscaler.fit_transform(test_f1), columns = Ford_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2159c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_y2(df, target, window_size=5):\n",
    "  df_as_np = df.to_numpy() # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df_as_np[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "    label = target[i+window_size][0:] # pulls the target variable after the window, target varible needs to be column zero in this \n",
    "    y.append(label) # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b42ae",
   "metadata": {},
   "source": [
    "### Switching Focus to Just Ford_0 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "eedd7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Features         Score\n",
      "164                target_3  1.853982e+17\n",
      "57      Wiki_Disparity_Move  5.048879e+00\n",
      "155                  Dow_rs  3.490399e+00\n",
      "58    Wiki_Disparity_s_Move  3.014178e+00\n",
      "3            Ford Mustang_x  2.128389e+00\n",
      "53                Wiki_Move  2.075053e+00\n",
      "52                 Wiki_RSI  2.059093e+00\n",
      "154            Dow_avg_loss  2.040229e+00\n",
      "75          Google_avg_gain  1.990507e+00\n",
      "41         Wiki_Disparity_s  1.941543e+00\n",
      "128            Nas_avg_loss  1.866438e+00\n",
      "87           Stock_Moment_2  1.839803e+00\n",
      "110  Stock_Disparity_s_Move  1.623449e+00\n",
      "4                Ford Stock  1.510923e+00\n",
      "136    Nas_Disparity_s_Move  1.426169e+00\n",
      "148                Dow_Rocp  1.350381e+00\n",
      "82          Google_EMA_Move  1.347818e+00\n",
      "153            Dow_avg_gain  1.339265e+00\n",
      "76          Google_avg_loss  1.188484e+00\n",
      "10                Dividends  1.159343e+00\n",
      "15            Ford Bronco_y  1.009442e+00\n",
      "70              Google_Rocp  9.298293e-01\n",
      "104               Stock_RSI  9.298293e-01\n",
      "13           Ford Mustang_y  9.202174e-01\n",
      "18                  Ford GT  8.989922e-01\n",
      "94                Stock_ROC  8.890224e-01\n",
      "86           Stock_Moment_1  8.890224e-01\n",
      "12       Ford Motor Company  8.557236e-01\n",
      "119         Nas_Disparity_s  8.274363e-01\n",
      "118           Nas_Disparity  8.230253e-01\n",
      "85          Google_RSI_Move  7.997378e-01\n",
      "35            Wiki_Moment_2  7.776846e-01\n",
      "162    Dow_Disparity_s_Move  7.768721e-01\n",
      "131                Nas_Move  7.590650e-01\n",
      "108          Stock_EMA_Move  7.547244e-01\n",
      "47                Wiki_gain  7.470438e-01\n",
      "54           Wiki_MAvg_Move  7.190388e-01\n",
      "46                Wiki_diff  6.987742e-01\n",
      "89         Stock_Moment_2_s  6.953927e-01\n",
      "129                  Nas_rs  6.324531e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 40 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=40)\n",
    "best_fit = bestfeatures.fit(train_f0, train_f0t)\n",
    "best_scores = pd.DataFrame(best_fit.scores_)\n",
    "best_columns = pd.DataFrame(Ford_0.columns)\n",
    "\n",
    "# concatenate the dataframes for better visualization\n",
    "features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "print(features_score.nlargest(40, 'Score'))  # print the top 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "683d2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(features_score.nlargest(40, 'Score')['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "048d7967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 15.68433691,  24.74667373,  33.10378249,  40.54965109,\n",
       "        47.39486325,  52.10029515,  55.50687153,  58.50470073,\n",
       "        61.42530001,  64.16698358,  66.53422149,  68.88994394,\n",
       "        71.02367211,  73.08023374,  75.02227321,  76.60158477,\n",
       "        78.0620214 ,  79.39525157,  80.67360492,  81.84725939,\n",
       "        83.00384916,  84.08061599,  85.07367374,  86.01668656,\n",
       "        86.88167021,  87.66126402,  88.38898007,  89.07250363,\n",
       "        89.72934819,  90.35721032,  90.96092432,  91.51678808,\n",
       "        92.05310805,  92.55542469,  93.03539412,  93.48671903,\n",
       "        93.90272285,  94.30439446,  94.68386345,  95.03240113,\n",
       "        95.3773965 ,  95.69977774,  96.00449478,  96.27854221,\n",
       "        96.53127052,  96.77177334,  96.99681187,  97.20554906,\n",
       "        97.39294777,  97.57709392,  97.74530298,  97.90443261,\n",
       "        98.05346752,  98.18960852,  98.31107071,  98.42348617,\n",
       "        98.52366819,  98.62112629,  98.70982943,  98.7923154 ,\n",
       "        98.86712008,  98.9372757 ,  99.00342101,  99.06554473,\n",
       "        99.12432462,  99.17917825,  99.23162819,  99.28083157,\n",
       "        99.32823752,  99.37265359,  99.41454389,  99.45282069,\n",
       "        99.48730254,  99.51861474,  99.5471169 ,  99.57270153,\n",
       "        99.59784448,  99.62100526,  99.64350963,  99.66522011,\n",
       "        99.6856891 ,  99.70476123,  99.72290744,  99.74025873,\n",
       "        99.75691696,  99.77298203,  99.78846039,  99.80342555,\n",
       "        99.81769669,  99.83084151,  99.84374413,  99.85580942,\n",
       "        99.86764828,  99.87914171,  99.89013066,  99.90069459,\n",
       "        99.91091898,  99.92031976,  99.92834919,  99.93602977,\n",
       "        99.94327895,  99.94966882,  99.9551483 ,  99.96013141,\n",
       "        99.96492254,  99.96896246,  99.97267246,  99.97624287,\n",
       "        99.97936168,  99.98222935,  99.9848438 ,  99.98683568,\n",
       "        99.98857062,  99.99008194,  99.9915254 ,  99.99276596,\n",
       "        99.99397484,  99.99503114,  99.99569156,  99.99629903,\n",
       "        99.99677637,  99.99724637,  99.99763481,  99.99800224,\n",
       "        99.99833522,  99.99865038,  99.998931  ,  99.99913134,\n",
       "        99.99928818,  99.99942647,  99.99954131,  99.99963876,\n",
       "        99.99971003,  99.99977048,  99.99982408,  99.99986711,\n",
       "        99.99990905,  99.99993972,  99.99995556,  99.99997032,\n",
       "        99.99998449,  99.99999052,  99.99999561,  99.99999892,\n",
       "        99.99999999, 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        ])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA().fit(train_f0)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "# reach 85% variance explained with 23 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e186f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 18.66241111,  35.84253923,  45.30504038,  52.922487  ,\n",
       "        60.4248241 ,  67.50426855,  74.21093435,  80.54626761,\n",
       "        84.05727658,  86.62596149,  88.99254744,  91.1645831 ,\n",
       "        92.92216156,  94.55803141,  95.52117579,  96.38660887,\n",
       "        97.08481547,  97.60281808,  98.08808651,  98.32139431,\n",
       "        98.54934806,  98.75599665,  98.94597887,  99.12335478,\n",
       "        99.26581491,  99.39748682,  99.50860948,  99.5814469 ,\n",
       "        99.64646238,  99.70346206,  99.75636888,  99.8053512 ,\n",
       "        99.85332717,  99.89527546,  99.92696851,  99.95199898,\n",
       "        99.97634481,  99.99802256, 100.        , 100.        ])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA with 40 best components\n",
    "pca_1 = PCA().fit(train_f0[feats])\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca_1.explained_variance_ratio_) * 100\n",
    "# can use 8 principle compents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b2cc0",
   "metadata": {},
   "source": [
    "Keeping score: Will test 5 models with Ford_0 data variations,\n",
    "- Model_0_0 = All varaibles,\n",
    "- Model_0_1 = PCA All varaibles\n",
    "- Model_0_2 = 40 best k score\n",
    "- Model_0_3 = PCA of 40 Best K Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f69f2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conveting the target to binary categorical for the keras models\n",
    "train_f0t = to_categorical(train_f0t)\n",
    "val_f0t = to_categorical(val_f0t)\n",
    "test_f0t = to_categorical(test_f0t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b9ef8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_0 final data prep\n",
    "\n",
    "# cononverting to window format, in this case 5 periods\n",
    "train_X_0_0, train_f0t_tc = df_to_X_y2(train_f0,train_f0t)\n",
    "val_X_0_0, val_f0t_tc= df_to_X_y2(val_f0, val_f0t)\n",
    "test_X_0_0, test_f0t_tc = df_to_X_y2(test_f0,test_f0t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9c108603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 491)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_f0t), len(train_X_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "602a57f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_0.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0375a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.6946 - accuracy: 0.5051 - val_loss: 0.6950 - val_accuracy: 0.5109\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.5214 - val_loss: 0.6952 - val_accuracy: 0.5328\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6912 - accuracy: 0.5193 - val_loss: 0.6939 - val_accuracy: 0.5182\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5214 - val_loss: 0.6935 - val_accuracy: 0.5182\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5193 - val_loss: 0.6949 - val_accuracy: 0.5036\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5214 - val_loss: 0.6944 - val_accuracy: 0.5182\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5356 - val_loss: 0.6947 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6849 - accuracy: 0.5784 - val_loss: 0.6959 - val_accuracy: 0.4526\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6822 - accuracy: 0.5682 - val_loss: 0.6958 - val_accuracy: 0.5036\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6810 - accuracy: 0.5682 - val_loss: 0.6989 - val_accuracy: 0.5255\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6779 - accuracy: 0.5886 - val_loss: 0.7000 - val_accuracy: 0.4964\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.5825 - val_loss: 0.7011 - val_accuracy: 0.4891\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6726 - accuracy: 0.6151 - val_loss: 0.7013 - val_accuracy: 0.4964\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.5866 - val_loss: 0.7061 - val_accuracy: 0.4964\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.6069 - val_loss: 0.7040 - val_accuracy: 0.4964\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.6273 - val_loss: 0.7033 - val_accuracy: 0.5036\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6548 - accuracy: 0.6436 - val_loss: 0.7106 - val_accuracy: 0.4818\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6533 - accuracy: 0.6375 - val_loss: 0.7090 - val_accuracy: 0.4599\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6510 - accuracy: 0.6171 - val_loss: 0.7089 - val_accuracy: 0.5182\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6368 - accuracy: 0.6680 - val_loss: 0.7180 - val_accuracy: 0.4672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14eaeb820>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_0.shape[1]\n",
    "n_features = train_X_0_0.shape[2]\n",
    "\n",
    "model_0_0_1 = Sequential()\n",
    "model_0_0_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_1.add(Flatten())\n",
    "model_0_0_1.add(Dense(50, activation='relu')) \n",
    "model_0_0_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_0_0_1.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_1.fit(train_X_0_0, train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2cc060af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.7108 - accuracy: 0.4929 - val_loss: 0.6984 - val_accuracy: 0.5328\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6938 - accuracy: 0.5193 - val_loss: 0.6939 - val_accuracy: 0.5401\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6863 - accuracy: 0.5703 - val_loss: 0.6967 - val_accuracy: 0.5036\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6830 - accuracy: 0.5601 - val_loss: 0.6943 - val_accuracy: 0.5401\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.5743 - val_loss: 0.6966 - val_accuracy: 0.5328\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6612 - accuracy: 0.6029 - val_loss: 0.7328 - val_accuracy: 0.4964\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6657 - accuracy: 0.5743 - val_loss: 0.6926 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6424 - accuracy: 0.6884 - val_loss: 0.7018 - val_accuracy: 0.5401\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6392 - accuracy: 0.6477 - val_loss: 0.6999 - val_accuracy: 0.5620\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6151 - accuracy: 0.7108 - val_loss: 0.7097 - val_accuracy: 0.4818\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6038 - accuracy: 0.6864 - val_loss: 0.7673 - val_accuracy: 0.4672\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5911 - accuracy: 0.7047 - val_loss: 0.7232 - val_accuracy: 0.5328\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5782 - accuracy: 0.7108 - val_loss: 0.7334 - val_accuracy: 0.4891\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5532 - accuracy: 0.7413 - val_loss: 0.7175 - val_accuracy: 0.5109\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.5627 - accuracy: 0.6843 - val_loss: 0.7190 - val_accuracy: 0.5328\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.5338 - accuracy: 0.7393 - val_loss: 0.7562 - val_accuracy: 0.5036\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.4961 - accuracy: 0.8248 - val_loss: 0.7310 - val_accuracy: 0.5620\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4861 - accuracy: 0.7637 - val_loss: 0.7659 - val_accuracy: 0.5474\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4553 - accuracy: 0.8147 - val_loss: 0.7534 - val_accuracy: 0.5547\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4313 - accuracy: 0.8411 - val_loss: 0.8127 - val_accuracy: 0.5620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14f2b4040>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_2 = Sequential()\n",
    "model_0_0_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_2.add(Flatten())\n",
    "model_0_0_2.add(Dense(50, activation='relu')) \n",
    "model_0_0_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_0_0_2.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_2.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "38d44d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.7010 - accuracy: 0.5193 - val_loss: 0.6908 - val_accuracy: 0.5255\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6918 - accuracy: 0.5173 - val_loss: 0.6930 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6877 - accuracy: 0.5560 - val_loss: 0.6932 - val_accuracy: 0.5109\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6949 - accuracy: 0.5316 - val_loss: 0.6950 - val_accuracy: 0.4599\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.5397 - val_loss: 0.6964 - val_accuracy: 0.4307\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6843 - accuracy: 0.5458 - val_loss: 0.6981 - val_accuracy: 0.5255\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5316 - val_loss: 0.6929 - val_accuracy: 0.4745\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6742 - accuracy: 0.6008 - val_loss: 0.7027 - val_accuracy: 0.4672\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6874 - accuracy: 0.5295 - val_loss: 0.6999 - val_accuracy: 0.5474\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6694 - accuracy: 0.5825 - val_loss: 0.7053 - val_accuracy: 0.4891\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6547 - accuracy: 0.6191 - val_loss: 0.7209 - val_accuracy: 0.4599\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 20ms/step - loss: 0.6515 - accuracy: 0.5967 - val_loss: 0.7173 - val_accuracy: 0.5036\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6407 - accuracy: 0.6293 - val_loss: 0.7255 - val_accuracy: 0.5036\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6430 - accuracy: 0.6477 - val_loss: 0.7080 - val_accuracy: 0.4818\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6161 - accuracy: 0.6762 - val_loss: 0.7250 - val_accuracy: 0.4745\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5988 - accuracy: 0.6823 - val_loss: 0.8030 - val_accuracy: 0.4818\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6237 - accuracy: 0.6578 - val_loss: 0.7405 - val_accuracy: 0.5328\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5908 - accuracy: 0.6721 - val_loss: 0.7531 - val_accuracy: 0.5474\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6106 - accuracy: 0.6293 - val_loss: 0.7083 - val_accuracy: 0.5036\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5639 - accuracy: 0.7271 - val_loss: 0.7591 - val_accuracy: 0.5036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14f5536d0>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_3 = Sequential()\n",
    "model_0_0_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_3.add(Flatten())\n",
    "model_0_0_3.add(Dense(50, activation='relu')) \n",
    "model_0_0_3.add(Dense(2, activation='softmax')) \n",
    "\n",
    "model_0_0_3.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_3.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "79a0e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 18ms/step - loss: 0.6993 - accuracy: 0.4949 - val_loss: 0.6936 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6946 - accuracy: 0.5132 - val_loss: 0.6936 - val_accuracy: 0.4672\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5214 - val_loss: 0.6950 - val_accuracy: 0.5109\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.5336 - val_loss: 0.6943 - val_accuracy: 0.5109\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.5743 - val_loss: 0.6953 - val_accuracy: 0.4599\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6853 - accuracy: 0.5458 - val_loss: 0.7051 - val_accuracy: 0.4818\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6797 - accuracy: 0.5621 - val_loss: 0.6966 - val_accuracy: 0.5401\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6771 - accuracy: 0.5621 - val_loss: 0.7027 - val_accuracy: 0.4745\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6714 - accuracy: 0.5825 - val_loss: 0.6986 - val_accuracy: 0.4526\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6470 - accuracy: 0.6538 - val_loss: 0.7086 - val_accuracy: 0.4526\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6304 - accuracy: 0.6619 - val_loss: 0.7261 - val_accuracy: 0.5328\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6127 - accuracy: 0.6599 - val_loss: 0.7259 - val_accuracy: 0.4891\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6189 - accuracy: 0.6680 - val_loss: 0.7564 - val_accuracy: 0.4891\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5852 - accuracy: 0.6965 - val_loss: 0.7379 - val_accuracy: 0.4672\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5424 - accuracy: 0.7373 - val_loss: 0.7805 - val_accuracy: 0.4745\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5348 - accuracy: 0.7556 - val_loss: 0.7943 - val_accuracy: 0.4818\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4991 - accuracy: 0.7637 - val_loss: 0.8122 - val_accuracy: 0.4891\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4625 - accuracy: 0.8086 - val_loss: 0.9024 - val_accuracy: 0.4526\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4414 - accuracy: 0.8086 - val_loss: 0.9391 - val_accuracy: 0.4453\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4137 - accuracy: 0.8228 - val_loss: 0.8931 - val_accuracy: 0.4526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14fc0af70>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_4 = Sequential()\n",
    "model_0_0_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_4.add(Flatten())\n",
    "model_0_0_4.add(Dense(50, activation='relu')) \n",
    "model_0_0_4.add(Dense(2, activation='softmax')) \n",
    "model_0_0_4.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_4.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614b3b8",
   "metadata": {},
   "source": [
    "Model_0_0_3 appears to be an initial best using all the data. Will use this one with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f47487ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_1 final data prep \n",
    "\n",
    "# PCA with 23 components to explain 85% of variance\n",
    "sklearn_pca = PCA(n_components=23)\n",
    "train_X_0_1 = pd.DataFrame(sklearn_pca.fit_transform(train_f0))\n",
    "val_y_0_1 = pd.DataFrame(sklearn_pca.transform(val_f0))\n",
    "test_y_0_1 = pd.DataFrame(sklearn_pca.transform(test_f0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c7c40f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.219217</td>\n",
       "      <td>-1.462922</td>\n",
       "      <td>1.178301</td>\n",
       "      <td>-1.158433</td>\n",
       "      <td>-0.310762</td>\n",
       "      <td>-0.971497</td>\n",
       "      <td>-0.755831</td>\n",
       "      <td>-0.230518</td>\n",
       "      <td>-0.494264</td>\n",
       "      <td>0.538112</td>\n",
       "      <td>-0.219431</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.489966</td>\n",
       "      <td>0.258522</td>\n",
       "      <td>-0.183544</td>\n",
       "      <td>-0.996880</td>\n",
       "      <td>-0.325106</td>\n",
       "      <td>0.077066</td>\n",
       "      <td>0.605346</td>\n",
       "      <td>0.326053</td>\n",
       "      <td>-0.370117</td>\n",
       "      <td>-0.036833</td>\n",
       "      <td>-0.773960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.074831</td>\n",
       "      <td>-0.975247</td>\n",
       "      <td>-1.117501</td>\n",
       "      <td>1.467582</td>\n",
       "      <td>0.600116</td>\n",
       "      <td>0.752940</td>\n",
       "      <td>-0.501049</td>\n",
       "      <td>0.335687</td>\n",
       "      <td>-0.131784</td>\n",
       "      <td>0.433012</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>-0.671462</td>\n",
       "      <td>0.304404</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.698974</td>\n",
       "      <td>0.178367</td>\n",
       "      <td>0.673246</td>\n",
       "      <td>-0.525624</td>\n",
       "      <td>-0.297949</td>\n",
       "      <td>0.272397</td>\n",
       "      <td>-0.351474</td>\n",
       "      <td>0.338810</td>\n",
       "      <td>0.609654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.709315</td>\n",
       "      <td>-1.138903</td>\n",
       "      <td>-1.021033</td>\n",
       "      <td>-0.682275</td>\n",
       "      <td>-0.695543</td>\n",
       "      <td>0.764778</td>\n",
       "      <td>-0.214868</td>\n",
       "      <td>-0.390358</td>\n",
       "      <td>0.666524</td>\n",
       "      <td>-0.413437</td>\n",
       "      <td>-0.139482</td>\n",
       "      <td>-0.195169</td>\n",
       "      <td>-1.277626</td>\n",
       "      <td>0.280918</td>\n",
       "      <td>-0.169060</td>\n",
       "      <td>0.222243</td>\n",
       "      <td>-0.186811</td>\n",
       "      <td>0.107063</td>\n",
       "      <td>-0.120785</td>\n",
       "      <td>-0.109009</td>\n",
       "      <td>-0.166495</td>\n",
       "      <td>0.097733</td>\n",
       "      <td>-0.190754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.663753</td>\n",
       "      <td>-0.627343</td>\n",
       "      <td>0.902401</td>\n",
       "      <td>0.203277</td>\n",
       "      <td>-1.282240</td>\n",
       "      <td>0.595241</td>\n",
       "      <td>-0.127063</td>\n",
       "      <td>-0.537176</td>\n",
       "      <td>-0.238251</td>\n",
       "      <td>0.242232</td>\n",
       "      <td>0.642603</td>\n",
       "      <td>-0.382570</td>\n",
       "      <td>-0.783190</td>\n",
       "      <td>0.615173</td>\n",
       "      <td>-0.315056</td>\n",
       "      <td>-0.221828</td>\n",
       "      <td>0.419174</td>\n",
       "      <td>-0.571787</td>\n",
       "      <td>-0.581266</td>\n",
       "      <td>0.398642</td>\n",
       "      <td>-0.176412</td>\n",
       "      <td>-0.606962</td>\n",
       "      <td>0.271133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.700241</td>\n",
       "      <td>-0.873808</td>\n",
       "      <td>-0.584979</td>\n",
       "      <td>1.089007</td>\n",
       "      <td>0.456954</td>\n",
       "      <td>1.263407</td>\n",
       "      <td>1.250353</td>\n",
       "      <td>-0.869443</td>\n",
       "      <td>0.494251</td>\n",
       "      <td>-0.086577</td>\n",
       "      <td>0.916465</td>\n",
       "      <td>0.613792</td>\n",
       "      <td>-0.102432</td>\n",
       "      <td>0.155376</td>\n",
       "      <td>-0.608668</td>\n",
       "      <td>-0.055980</td>\n",
       "      <td>-0.542087</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.271773</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>0.263052</td>\n",
       "      <td>0.130627</td>\n",
       "      <td>-0.436842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.215505</td>\n",
       "      <td>1.415952</td>\n",
       "      <td>1.284548</td>\n",
       "      <td>-1.906232</td>\n",
       "      <td>1.640753</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>-0.619236</td>\n",
       "      <td>-0.205285</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>-0.327709</td>\n",
       "      <td>-0.513953</td>\n",
       "      <td>-0.226027</td>\n",
       "      <td>0.608667</td>\n",
       "      <td>-0.341245</td>\n",
       "      <td>0.302062</td>\n",
       "      <td>-0.176278</td>\n",
       "      <td>0.025857</td>\n",
       "      <td>-0.183068</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>0.425254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-1.855585</td>\n",
       "      <td>1.815472</td>\n",
       "      <td>1.542833</td>\n",
       "      <td>-0.925159</td>\n",
       "      <td>0.407913</td>\n",
       "      <td>-0.316808</td>\n",
       "      <td>0.891505</td>\n",
       "      <td>-0.587438</td>\n",
       "      <td>-0.011707</td>\n",
       "      <td>-0.090071</td>\n",
       "      <td>0.917244</td>\n",
       "      <td>-0.334305</td>\n",
       "      <td>-0.091007</td>\n",
       "      <td>-0.375539</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>-0.060973</td>\n",
       "      <td>0.260019</td>\n",
       "      <td>-0.619008</td>\n",
       "      <td>0.185277</td>\n",
       "      <td>-0.507739</td>\n",
       "      <td>0.072617</td>\n",
       "      <td>-0.163648</td>\n",
       "      <td>-0.072349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.344175</td>\n",
       "      <td>2.714250</td>\n",
       "      <td>-0.050140</td>\n",
       "      <td>1.376521</td>\n",
       "      <td>-0.163887</td>\n",
       "      <td>0.344485</td>\n",
       "      <td>1.132100</td>\n",
       "      <td>0.093943</td>\n",
       "      <td>1.117169</td>\n",
       "      <td>0.132252</td>\n",
       "      <td>-0.223988</td>\n",
       "      <td>0.390860</td>\n",
       "      <td>-0.403981</td>\n",
       "      <td>-0.299661</td>\n",
       "      <td>0.802703</td>\n",
       "      <td>-0.494383</td>\n",
       "      <td>0.226498</td>\n",
       "      <td>-0.423929</td>\n",
       "      <td>-0.091523</td>\n",
       "      <td>0.236133</td>\n",
       "      <td>-0.281697</td>\n",
       "      <td>0.132193</td>\n",
       "      <td>-0.145807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1.359647</td>\n",
       "      <td>2.496757</td>\n",
       "      <td>-0.654457</td>\n",
       "      <td>-0.348960</td>\n",
       "      <td>-0.545734</td>\n",
       "      <td>0.588113</td>\n",
       "      <td>0.224513</td>\n",
       "      <td>0.563510</td>\n",
       "      <td>1.070476</td>\n",
       "      <td>0.663969</td>\n",
       "      <td>-0.192287</td>\n",
       "      <td>0.316345</td>\n",
       "      <td>0.036057</td>\n",
       "      <td>-0.203978</td>\n",
       "      <td>0.410482</td>\n",
       "      <td>-0.595080</td>\n",
       "      <td>0.101182</td>\n",
       "      <td>-0.634113</td>\n",
       "      <td>-0.960526</td>\n",
       "      <td>-0.359390</td>\n",
       "      <td>0.367505</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.150783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.151745</td>\n",
       "      <td>2.187918</td>\n",
       "      <td>-0.498737</td>\n",
       "      <td>-0.924619</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>-0.762025</td>\n",
       "      <td>0.464947</td>\n",
       "      <td>0.333831</td>\n",
       "      <td>0.880404</td>\n",
       "      <td>0.345708</td>\n",
       "      <td>-0.569200</td>\n",
       "      <td>-0.029051</td>\n",
       "      <td>0.585565</td>\n",
       "      <td>0.245967</td>\n",
       "      <td>-0.217486</td>\n",
       "      <td>-0.136106</td>\n",
       "      <td>0.672648</td>\n",
       "      <td>-0.881246</td>\n",
       "      <td>0.286066</td>\n",
       "      <td>-0.545320</td>\n",
       "      <td>-0.169566</td>\n",
       "      <td>-0.138191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.219217 -1.462922  1.178301 -1.158433 -0.310762 -0.971497 -0.755831   \n",
       "1   -1.074831 -0.975247 -1.117501  1.467582  0.600116  0.752940 -0.501049   \n",
       "2   -1.709315 -1.138903 -1.021033 -0.682275 -0.695543  0.764778 -0.214868   \n",
       "3    1.663753 -0.627343  0.902401  0.203277 -1.282240  0.595241 -0.127063   \n",
       "4   -0.700241 -0.873808 -0.584979  1.089007  0.456954  1.263407  1.250353   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491  0.215505  1.415952  1.284548 -1.906232  1.640753  0.031301  0.003976   \n",
       "492 -1.855585  1.815472  1.542833 -0.925159  0.407913 -0.316808  0.891505   \n",
       "493 -0.344175  2.714250 -0.050140  1.376521 -0.163887  0.344485  1.132100   \n",
       "494  1.359647  2.496757 -0.654457 -0.348960 -0.545734  0.588113  0.224513   \n",
       "495  1.151745  2.187918 -0.498737 -0.924619  0.119817 -0.005517 -0.762025   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -0.230518 -0.494264  0.538112 -0.219431  0.110541  0.489966  0.258522   \n",
       "1    0.335687 -0.131784  0.433012  0.971562 -0.671462  0.304404  0.032035   \n",
       "2   -0.390358  0.666524 -0.413437 -0.139482 -0.195169 -1.277626  0.280918   \n",
       "3   -0.537176 -0.238251  0.242232  0.642603 -0.382570 -0.783190  0.615173   \n",
       "4   -0.869443  0.494251 -0.086577  0.916465  0.613792 -0.102432  0.155376   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491 -0.619236 -0.205285  0.207232 -0.327709 -0.513953 -0.226027  0.608667   \n",
       "492 -0.587438 -0.011707 -0.090071  0.917244 -0.334305 -0.091007 -0.375539   \n",
       "493  0.093943  1.117169  0.132252 -0.223988  0.390860 -0.403981 -0.299661   \n",
       "494  0.563510  1.070476  0.663969 -0.192287  0.316345  0.036057 -0.203978   \n",
       "495  0.464947  0.333831  0.880404  0.345708 -0.569200 -0.029051  0.585565   \n",
       "\n",
       "           14        15        16        17        18        19        20  \\\n",
       "0   -0.183544 -0.996880 -0.325106  0.077066  0.605346  0.326053 -0.370117   \n",
       "1    0.698974  0.178367  0.673246 -0.525624 -0.297949  0.272397 -0.351474   \n",
       "2   -0.169060  0.222243 -0.186811  0.107063 -0.120785 -0.109009 -0.166495   \n",
       "3   -0.315056 -0.221828  0.419174 -0.571787 -0.581266  0.398642 -0.176412   \n",
       "4   -0.608668 -0.055980 -0.542087  0.043258  0.271773 -0.561970  0.263052   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491 -0.341245  0.302062 -0.176278  0.025857 -0.183068  0.119498 -0.056718   \n",
       "492  0.028622 -0.060973  0.260019 -0.619008  0.185277 -0.507739  0.072617   \n",
       "493  0.802703 -0.494383  0.226498 -0.423929 -0.091523  0.236133 -0.281697   \n",
       "494  0.410482 -0.595080  0.101182 -0.634113 -0.960526 -0.359390  0.367505   \n",
       "495  0.245967 -0.217486 -0.136106  0.672648 -0.881246  0.286066 -0.545320   \n",
       "\n",
       "           21        22  \n",
       "0   -0.036833 -0.773960  \n",
       "1    0.338810  0.609654  \n",
       "2    0.097733 -0.190754  \n",
       "3   -0.606962  0.271133  \n",
       "4    0.130627 -0.436842  \n",
       "..        ...       ...  \n",
       "491  0.063401  0.425254  \n",
       "492 -0.163648 -0.072349  \n",
       "493  0.132193 -0.145807  \n",
       "494  0.000487  0.150783  \n",
       "495 -0.169566 -0.138191  \n",
       "\n",
       "[496 rows x 23 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5a07cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked function to handle numpy arrays\n",
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_y_np(np, window_size=5): # converts to matrix of numpy arrays\n",
    "  np.to_numpy()\n",
    "  X = []\n",
    "  for i in range(len(np)-window_size): # length of np array - window_size so it does't take empty values at the end\n",
    "    row = [r for r in np[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "     # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X)\n",
    "\n",
    "def df_to_X_no(df, window_size=5):\n",
    "  df_as_np = df.to_numpy() # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  for i in range(len(df_as_np)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df_as_np[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "  return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9f9b0673",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/v484103954dd6vtd4swk8hd40000gn/T/ipykernel_32546/1063992594.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cononverting to window format, in this case 5 periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_X_0_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_no\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_0_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_X_0_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y_0_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_X_0_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_0_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# cononverting to window format, in this case 5 periods\n",
    "train_X_0_1, _ = df_to_X_no(train_X_0_1)\n",
    "val_X_0_1, _ = df_to_X_y_np(val_y_0_1)\n",
    "test_X_0_1, _ = df_to_X_y_np(test_y_0_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515730c1-45ed-49e9-86b8-dbd07f2dbd0e",
   "metadata": {},
   "source": [
    "# CNN Model using full Ford data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "23263a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, InputLayer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, model_selection\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb74ca8-4ea0-46a4-afa0-b61173feef12",
   "metadata": {},
   "source": [
    "## Part 1: Clean and Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3e760be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9fb5de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ford</th>\n",
       "      <th>F-150</th>\n",
       "      <th>Ford Bronco_x</th>\n",
       "      <th>Ford Mustang_x</th>\n",
       "      <th>Ford Stock</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Ford Motor Company</th>\n",
       "      <th>Ford Mustang_y</th>\n",
       "      <th>Ford F Series</th>\n",
       "      <th>Ford Bronco_y</th>\n",
       "      <th>Lincoln Navigator</th>\n",
       "      <th>Lincoln Aviator</th>\n",
       "      <th>Ford GT</th>\n",
       "      <th>dow_open</th>\n",
       "      <th>dow_high</th>\n",
       "      <th>dow_low</th>\n",
       "      <th>dow_close</th>\n",
       "      <th>dow_vol</th>\n",
       "      <th>nas_open</th>\n",
       "      <th>nas_high</th>\n",
       "      <th>nas_low</th>\n",
       "      <th>nas_close</th>\n",
       "      <th>nas_vol</th>\n",
       "      <th>Wiki_total</th>\n",
       "      <th>Google_total</th>\n",
       "      <th>Stock_total</th>\n",
       "      <th>Nas_total</th>\n",
       "      <th>Dow_total</th>\n",
       "      <th>Wiki_Moment_1</th>\n",
       "      <th>Wiki_Moment_2</th>\n",
       "      <th>Wiki_Moment_1_s</th>\n",
       "      <th>Wiki_Moment_2_s</th>\n",
       "      <th>Wiki_MAvg</th>\n",
       "      <th>Wiki_MAvg_s</th>\n",
       "      <th>Wiki_Disparity</th>\n",
       "      <th>Wiki_Disparity_s</th>\n",
       "      <th>Wiki_ROC</th>\n",
       "      <th>Wiki_ROC_s</th>\n",
       "      <th>Wiki_Rocp</th>\n",
       "      <th>Wiki_EMA</th>\n",
       "      <th>Wiki_diff</th>\n",
       "      <th>Wiki_gain</th>\n",
       "      <th>Wiki_loss</th>\n",
       "      <th>Wiki_avg_gain</th>\n",
       "      <th>Wiki_avg_loss</th>\n",
       "      <th>Wiki_rs</th>\n",
       "      <th>Wiki_RSI</th>\n",
       "      <th>Wiki_Move</th>\n",
       "      <th>Wiki_MAvg_Move</th>\n",
       "      <th>Wiki_MAvg_s_Move</th>\n",
       "      <th>Wiki_EMA_Move</th>\n",
       "      <th>Wiki_Disparity_Move</th>\n",
       "      <th>Wiki_Disparity_s_Move</th>\n",
       "      <th>Wiki_RSI_Move</th>\n",
       "      <th>Google_Moment_1</th>\n",
       "      <th>Google_Moment_2</th>\n",
       "      <th>Google_Moment_1_s</th>\n",
       "      <th>Google_Moment_2_s</th>\n",
       "      <th>Google_MAvg</th>\n",
       "      <th>Google_MAvg_s</th>\n",
       "      <th>Google_Disparity</th>\n",
       "      <th>Google_Disparity_s</th>\n",
       "      <th>Google_ROC</th>\n",
       "      <th>Google_ROC_s</th>\n",
       "      <th>Google_Rocp</th>\n",
       "      <th>Google_EMA</th>\n",
       "      <th>Google_diff</th>\n",
       "      <th>Google_gain</th>\n",
       "      <th>Google_loss</th>\n",
       "      <th>Google_avg_gain</th>\n",
       "      <th>Google_avg_loss</th>\n",
       "      <th>Google_rs</th>\n",
       "      <th>Google_RSI</th>\n",
       "      <th>Google_Move</th>\n",
       "      <th>Google_MAvg_Move</th>\n",
       "      <th>Google_MAvg_s_Move</th>\n",
       "      <th>Google_EMA_Move</th>\n",
       "      <th>Google_Disparity_Move</th>\n",
       "      <th>Google_Disparity_s_Move</th>\n",
       "      <th>Google_RSI_Move</th>\n",
       "      <th>Stock_Moment_1</th>\n",
       "      <th>Stock_Moment_2</th>\n",
       "      <th>Stock_Moment_1_s</th>\n",
       "      <th>Stock_Moment_2_s</th>\n",
       "      <th>Stock_MAvg</th>\n",
       "      <th>Stock_MAvg_s</th>\n",
       "      <th>Stock_Disparity</th>\n",
       "      <th>Stock_Disparity_s</th>\n",
       "      <th>Stock_ROC</th>\n",
       "      <th>Stock_ROC_s</th>\n",
       "      <th>Stock_Rocp</th>\n",
       "      <th>Stock_EMA</th>\n",
       "      <th>Stock_diff</th>\n",
       "      <th>Stock_gain</th>\n",
       "      <th>Stock_loss</th>\n",
       "      <th>Stock_avg_gain</th>\n",
       "      <th>Stock_avg_loss</th>\n",
       "      <th>Stock_rs</th>\n",
       "      <th>Stock_RSI</th>\n",
       "      <th>Stock_Move</th>\n",
       "      <th>Stock_MAvg_Move</th>\n",
       "      <th>Stock_MAvg_s_Move</th>\n",
       "      <th>Stock_EMA_Move</th>\n",
       "      <th>Stock_Disparity_Move</th>\n",
       "      <th>Stock_Disparity_s_Move</th>\n",
       "      <th>Stock_RSI_Move</th>\n",
       "      <th>Nas_Moment_1</th>\n",
       "      <th>Nas_Moment_2</th>\n",
       "      <th>Nas_Moment_1_s</th>\n",
       "      <th>Nas_Moment_2_s</th>\n",
       "      <th>Nas_MAvg</th>\n",
       "      <th>Nas_MAvg_s</th>\n",
       "      <th>Nas_Disparity</th>\n",
       "      <th>Nas_Disparity_s</th>\n",
       "      <th>Nas_ROC</th>\n",
       "      <th>Nas_ROC_s</th>\n",
       "      <th>Nas_Rocp</th>\n",
       "      <th>Nas_EMA</th>\n",
       "      <th>Nas_diff</th>\n",
       "      <th>Nas_gain</th>\n",
       "      <th>Nas_loss</th>\n",
       "      <th>Nas_avg_gain</th>\n",
       "      <th>Nas_avg_loss</th>\n",
       "      <th>Nas_rs</th>\n",
       "      <th>Nas_RSI</th>\n",
       "      <th>Nas_Move</th>\n",
       "      <th>Nas_MAvg_Move</th>\n",
       "      <th>Nas_MAvg_s_Move</th>\n",
       "      <th>Nas_EMA_Move</th>\n",
       "      <th>Nas_Disparity_Move</th>\n",
       "      <th>Nas_Disparity_s_Move</th>\n",
       "      <th>Nas_RSI_Move</th>\n",
       "      <th>Dow_Moment_1</th>\n",
       "      <th>Dow_Moment_2</th>\n",
       "      <th>Dow_Moment_1_s</th>\n",
       "      <th>Dow_Moment_2_s</th>\n",
       "      <th>Dow_MAvg</th>\n",
       "      <th>Dow_MAvg_s</th>\n",
       "      <th>Dow_Disparity</th>\n",
       "      <th>Dow_Disparity_s</th>\n",
       "      <th>Dow_ROC</th>\n",
       "      <th>Dow_ROC_s</th>\n",
       "      <th>Dow_Rocp</th>\n",
       "      <th>Dow_EMA</th>\n",
       "      <th>Dow_diff</th>\n",
       "      <th>Dow_gain</th>\n",
       "      <th>Dow_loss</th>\n",
       "      <th>Dow_avg_gain</th>\n",
       "      <th>Dow_avg_loss</th>\n",
       "      <th>Dow_rs</th>\n",
       "      <th>Dow_RSI</th>\n",
       "      <th>Dow_Move</th>\n",
       "      <th>Dow_MAvg_Move</th>\n",
       "      <th>Dow_MAvg_s_Move</th>\n",
       "      <th>Dow_EMA_Move</th>\n",
       "      <th>Dow_Disparity_Move</th>\n",
       "      <th>Dow_Disparity_s_Move</th>\n",
       "      <th>Dow_RSI_Move</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ford  F-150  Ford Bronco_x  Ford Mustang_x  Ford Stock  Open  High  Low  \\\n",
       "0     0      0              0               0           0     6     6    6   \n",
       "\n",
       "   Close  Volume  Dividends  Stock Splits  Ford Motor Company  Ford Mustang_y  \\\n",
       "0      6       6          6             6                   0               0   \n",
       "\n",
       "   Ford F Series  Ford Bronco_y  Lincoln Navigator  Lincoln Aviator  Ford GT  \\\n",
       "0              0              0                  0                0        0   \n",
       "\n",
       "   dow_open  dow_high  dow_low  dow_close  dow_vol  nas_open  nas_high  \\\n",
       "0         4         4        4          4        4         4         4   \n",
       "\n",
       "   nas_low  nas_close  nas_vol  Wiki_total  Google_total  Stock_total  \\\n",
       "0        4          4        4           0             0            6   \n",
       "\n",
       "   Nas_total  Dow_total  Wiki_Moment_1  Wiki_Moment_2  Wiki_Moment_1_s  \\\n",
       "0          4          4              0              0                0   \n",
       "\n",
       "   Wiki_Moment_2_s  Wiki_MAvg  Wiki_MAvg_s  Wiki_Disparity  Wiki_Disparity_s  \\\n",
       "0                0          0            0               0                 0   \n",
       "\n",
       "   Wiki_ROC  Wiki_ROC_s  Wiki_Rocp  Wiki_EMA  Wiki_diff  Wiki_gain  Wiki_loss  \\\n",
       "0         0           0          0         0          0          0          0   \n",
       "\n",
       "   Wiki_avg_gain  Wiki_avg_loss  Wiki_rs  Wiki_RSI  Wiki_Move  Wiki_MAvg_Move  \\\n",
       "0              0              0        0         0          0               0   \n",
       "\n",
       "   Wiki_MAvg_s_Move  Wiki_EMA_Move  Wiki_Disparity_Move  \\\n",
       "0                 0              0                    0   \n",
       "\n",
       "   Wiki_Disparity_s_Move  Wiki_RSI_Move  Google_Moment_1  Google_Moment_2  \\\n",
       "0                      0              0                0                0   \n",
       "\n",
       "   Google_Moment_1_s  Google_Moment_2_s  Google_MAvg  Google_MAvg_s  \\\n",
       "0                  0                  0            0              0   \n",
       "\n",
       "   Google_Disparity  Google_Disparity_s  Google_ROC  Google_ROC_s  \\\n",
       "0                 0                   0           0             0   \n",
       "\n",
       "   Google_Rocp  Google_EMA  Google_diff  Google_gain  Google_loss  \\\n",
       "0            0           0            0            0            0   \n",
       "\n",
       "   Google_avg_gain  Google_avg_loss  Google_rs  Google_RSI  Google_Move  \\\n",
       "0                0                0          0           0            0   \n",
       "\n",
       "   Google_MAvg_Move  Google_MAvg_s_Move  Google_EMA_Move  \\\n",
       "0                 0                   0                0   \n",
       "\n",
       "   Google_Disparity_Move  Google_Disparity_s_Move  Google_RSI_Move  \\\n",
       "0                      0                        0                0   \n",
       "\n",
       "   Stock_Moment_1  Stock_Moment_2  Stock_Moment_1_s  Stock_Moment_2_s  \\\n",
       "0              11              11                11                11   \n",
       "\n",
       "   Stock_MAvg  Stock_MAvg_s  Stock_Disparity  Stock_Disparity_s  Stock_ROC  \\\n",
       "0           0             0                6                  6         11   \n",
       "\n",
       "   Stock_ROC_s  Stock_Rocp  Stock_EMA  Stock_diff  Stock_gain  Stock_loss  \\\n",
       "0           11          11          6          11          11          11   \n",
       "\n",
       "   Stock_avg_gain  Stock_avg_loss  Stock_rs  Stock_RSI  Stock_Move  \\\n",
       "0              76              76        76         76           0   \n",
       "\n",
       "   Stock_MAvg_Move  Stock_MAvg_s_Move  Stock_EMA_Move  Stock_Disparity_Move  \\\n",
       "0                0                  0               0                     0   \n",
       "\n",
       "   Stock_Disparity_s_Move  Stock_RSI_Move  Nas_Moment_1  Nas_Moment_2  \\\n",
       "0                       0               0             7             7   \n",
       "\n",
       "   Nas_Moment_1_s  Nas_Moment_2_s  Nas_MAvg  Nas_MAvg_s  Nas_Disparity  \\\n",
       "0               7               7         0           0              4   \n",
       "\n",
       "   Nas_Disparity_s  Nas_ROC  Nas_ROC_s  Nas_Rocp  Nas_EMA  Nas_diff  Nas_gain  \\\n",
       "0                4        7          7         7        4         7         7   \n",
       "\n",
       "   Nas_loss  Nas_avg_gain  Nas_avg_loss  Nas_rs  Nas_RSI  Nas_Move  \\\n",
       "0         7            46            46      46       46         0   \n",
       "\n",
       "   Nas_MAvg_Move  Nas_MAvg_s_Move  Nas_EMA_Move  Nas_Disparity_Move  \\\n",
       "0              0                0             0                   0   \n",
       "\n",
       "   Nas_Disparity_s_Move  Nas_RSI_Move  Dow_Moment_1  Dow_Moment_2  \\\n",
       "0                     0             0             7             7   \n",
       "\n",
       "   Dow_Moment_1_s  Dow_Moment_2_s  Dow_MAvg  Dow_MAvg_s  Dow_Disparity  \\\n",
       "0               7               7         0           0              4   \n",
       "\n",
       "   Dow_Disparity_s  Dow_ROC  Dow_ROC_s  Dow_Rocp  Dow_EMA  Dow_diff  Dow_gain  \\\n",
       "0                4        7          7         7        4         7         7   \n",
       "\n",
       "   Dow_loss  Dow_avg_gain  Dow_avg_loss  Dow_rs  Dow_RSI  Dow_Move  \\\n",
       "0         7            46            46      46       46         0   \n",
       "\n",
       "   Dow_MAvg_Move  Dow_MAvg_s_Move  Dow_EMA_Move  Dow_Disparity_Move  \\\n",
       "0              0                0             0                   0   \n",
       "\n",
       "   Dow_Disparity_s_Move  Dow_RSI_Move  target_1  target_2  target_3  target_4  \\\n",
       "0                     0             0         0         0         0         0   \n",
       "\n",
       "   target_5  \n",
       "0         0  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ford = pd.read_csv(\"Ford_Cleaned_Date.csv\")\n",
    "Ford.date = pd.to_datetime(Ford.date)\n",
    "Ford = Ford.set_index(\"date\")\n",
    "Ford = Ford.iloc[14:, :] # to remove first 14 days that include NaNs due to some calculations\n",
    "pd.DataFrame(Ford.isna().sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8736c8",
   "metadata": {},
   "source": [
    "We see from above that some varaibles contain a lot of NaN's so they either might not be useful, or they're going to lead us into eliminating a lot of data points.\n",
    "\n",
    "To Solve NaN problem we will create two initial data sets, one removing high NaN values, one not, and see which produced better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "997c5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ford = Ford.drop([['Ford', 'Ford_Bronco_x', 'Ford_Stock', 'F-150', 'Ford_Bronco_y', 'Ford Motor Company', 'Ford F Series', 'Lincoln Navigator', 'Lincoln Aviator']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74c58691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((709, 169), (768, 134))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High NaN varaibles included will be Ford_0\n",
    "Ford_0 = Ford.dropna()\n",
    "Ford_0 = Ford_0[~(Ford_0.isin([np.inf, -np.inf]).any(axis=1))] # to remove inf\n",
    "\n",
    "# Ford_1 will remove the high NaN columns\n",
    "Ford_1 = Ford[Ford.columns.drop(list(Ford.filter(regex='gain')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='loss')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='RSI')))]\n",
    "Ford_1 = Ford_1[Ford_1.columns.drop(list(Ford_1.filter(regex='_rs')))]\n",
    "Ford_1 = Ford_1.dropna()\n",
    "Ford_1 = Ford_1[~(Ford_1.isin([np.inf, -np.inf]).any(axis=1))]\n",
    "\n",
    "Ford_0.shape,Ford_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8a68ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Chosen is target_3\n",
    "Ford_0 = Ford_0.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "Ford_1 = Ford_1.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "\n",
    "target_3_0 = Ford_0[\"target_3\"]\n",
    "target_3_1 = Ford_0[\"target_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "262d44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training sets \n",
    "column_indices = {name: i for i, name in enumerate(Ford_1.columns)}\n",
    "\n",
    "n = len(Ford_0)\n",
    "train_f0 = Ford_0[0:int(n*0.7)]\n",
    "val_f0 = Ford_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0 = Ford_0[int(n*0.9):]\n",
    "\n",
    "train_f0t = target_3_0[0:int(n*0.7)]\n",
    "val_f0t = target_3_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0t = target_3_0[int(n*0.9):]\n",
    "\n",
    "# now with Ford_1\n",
    "n = len(Ford_1)\n",
    "train_f1 = Ford_1[0:int(n*0.7)]\n",
    "val_f1 = Ford_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1 = Ford_1[int(n*0.9):]\n",
    "\n",
    "train_f1t = target_3_1[0:int(n*0.7)]\n",
    "val_f1t = target_3_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1t = target_3_1[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e5e31954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preporocessing and standardizing the data\n",
    "Mscaler = MinMaxScaler() # keeps binarys at zero and 1 :)\n",
    "\n",
    "train_f0 = pd.DataFrame(Mscaler.fit_transform(train_f0), columns = Ford_0.columns)\n",
    "val_f0 = pd.DataFrame(Mscaler.fit_transform(val_f0), columns = Ford_0.columns)\n",
    "test_f0 = pd.DataFrame(Mscaler.fit_transform(test_f0), columns = Ford_0.columns)\n",
    "\n",
    "train_f1 = pd.DataFrame(Mscaler.fit_transform(train_f1), columns = Ford_1.columns)\n",
    "val_f1 = pd.DataFrame(Mscaler.fit_transform(val_f1), columns = Ford_1.columns)\n",
    "test_f1 = pd.DataFrame(Mscaler.fit_transform(test_f1), columns = Ford_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2159c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_y2(df, target, window_size=5):\n",
    "  df_as_np = df.to_numpy() # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df_as_np[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "    label = target[i+window_size] # pulls the target variable after the window, target varible needs to be column zero in this \n",
    "    y.append(label) # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b42ae",
   "metadata": {},
   "source": [
    "## Switching Focus to Just Ford_0, High NaN Varibles included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedd7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Features         Score\n",
      "164                target_3  1.853982e+17\n",
      "57      Wiki_Disparity_Move  5.048879e+00\n",
      "155                  Dow_rs  3.490399e+00\n",
      "58    Wiki_Disparity_s_Move  3.014178e+00\n",
      "3            Ford Mustang_x  2.128389e+00\n",
      "53                Wiki_Move  2.075053e+00\n",
      "52                 Wiki_RSI  2.059093e+00\n",
      "154            Dow_avg_loss  2.040229e+00\n",
      "75          Google_avg_gain  1.990507e+00\n",
      "41         Wiki_Disparity_s  1.941543e+00\n",
      "128            Nas_avg_loss  1.866438e+00\n",
      "87           Stock_Moment_2  1.839803e+00\n",
      "110  Stock_Disparity_s_Move  1.623449e+00\n",
      "4                Ford Stock  1.510923e+00\n",
      "136    Nas_Disparity_s_Move  1.426169e+00\n",
      "148                Dow_Rocp  1.350381e+00\n",
      "82          Google_EMA_Move  1.347818e+00\n",
      "153            Dow_avg_gain  1.339265e+00\n",
      "76          Google_avg_loss  1.188484e+00\n",
      "10                Dividends  1.159343e+00\n",
      "15            Ford Bronco_y  1.009442e+00\n",
      "70              Google_Rocp  9.298293e-01\n",
      "104               Stock_RSI  9.298293e-01\n",
      "13           Ford Mustang_y  9.202174e-01\n",
      "18                  Ford GT  8.989922e-01\n",
      "94                Stock_ROC  8.890224e-01\n",
      "86           Stock_Moment_1  8.890224e-01\n",
      "12       Ford Motor Company  8.557236e-01\n",
      "119         Nas_Disparity_s  8.274363e-01\n",
      "118           Nas_Disparity  8.230253e-01\n",
      "85          Google_RSI_Move  7.997378e-01\n",
      "35            Wiki_Moment_2  7.776846e-01\n",
      "162    Dow_Disparity_s_Move  7.768721e-01\n",
      "131                Nas_Move  7.590650e-01\n",
      "108          Stock_EMA_Move  7.547244e-01\n",
      "47                Wiki_gain  7.470438e-01\n",
      "54           Wiki_MAvg_Move  7.190388e-01\n",
      "46                Wiki_diff  6.987742e-01\n",
      "89         Stock_Moment_2_s  6.953927e-01\n",
      "129                  Nas_rs  6.324531e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 40 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=40)\n",
    "best_fit = bestfeatures.fit(train_f0, train_f0t)\n",
    "best_scores = pd.DataFrame(best_fit.scores_)\n",
    "best_columns = pd.DataFrame(Ford_0.columns)\n",
    "\n",
    "# concatenate the dataframes for better visualization\n",
    "features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "print(features_score.nlargest(40, 'Score'))  # print the top 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683d2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(features_score.nlargest(40, 'Score')['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048d7967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 15.68433691,  24.74667373,  33.10378249,  40.54965109,\n",
       "        47.39486325,  52.10029515,  55.50687153,  58.50470073,\n",
       "        61.42530001,  64.16698358,  66.53422149,  68.88994394,\n",
       "        71.02367211,  73.08023374,  75.02227321,  76.60158477,\n",
       "        78.0620214 ,  79.39525157,  80.67360492,  81.84725939,\n",
       "        83.00384916,  84.08061599,  85.07367374,  86.01668656,\n",
       "        86.88167021,  87.66126402,  88.38898007,  89.07250363,\n",
       "        89.72934819,  90.35721032,  90.96092432,  91.51678808,\n",
       "        92.05310805,  92.55542469,  93.03539412,  93.48671903,\n",
       "        93.90272285,  94.30439446,  94.68386345,  95.03240113,\n",
       "        95.3773965 ,  95.69977774,  96.00449478,  96.27854221,\n",
       "        96.53127052,  96.77177334,  96.99681187,  97.20554906,\n",
       "        97.39294777,  97.57709392,  97.74530298,  97.90443261,\n",
       "        98.05346752,  98.18960852,  98.31107071,  98.42348617,\n",
       "        98.52366819,  98.62112629,  98.70982943,  98.7923154 ,\n",
       "        98.86712008,  98.9372757 ,  99.00342101,  99.06554473,\n",
       "        99.12432462,  99.17917825,  99.23162819,  99.28083157,\n",
       "        99.32823752,  99.37265359,  99.41454389,  99.45282069,\n",
       "        99.48730254,  99.51861474,  99.5471169 ,  99.57270153,\n",
       "        99.59784448,  99.62100526,  99.64350963,  99.66522011,\n",
       "        99.6856891 ,  99.70476123,  99.72290744,  99.74025873,\n",
       "        99.75691696,  99.77298203,  99.78846039,  99.80342555,\n",
       "        99.81769669,  99.83084151,  99.84374413,  99.85580942,\n",
       "        99.86764828,  99.87914171,  99.89013066,  99.90069459,\n",
       "        99.91091898,  99.92031976,  99.92834919,  99.93602977,\n",
       "        99.94327895,  99.94966882,  99.9551483 ,  99.96013141,\n",
       "        99.96492254,  99.96896246,  99.97267246,  99.97624287,\n",
       "        99.97936168,  99.98222935,  99.9848438 ,  99.98683568,\n",
       "        99.98857062,  99.99008194,  99.9915254 ,  99.99276596,\n",
       "        99.99397484,  99.99503114,  99.99569156,  99.99629903,\n",
       "        99.99677637,  99.99724637,  99.99763481,  99.99800224,\n",
       "        99.99833522,  99.99865038,  99.998931  ,  99.99913134,\n",
       "        99.99928818,  99.99942647,  99.99954131,  99.99963876,\n",
       "        99.99971003,  99.99977048,  99.99982408,  99.99986711,\n",
       "        99.99990905,  99.99993972,  99.99995556,  99.99997032,\n",
       "        99.99998449,  99.99999052,  99.99999561,  99.99999892,\n",
       "        99.99999999, 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA().fit(train_f0)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "# reach 85% variance explained with 23 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e186f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 18.66241111,  35.84253923,  45.30504038,  52.922487  ,\n",
       "        60.4248241 ,  67.50426855,  74.21093435,  80.54626761,\n",
       "        84.05727658,  86.62596149,  88.99254744,  91.1645831 ,\n",
       "        92.92216156,  94.55803141,  95.52117579,  96.38660887,\n",
       "        97.08481547,  97.60281808,  98.08808651,  98.32139431,\n",
       "        98.54934806,  98.75599665,  98.94597887,  99.12335478,\n",
       "        99.26581491,  99.39748682,  99.50860948,  99.5814469 ,\n",
       "        99.64646238,  99.70346206,  99.75636888,  99.8053512 ,\n",
       "        99.85332717,  99.89527546,  99.92696851,  99.95199898,\n",
       "        99.97634481,  99.99802256, 100.        , 100.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA with 40 best components\n",
    "pca_1 = PCA().fit(train_f0[feats])\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca_1.explained_variance_ratio_) * 100\n",
    "# can use 8 principle compents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b2cc0",
   "metadata": {},
   "source": [
    "Keeping score: Will test 5 models with Ford_0 data variations,\n",
    "- Model_0_0 = All varaibles,\n",
    "- Model_0_1 = PCA All varaibles\n",
    "- Model_0_2 = 40 best k score\n",
    "- Model_0_3 = PCA of 40 Best K Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a449c60",
   "metadata": {},
   "source": [
    "### Model_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ef8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_0, train_f0t_tc = df_to_X_y2(train_f0,train_f0t)\n",
    "val_X_0_0, val_f0t_tc= df_to_X_y2(val_f0, val_f0t)\n",
    "test_X_0_0, test_f0t_tc = df_to_X_y2(test_f0,test_f0t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c108603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 491)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_f0t), len(train_X_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602a57f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_0.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0375a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.6945 - accuracy: 0.5173 - val_loss: 0.6911 - val_accuracy: 0.5766\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5295 - val_loss: 0.6908 - val_accuracy: 0.5401\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.5418 - val_loss: 0.6906 - val_accuracy: 0.5255\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6886 - accuracy: 0.5418 - val_loss: 0.6905 - val_accuracy: 0.5401\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.5255 - val_loss: 0.6904 - val_accuracy: 0.5182\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6834 - accuracy: 0.5927 - val_loss: 0.6910 - val_accuracy: 0.5401\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6820 - accuracy: 0.5886 - val_loss: 0.6922 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6777 - accuracy: 0.5927 - val_loss: 0.6958 - val_accuracy: 0.5328\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5377 - val_loss: 0.7059 - val_accuracy: 0.5182\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6844 - accuracy: 0.5519 - val_loss: 0.6950 - val_accuracy: 0.4964\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.5804 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6708 - accuracy: 0.6253 - val_loss: 0.6977 - val_accuracy: 0.4964\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6728 - accuracy: 0.5743 - val_loss: 0.6959 - val_accuracy: 0.5036\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6678 - accuracy: 0.6130 - val_loss: 0.6986 - val_accuracy: 0.4964\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6580 - accuracy: 0.6232 - val_loss: 0.7006 - val_accuracy: 0.5255\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6542 - accuracy: 0.6151 - val_loss: 0.7034 - val_accuracy: 0.4964\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6441 - accuracy: 0.6477 - val_loss: 0.7070 - val_accuracy: 0.5255\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6397 - accuracy: 0.6599 - val_loss: 0.7115 - val_accuracy: 0.4891\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6305 - accuracy: 0.6619 - val_loss: 0.7131 - val_accuracy: 0.4964\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6264 - accuracy: 0.6456 - val_loss: 0.7232 - val_accuracy: 0.5255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150177d60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_0.shape[1]\n",
    "n_features = train_X_0_0.shape[2]\n",
    "\n",
    "model_0_0_1 = Sequential()\n",
    "model_0_0_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_1.add(Flatten())\n",
    "model_0_0_1.add(Dense(50, activation='relu')) \n",
    "model_0_0_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_0_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_1.fit(train_X_0_0, train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc060af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 2s 18ms/step - loss: 0.7201 - accuracy: 0.5071 - val_loss: 0.7042 - val_accuracy: 0.5109\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6984 - accuracy: 0.5173 - val_loss: 0.7141 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6812 - accuracy: 0.5397 - val_loss: 0.7284 - val_accuracy: 0.4818\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6909 - accuracy: 0.5397 - val_loss: 0.7122 - val_accuracy: 0.5109\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6648 - accuracy: 0.6253 - val_loss: 0.7051 - val_accuracy: 0.4745\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6612 - accuracy: 0.6191 - val_loss: 0.7285 - val_accuracy: 0.5109\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6576 - accuracy: 0.5784 - val_loss: 0.7022 - val_accuracy: 0.5036\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6547 - accuracy: 0.5988 - val_loss: 0.7065 - val_accuracy: 0.5036\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6341 - accuracy: 0.6619 - val_loss: 0.7060 - val_accuracy: 0.4672\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6182 - accuracy: 0.6741 - val_loss: 0.7125 - val_accuracy: 0.4672\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6177 - accuracy: 0.6640 - val_loss: 0.7118 - val_accuracy: 0.5109\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5861 - accuracy: 0.7271 - val_loss: 0.7111 - val_accuracy: 0.4745\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5622 - accuracy: 0.7597 - val_loss: 0.7328 - val_accuracy: 0.4891\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5540 - accuracy: 0.7475 - val_loss: 0.7365 - val_accuracy: 0.4818\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5237 - accuracy: 0.7658 - val_loss: 0.7903 - val_accuracy: 0.5109\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5238 - accuracy: 0.7373 - val_loss: 0.7431 - val_accuracy: 0.4818\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4950 - accuracy: 0.7902 - val_loss: 0.7542 - val_accuracy: 0.4818\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4760 - accuracy: 0.7760 - val_loss: 0.7880 - val_accuracy: 0.4818\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8615 - val_loss: 0.8489 - val_accuracy: 0.4818\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4576 - accuracy: 0.7902 - val_loss: 0.8232 - val_accuracy: 0.5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15058cfa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_2 = Sequential()\n",
    "model_0_0_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_2.add(Flatten())\n",
    "model_0_0_2.add(Dense(50, activation='relu')) \n",
    "model_0_0_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_0_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_2.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d44d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 27ms/step - loss: 0.7025 - accuracy: 0.4786 - val_loss: 0.6871 - val_accuracy: 0.5547\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6926 - accuracy: 0.5275 - val_loss: 0.6909 - val_accuracy: 0.5474\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6846 - accuracy: 0.5540 - val_loss: 0.6938 - val_accuracy: 0.5255\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6807 - accuracy: 0.5601 - val_loss: 0.6906 - val_accuracy: 0.5109\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6789 - accuracy: 0.5723 - val_loss: 0.6967 - val_accuracy: 0.4964\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6639 - accuracy: 0.6232 - val_loss: 0.6889 - val_accuracy: 0.5547\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6547 - accuracy: 0.6415 - val_loss: 0.6946 - val_accuracy: 0.4672\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6480 - accuracy: 0.6517 - val_loss: 0.6967 - val_accuracy: 0.5474\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6229 - accuracy: 0.6762 - val_loss: 0.7223 - val_accuracy: 0.4599\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6167 - accuracy: 0.6864 - val_loss: 0.7056 - val_accuracy: 0.5620\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6046 - accuracy: 0.6802 - val_loss: 0.7218 - val_accuracy: 0.5182\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5700 - accuracy: 0.7576 - val_loss: 0.7367 - val_accuracy: 0.4745\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5501 - accuracy: 0.7597 - val_loss: 0.7483 - val_accuracy: 0.4891\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5339 - accuracy: 0.7475 - val_loss: 0.7935 - val_accuracy: 0.4672\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5174 - accuracy: 0.7475 - val_loss: 0.8301 - val_accuracy: 0.4599\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.5053 - accuracy: 0.7291 - val_loss: 0.7788 - val_accuracy: 0.4891\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4623 - accuracy: 0.8147 - val_loss: 0.8090 - val_accuracy: 0.4964\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4241 - accuracy: 0.8452 - val_loss: 0.8684 - val_accuracy: 0.5036\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3956 - accuracy: 0.8595 - val_loss: 0.8567 - val_accuracy: 0.5401\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3922 - accuracy: 0.8473 - val_loss: 0.8975 - val_accuracy: 0.4599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14f6aa400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_3 = Sequential()\n",
    "model_0_0_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_3.add(Flatten())\n",
    "model_0_0_3.add(Dense(50, activation='relu')) \n",
    "model_0_0_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_0_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_3.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79a0e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 18ms/step - loss: 0.6964 - accuracy: 0.4725 - val_loss: 0.6923 - val_accuracy: 0.4964\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6895 - accuracy: 0.5723 - val_loss: 0.6947 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5295 - val_loss: 0.6999 - val_accuracy: 0.4818\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.5112 - val_loss: 0.6955 - val_accuracy: 0.5182\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5173 - val_loss: 0.6953 - val_accuracy: 0.5036\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5193 - val_loss: 0.6957 - val_accuracy: 0.4307\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6899 - accuracy: 0.5499 - val_loss: 0.6954 - val_accuracy: 0.5109\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6882 - accuracy: 0.5764 - val_loss: 0.6936 - val_accuracy: 0.4745\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6808 - accuracy: 0.6049 - val_loss: 0.6953 - val_accuracy: 0.5182\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6784 - accuracy: 0.5499 - val_loss: 0.7059 - val_accuracy: 0.5182\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6775 - accuracy: 0.5723 - val_loss: 0.6988 - val_accuracy: 0.5036\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6552 - accuracy: 0.6212 - val_loss: 0.7075 - val_accuracy: 0.5109\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6627 - accuracy: 0.5927 - val_loss: 0.7548 - val_accuracy: 0.4891\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6609 - accuracy: 0.6090 - val_loss: 0.7181 - val_accuracy: 0.4745\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6304 - accuracy: 0.6354 - val_loss: 0.7425 - val_accuracy: 0.5474\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6212 - accuracy: 0.6477 - val_loss: 0.7248 - val_accuracy: 0.4818\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5871 - accuracy: 0.7088 - val_loss: 0.7335 - val_accuracy: 0.5182\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5596 - accuracy: 0.7312 - val_loss: 0.8398 - val_accuracy: 0.4745\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5377 - accuracy: 0.7189 - val_loss: 0.7648 - val_accuracy: 0.5036\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5565 - accuracy: 0.7149 - val_loss: 0.8020 - val_accuracy: 0.5255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x150b074f0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_4 = Sequential()\n",
    "model_0_0_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_0_4.add(Flatten())\n",
    "model_0_0_4.add(Dense(50, activation='relu')) \n",
    "model_0_0_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_0_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_4.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19af56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 20ms/step - loss: 0.6988 - accuracy: 0.4725 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.6935 - accuracy: 0.5214 - val_loss: 0.6917 - val_accuracy: 0.5109\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6912 - accuracy: 0.5295 - val_loss: 0.6930 - val_accuracy: 0.5182\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.5214 - val_loss: 0.6935 - val_accuracy: 0.4964\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6986 - accuracy: 0.4908 - val_loss: 0.6921 - val_accuracy: 0.5182\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6952 - accuracy: 0.5214 - val_loss: 0.6918 - val_accuracy: 0.5255\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6899 - accuracy: 0.5214 - val_loss: 0.6919 - val_accuracy: 0.5255\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6889 - accuracy: 0.5458 - val_loss: 0.6912 - val_accuracy: 0.5182\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6837 - accuracy: 0.5642 - val_loss: 0.6927 - val_accuracy: 0.5182\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6786 - accuracy: 0.5540 - val_loss: 0.6891 - val_accuracy: 0.5547\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6711 - accuracy: 0.6212 - val_loss: 0.6964 - val_accuracy: 0.5328\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6675 - accuracy: 0.5967 - val_loss: 0.6954 - val_accuracy: 0.5401\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6755 - accuracy: 0.5621 - val_loss: 0.6872 - val_accuracy: 0.5547\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6747 - accuracy: 0.5703 - val_loss: 0.7031 - val_accuracy: 0.5109\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6393 - accuracy: 0.6721 - val_loss: 0.7044 - val_accuracy: 0.5255\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6228 - accuracy: 0.6660 - val_loss: 0.7238 - val_accuracy: 0.5182\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6182 - accuracy: 0.6558 - val_loss: 0.7122 - val_accuracy: 0.5036\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5843 - accuracy: 0.7291 - val_loss: 0.7232 - val_accuracy: 0.5839\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5524 - accuracy: 0.7536 - val_loss: 0.7736 - val_accuracy: 0.5036\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5198 - accuracy: 0.7617 - val_loss: 0.8206 - val_accuracy: 0.5182\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5108 - accuracy: 0.7678 - val_loss: 0.8023 - val_accuracy: 0.4964\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5345 - accuracy: 0.7312 - val_loss: 0.8084 - val_accuracy: 0.4745\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4611 - accuracy: 0.7984 - val_loss: 0.8751 - val_accuracy: 0.5401\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4859 - accuracy: 0.7780 - val_loss: 0.8297 - val_accuracy: 0.5182\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4257 - accuracy: 0.8086 - val_loss: 0.8549 - val_accuracy: 0.5328\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4312 - accuracy: 0.8024 - val_loss: 0.9944 - val_accuracy: 0.5182\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.3826 - accuracy: 0.8452 - val_loss: 0.9352 - val_accuracy: 0.4891\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3409 - accuracy: 0.8880 - val_loss: 1.0354 - val_accuracy: 0.5255\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3154 - accuracy: 0.8859 - val_loss: 1.1269 - val_accuracy: 0.4672\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.3036 - accuracy: 0.8900 - val_loss: 1.1639 - val_accuracy: 0.4891\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.2831 - accuracy: 0.8839 - val_loss: 1.2322 - val_accuracy: 0.4599\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2572 - accuracy: 0.9104 - val_loss: 1.2542 - val_accuracy: 0.4891\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2291 - accuracy: 0.9267 - val_loss: 1.3448 - val_accuracy: 0.4964\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2050 - accuracy: 0.9328 - val_loss: 1.4701 - val_accuracy: 0.4453\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2207 - accuracy: 0.9287 - val_loss: 1.4029 - val_accuracy: 0.4672\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2032 - accuracy: 0.9267 - val_loss: 1.4734 - val_accuracy: 0.4672\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.9389 - val_loss: 1.5840 - val_accuracy: 0.4818\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.1306 - accuracy: 0.9695 - val_loss: 1.6917 - val_accuracy: 0.4672\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9776 - val_loss: 1.8649 - val_accuracy: 0.4672\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0923 - accuracy: 0.9817 - val_loss: 1.9830 - val_accuracy: 0.4672\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0864 - accuracy: 0.9776 - val_loss: 1.9772 - val_accuracy: 0.4745\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0935 - accuracy: 0.9776 - val_loss: 2.0085 - val_accuracy: 0.5036\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0905 - accuracy: 0.9715 - val_loss: 2.0693 - val_accuracy: 0.4818\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0720 - accuracy: 0.9857 - val_loss: 2.2601 - val_accuracy: 0.4891\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0623 - accuracy: 0.9857 - val_loss: 2.3771 - val_accuracy: 0.4672\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0599 - accuracy: 0.9857 - val_loss: 2.4921 - val_accuracy: 0.4526\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 2.4967 - val_accuracy: 0.4964\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0259 - accuracy: 0.9939 - val_loss: 2.6629 - val_accuracy: 0.4964\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 2.7292 - val_accuracy: 0.4745\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.8181 - val_accuracy: 0.4672\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.8702 - val_accuracy: 0.4818\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.9179 - val_accuracy: 0.4745\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 2.9706 - val_accuracy: 0.4745\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 3.0293 - val_accuracy: 0.4891\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 3.0629 - val_accuracy: 0.4672\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 3.1643 - val_accuracy: 0.4891\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 3.1569 - val_accuracy: 0.4818\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 3.1716 - val_accuracy: 0.4672\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 3.2631 - val_accuracy: 0.4745\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.2946 - val_accuracy: 0.4818\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 3.3123 - val_accuracy: 0.4818\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.3495 - val_accuracy: 0.4818\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.3803 - val_accuracy: 0.4891\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.3998 - val_accuracy: 0.4745\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.4382 - val_accuracy: 0.4818\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 3.4701 - val_accuracy: 0.4818\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 3.4913 - val_accuracy: 0.4818\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 3.5243 - val_accuracy: 0.4891\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 3.5710 - val_accuracy: 0.4891\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.5773 - val_accuracy: 0.4891\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.5931 - val_accuracy: 0.4818\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.6233 - val_accuracy: 0.4818\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 3.6585 - val_accuracy: 0.4891\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 3.6715 - val_accuracy: 0.4818\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.6912 - val_accuracy: 0.4818\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7225 - val_accuracy: 0.4891\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.7413 - val_accuracy: 0.4818\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7590 - val_accuracy: 0.4891\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.7701 - val_accuracy: 0.4891\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8010 - val_accuracy: 0.4891\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8044 - val_accuracy: 0.4891\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8350 - val_accuracy: 0.4891\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8646 - val_accuracy: 0.4891\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.4738e-04 - accuracy: 1.0000 - val_loss: 3.8724 - val_accuracy: 0.4891\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 9.6638e-04 - accuracy: 1.0000 - val_loss: 3.8978 - val_accuracy: 0.4891\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.1369e-04 - accuracy: 1.0000 - val_loss: 3.9083 - val_accuracy: 0.4891\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.5105e-04 - accuracy: 1.0000 - val_loss: 3.9315 - val_accuracy: 0.4891\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.3772e-04 - accuracy: 1.0000 - val_loss: 3.9458 - val_accuracy: 0.4891\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.9015e-04 - accuracy: 1.0000 - val_loss: 3.9596 - val_accuracy: 0.4891\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.7754e-04 - accuracy: 1.0000 - val_loss: 3.9689 - val_accuracy: 0.4891\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.4887e-04 - accuracy: 1.0000 - val_loss: 3.9971 - val_accuracy: 0.4891\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.3492e-04 - accuracy: 1.0000 - val_loss: 4.0086 - val_accuracy: 0.4891\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.8937e-04 - accuracy: 1.0000 - val_loss: 4.0354 - val_accuracy: 0.4891\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.7319e-04 - accuracy: 1.0000 - val_loss: 4.0472 - val_accuracy: 0.4891\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.4653e-04 - accuracy: 1.0000 - val_loss: 4.0589 - val_accuracy: 0.4891\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 6.3073e-04 - accuracy: 1.0000 - val_loss: 4.0781 - val_accuracy: 0.4891\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 6.1411e-04 - accuracy: 1.0000 - val_loss: 4.0903 - val_accuracy: 0.4891\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.9535e-04 - accuracy: 1.0000 - val_loss: 4.1063 - val_accuracy: 0.4891\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.8099e-04 - accuracy: 1.0000 - val_loss: 4.1217 - val_accuracy: 0.4891\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.6028e-04 - accuracy: 1.0000 - val_loss: 4.1392 - val_accuracy: 0.4891\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.4824e-04 - accuracy: 1.0000 - val_loss: 4.1440 - val_accuracy: 0.4891\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.2868e-04 - accuracy: 1.0000 - val_loss: 4.1696 - val_accuracy: 0.4891\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1477e-04 - accuracy: 1.0000 - val_loss: 4.1785 - val_accuracy: 0.4891\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.9940e-04 - accuracy: 1.0000 - val_loss: 4.1884 - val_accuracy: 0.4891\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.8953e-04 - accuracy: 1.0000 - val_loss: 4.2059 - val_accuracy: 0.4891\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.7839e-04 - accuracy: 1.0000 - val_loss: 4.2257 - val_accuracy: 0.4891\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.5839e-04 - accuracy: 1.0000 - val_loss: 4.2327 - val_accuracy: 0.4891\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.5280e-04 - accuracy: 1.0000 - val_loss: 4.2474 - val_accuracy: 0.4891\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.3315e-04 - accuracy: 1.0000 - val_loss: 4.2603 - val_accuracy: 0.4891\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.2229e-04 - accuracy: 1.0000 - val_loss: 4.2735 - val_accuracy: 0.4891\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.1581e-04 - accuracy: 1.0000 - val_loss: 4.2856 - val_accuracy: 0.4891\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.0403e-04 - accuracy: 1.0000 - val_loss: 4.2988 - val_accuracy: 0.4891\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.9982e-04 - accuracy: 1.0000 - val_loss: 4.3115 - val_accuracy: 0.4891\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.8408e-04 - accuracy: 1.0000 - val_loss: 4.3321 - val_accuracy: 0.4891\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.7691e-04 - accuracy: 1.0000 - val_loss: 4.3359 - val_accuracy: 0.4891\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.6674e-04 - accuracy: 1.0000 - val_loss: 4.3538 - val_accuracy: 0.4891\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.5664e-04 - accuracy: 1.0000 - val_loss: 4.3595 - val_accuracy: 0.4891\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.4651e-04 - accuracy: 1.0000 - val_loss: 4.3749 - val_accuracy: 0.4891\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.3893e-04 - accuracy: 1.0000 - val_loss: 4.3885 - val_accuracy: 0.4891\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.3503e-04 - accuracy: 1.0000 - val_loss: 4.4019 - val_accuracy: 0.4891\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.2783e-04 - accuracy: 1.0000 - val_loss: 4.4111 - val_accuracy: 0.4891\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.2123e-04 - accuracy: 1.0000 - val_loss: 4.4181 - val_accuracy: 0.4891\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.1336e-04 - accuracy: 1.0000 - val_loss: 4.4352 - val_accuracy: 0.4891\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 3.0189e-04 - accuracy: 1.0000 - val_loss: 4.4430 - val_accuracy: 0.4891\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.9614e-04 - accuracy: 1.0000 - val_loss: 4.4603 - val_accuracy: 0.4891\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.8996e-04 - accuracy: 1.0000 - val_loss: 4.4732 - val_accuracy: 0.4891\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.8242e-04 - accuracy: 1.0000 - val_loss: 4.4819 - val_accuracy: 0.4891\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.7771e-04 - accuracy: 1.0000 - val_loss: 4.4895 - val_accuracy: 0.4891\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.7099e-04 - accuracy: 1.0000 - val_loss: 4.5068 - val_accuracy: 0.4891\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.6759e-04 - accuracy: 1.0000 - val_loss: 4.5171 - val_accuracy: 0.4891\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.5791e-04 - accuracy: 1.0000 - val_loss: 4.5272 - val_accuracy: 0.4891\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.5326e-04 - accuracy: 1.0000 - val_loss: 4.5310 - val_accuracy: 0.4891\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.5070e-04 - accuracy: 1.0000 - val_loss: 4.5497 - val_accuracy: 0.4891\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.4481e-04 - accuracy: 1.0000 - val_loss: 4.5539 - val_accuracy: 0.4891\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.3532e-04 - accuracy: 1.0000 - val_loss: 4.5721 - val_accuracy: 0.4891\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.3273e-04 - accuracy: 1.0000 - val_loss: 4.5784 - val_accuracy: 0.4891\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.2736e-04 - accuracy: 1.0000 - val_loss: 4.5889 - val_accuracy: 0.4891\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.2304e-04 - accuracy: 1.0000 - val_loss: 4.6008 - val_accuracy: 0.4891\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.1981e-04 - accuracy: 1.0000 - val_loss: 4.6085 - val_accuracy: 0.4891\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.1386e-04 - accuracy: 1.0000 - val_loss: 4.6214 - val_accuracy: 0.4891\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.1082e-04 - accuracy: 1.0000 - val_loss: 4.6265 - val_accuracy: 0.4891\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.0668e-04 - accuracy: 1.0000 - val_loss: 4.6389 - val_accuracy: 0.4891\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.0283e-04 - accuracy: 1.0000 - val_loss: 4.6531 - val_accuracy: 0.4891\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.9874e-04 - accuracy: 1.0000 - val_loss: 4.6609 - val_accuracy: 0.4891\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.9391e-04 - accuracy: 1.0000 - val_loss: 4.6705 - val_accuracy: 0.4891\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.9425e-04 - accuracy: 1.0000 - val_loss: 4.6730 - val_accuracy: 0.4818\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.8858e-04 - accuracy: 1.0000 - val_loss: 4.6946 - val_accuracy: 0.4891\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.8418e-04 - accuracy: 1.0000 - val_loss: 4.6923 - val_accuracy: 0.4818\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.7913e-04 - accuracy: 1.0000 - val_loss: 4.7060 - val_accuracy: 0.4891\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.7613e-04 - accuracy: 1.0000 - val_loss: 4.7154 - val_accuracy: 0.4818\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.7289e-04 - accuracy: 1.0000 - val_loss: 4.7278 - val_accuracy: 0.4818\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.6920e-04 - accuracy: 1.0000 - val_loss: 4.7370 - val_accuracy: 0.4818\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.6687e-04 - accuracy: 1.0000 - val_loss: 4.7407 - val_accuracy: 0.4818\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.6613e-04 - accuracy: 1.0000 - val_loss: 4.7566 - val_accuracy: 0.4818\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.6081e-04 - accuracy: 1.0000 - val_loss: 4.7687 - val_accuracy: 0.4891\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.5637e-04 - accuracy: 1.0000 - val_loss: 4.7717 - val_accuracy: 0.4818\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.5522e-04 - accuracy: 1.0000 - val_loss: 4.7859 - val_accuracy: 0.4818\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.5211e-04 - accuracy: 1.0000 - val_loss: 4.7931 - val_accuracy: 0.4818\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.4919e-04 - accuracy: 1.0000 - val_loss: 4.7995 - val_accuracy: 0.4818\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.4582e-04 - accuracy: 1.0000 - val_loss: 4.8133 - val_accuracy: 0.4818\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.4441e-04 - accuracy: 1.0000 - val_loss: 4.8205 - val_accuracy: 0.4818\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.4253e-04 - accuracy: 1.0000 - val_loss: 4.8296 - val_accuracy: 0.4818\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.3861e-04 - accuracy: 1.0000 - val_loss: 4.8437 - val_accuracy: 0.4818\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.3613e-04 - accuracy: 1.0000 - val_loss: 4.8506 - val_accuracy: 0.4818\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.3512e-04 - accuracy: 1.0000 - val_loss: 4.8577 - val_accuracy: 0.4818\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.3218e-04 - accuracy: 1.0000 - val_loss: 4.8707 - val_accuracy: 0.4818\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.2914e-04 - accuracy: 1.0000 - val_loss: 4.8743 - val_accuracy: 0.4818\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2690e-04 - accuracy: 1.0000 - val_loss: 4.8829 - val_accuracy: 0.4818\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2528e-04 - accuracy: 1.0000 - val_loss: 4.8910 - val_accuracy: 0.4818\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.2290e-04 - accuracy: 1.0000 - val_loss: 4.9004 - val_accuracy: 0.4818\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2096e-04 - accuracy: 1.0000 - val_loss: 4.9089 - val_accuracy: 0.4818\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1893e-04 - accuracy: 1.0000 - val_loss: 4.9219 - val_accuracy: 0.4818\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1740e-04 - accuracy: 1.0000 - val_loss: 4.9276 - val_accuracy: 0.4818\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1515e-04 - accuracy: 1.0000 - val_loss: 4.9334 - val_accuracy: 0.4818\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1368e-04 - accuracy: 1.0000 - val_loss: 4.9444 - val_accuracy: 0.4818\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1183e-04 - accuracy: 1.0000 - val_loss: 4.9508 - val_accuracy: 0.4818\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0996e-04 - accuracy: 1.0000 - val_loss: 4.9645 - val_accuracy: 0.4818\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0787e-04 - accuracy: 1.0000 - val_loss: 4.9718 - val_accuracy: 0.4818\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0624e-04 - accuracy: 1.0000 - val_loss: 4.9780 - val_accuracy: 0.4818\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.0384e-04 - accuracy: 1.0000 - val_loss: 4.9863 - val_accuracy: 0.4818\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.0261e-04 - accuracy: 1.0000 - val_loss: 4.9927 - val_accuracy: 0.4818\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.0092e-04 - accuracy: 1.0000 - val_loss: 5.0007 - val_accuracy: 0.4818\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 9.9451e-05 - accuracy: 1.0000 - val_loss: 5.0111 - val_accuracy: 0.4818\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.7730e-05 - accuracy: 1.0000 - val_loss: 5.0184 - val_accuracy: 0.4818\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.6874e-05 - accuracy: 1.0000 - val_loss: 5.0255 - val_accuracy: 0.4818\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.5129e-05 - accuracy: 1.0000 - val_loss: 5.0358 - val_accuracy: 0.4818\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.3364e-05 - accuracy: 1.0000 - val_loss: 5.0394 - val_accuracy: 0.4818\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.1994e-05 - accuracy: 1.0000 - val_loss: 5.0508 - val_accuracy: 0.4818\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.0172e-05 - accuracy: 1.0000 - val_loss: 5.0588 - val_accuracy: 0.4818\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.9556e-05 - accuracy: 1.0000 - val_loss: 5.0692 - val_accuracy: 0.4818\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 8.7890e-05 - accuracy: 1.0000 - val_loss: 5.0750 - val_accuracy: 0.4818\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.6734e-05 - accuracy: 1.0000 - val_loss: 5.0852 - val_accuracy: 0.4818\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.5479e-05 - accuracy: 1.0000 - val_loss: 5.0923 - val_accuracy: 0.4818\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.3362e-05 - accuracy: 1.0000 - val_loss: 5.1017 - val_accuracy: 0.4818\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.2292e-05 - accuracy: 1.0000 - val_loss: 5.1085 - val_accuracy: 0.4818\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 8.1389e-05 - accuracy: 1.0000 - val_loss: 5.1170 - val_accuracy: 0.4818\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.9651e-05 - accuracy: 1.0000 - val_loss: 5.1223 - val_accuracy: 0.4818\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.8810e-05 - accuracy: 1.0000 - val_loss: 5.1306 - val_accuracy: 0.4818\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 7.7879e-05 - accuracy: 1.0000 - val_loss: 5.1412 - val_accuracy: 0.4818\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.7472e-05 - accuracy: 1.0000 - val_loss: 5.1453 - val_accuracy: 0.4818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x151e44fa0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_5 = Sequential()\n",
    "model_0_0_5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_5.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_0_5.add(Flatten())\n",
    "model_0_0_5.add(Dense(50, activation='relu')) \n",
    "model_0_0_5.add(Dense(1, activation='sigmoid')) \n",
    "model_0_0_5.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_5.fit(train_X_0_0,train_f0t_tc,epochs=200,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614b3b8",
   "metadata": {},
   "source": [
    "Model_0_0_3 appears to be an initial best using all the data. Will use this one with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0ade9",
   "metadata": {},
   "source": [
    "### Model_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f47487ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_1 final data prep \n",
    "\n",
    "# PCA with 23 components to explain 85% of variance\n",
    "sklearn_pca = PCA(n_components=23)\n",
    "train_X_0_1 = pd.DataFrame(sklearn_pca.fit_transform(train_f0))\n",
    "val_X_0_1 = pd.DataFrame(sklearn_pca.transform(val_f0))\n",
    "test_X_0_1 = pd.DataFrame(sklearn_pca.transform(test_f0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c7c40f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.219217</td>\n",
       "      <td>-1.462922</td>\n",
       "      <td>1.178301</td>\n",
       "      <td>-1.158433</td>\n",
       "      <td>-0.310762</td>\n",
       "      <td>-0.971497</td>\n",
       "      <td>-0.755831</td>\n",
       "      <td>-0.230518</td>\n",
       "      <td>-0.494264</td>\n",
       "      <td>0.538112</td>\n",
       "      <td>-0.219431</td>\n",
       "      <td>0.110541</td>\n",
       "      <td>0.489966</td>\n",
       "      <td>0.258522</td>\n",
       "      <td>-0.183544</td>\n",
       "      <td>-0.996880</td>\n",
       "      <td>-0.325106</td>\n",
       "      <td>0.077066</td>\n",
       "      <td>0.605346</td>\n",
       "      <td>0.326053</td>\n",
       "      <td>-0.370117</td>\n",
       "      <td>-0.036833</td>\n",
       "      <td>-0.773960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.074831</td>\n",
       "      <td>-0.975247</td>\n",
       "      <td>-1.117501</td>\n",
       "      <td>1.467582</td>\n",
       "      <td>0.600116</td>\n",
       "      <td>0.752940</td>\n",
       "      <td>-0.501049</td>\n",
       "      <td>0.335687</td>\n",
       "      <td>-0.131784</td>\n",
       "      <td>0.433012</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>-0.671462</td>\n",
       "      <td>0.304404</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.698974</td>\n",
       "      <td>0.178367</td>\n",
       "      <td>0.673246</td>\n",
       "      <td>-0.525624</td>\n",
       "      <td>-0.297949</td>\n",
       "      <td>0.272397</td>\n",
       "      <td>-0.351474</td>\n",
       "      <td>0.338810</td>\n",
       "      <td>0.609654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.709315</td>\n",
       "      <td>-1.138903</td>\n",
       "      <td>-1.021033</td>\n",
       "      <td>-0.682275</td>\n",
       "      <td>-0.695543</td>\n",
       "      <td>0.764778</td>\n",
       "      <td>-0.214868</td>\n",
       "      <td>-0.390358</td>\n",
       "      <td>0.666524</td>\n",
       "      <td>-0.413437</td>\n",
       "      <td>-0.139482</td>\n",
       "      <td>-0.195169</td>\n",
       "      <td>-1.277626</td>\n",
       "      <td>0.280918</td>\n",
       "      <td>-0.169060</td>\n",
       "      <td>0.222243</td>\n",
       "      <td>-0.186811</td>\n",
       "      <td>0.107063</td>\n",
       "      <td>-0.120785</td>\n",
       "      <td>-0.109009</td>\n",
       "      <td>-0.166495</td>\n",
       "      <td>0.097733</td>\n",
       "      <td>-0.190754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.663753</td>\n",
       "      <td>-0.627343</td>\n",
       "      <td>0.902401</td>\n",
       "      <td>0.203277</td>\n",
       "      <td>-1.282240</td>\n",
       "      <td>0.595241</td>\n",
       "      <td>-0.127063</td>\n",
       "      <td>-0.537176</td>\n",
       "      <td>-0.238251</td>\n",
       "      <td>0.242232</td>\n",
       "      <td>0.642603</td>\n",
       "      <td>-0.382570</td>\n",
       "      <td>-0.783190</td>\n",
       "      <td>0.615173</td>\n",
       "      <td>-0.315056</td>\n",
       "      <td>-0.221828</td>\n",
       "      <td>0.419174</td>\n",
       "      <td>-0.571787</td>\n",
       "      <td>-0.581266</td>\n",
       "      <td>0.398642</td>\n",
       "      <td>-0.176412</td>\n",
       "      <td>-0.606962</td>\n",
       "      <td>0.271133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.700241</td>\n",
       "      <td>-0.873808</td>\n",
       "      <td>-0.584979</td>\n",
       "      <td>1.089007</td>\n",
       "      <td>0.456954</td>\n",
       "      <td>1.263407</td>\n",
       "      <td>1.250353</td>\n",
       "      <td>-0.869443</td>\n",
       "      <td>0.494251</td>\n",
       "      <td>-0.086577</td>\n",
       "      <td>0.916465</td>\n",
       "      <td>0.613792</td>\n",
       "      <td>-0.102432</td>\n",
       "      <td>0.155376</td>\n",
       "      <td>-0.608668</td>\n",
       "      <td>-0.055980</td>\n",
       "      <td>-0.542087</td>\n",
       "      <td>0.043258</td>\n",
       "      <td>0.271773</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>0.263052</td>\n",
       "      <td>0.130627</td>\n",
       "      <td>-0.436842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.215505</td>\n",
       "      <td>1.415952</td>\n",
       "      <td>1.284548</td>\n",
       "      <td>-1.906232</td>\n",
       "      <td>1.640753</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>-0.619236</td>\n",
       "      <td>-0.205285</td>\n",
       "      <td>0.207232</td>\n",
       "      <td>-0.327709</td>\n",
       "      <td>-0.513953</td>\n",
       "      <td>-0.226027</td>\n",
       "      <td>0.608667</td>\n",
       "      <td>-0.341245</td>\n",
       "      <td>0.302062</td>\n",
       "      <td>-0.176278</td>\n",
       "      <td>0.025857</td>\n",
       "      <td>-0.183068</td>\n",
       "      <td>0.119498</td>\n",
       "      <td>-0.056718</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>0.425254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-1.855585</td>\n",
       "      <td>1.815472</td>\n",
       "      <td>1.542833</td>\n",
       "      <td>-0.925159</td>\n",
       "      <td>0.407913</td>\n",
       "      <td>-0.316808</td>\n",
       "      <td>0.891505</td>\n",
       "      <td>-0.587438</td>\n",
       "      <td>-0.011707</td>\n",
       "      <td>-0.090071</td>\n",
       "      <td>0.917244</td>\n",
       "      <td>-0.334305</td>\n",
       "      <td>-0.091007</td>\n",
       "      <td>-0.375539</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>-0.060973</td>\n",
       "      <td>0.260019</td>\n",
       "      <td>-0.619008</td>\n",
       "      <td>0.185277</td>\n",
       "      <td>-0.507739</td>\n",
       "      <td>0.072617</td>\n",
       "      <td>-0.163648</td>\n",
       "      <td>-0.072349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.344175</td>\n",
       "      <td>2.714250</td>\n",
       "      <td>-0.050140</td>\n",
       "      <td>1.376521</td>\n",
       "      <td>-0.163887</td>\n",
       "      <td>0.344485</td>\n",
       "      <td>1.132100</td>\n",
       "      <td>0.093943</td>\n",
       "      <td>1.117169</td>\n",
       "      <td>0.132252</td>\n",
       "      <td>-0.223988</td>\n",
       "      <td>0.390860</td>\n",
       "      <td>-0.403981</td>\n",
       "      <td>-0.299661</td>\n",
       "      <td>0.802703</td>\n",
       "      <td>-0.494383</td>\n",
       "      <td>0.226498</td>\n",
       "      <td>-0.423929</td>\n",
       "      <td>-0.091523</td>\n",
       "      <td>0.236133</td>\n",
       "      <td>-0.281697</td>\n",
       "      <td>0.132193</td>\n",
       "      <td>-0.145807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1.359647</td>\n",
       "      <td>2.496757</td>\n",
       "      <td>-0.654457</td>\n",
       "      <td>-0.348960</td>\n",
       "      <td>-0.545734</td>\n",
       "      <td>0.588113</td>\n",
       "      <td>0.224513</td>\n",
       "      <td>0.563510</td>\n",
       "      <td>1.070476</td>\n",
       "      <td>0.663969</td>\n",
       "      <td>-0.192287</td>\n",
       "      <td>0.316345</td>\n",
       "      <td>0.036057</td>\n",
       "      <td>-0.203978</td>\n",
       "      <td>0.410482</td>\n",
       "      <td>-0.595080</td>\n",
       "      <td>0.101182</td>\n",
       "      <td>-0.634113</td>\n",
       "      <td>-0.960526</td>\n",
       "      <td>-0.359390</td>\n",
       "      <td>0.367505</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.150783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.151745</td>\n",
       "      <td>2.187918</td>\n",
       "      <td>-0.498737</td>\n",
       "      <td>-0.924619</td>\n",
       "      <td>0.119817</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>-0.762025</td>\n",
       "      <td>0.464947</td>\n",
       "      <td>0.333831</td>\n",
       "      <td>0.880404</td>\n",
       "      <td>0.345708</td>\n",
       "      <td>-0.569200</td>\n",
       "      <td>-0.029051</td>\n",
       "      <td>0.585565</td>\n",
       "      <td>0.245967</td>\n",
       "      <td>-0.217486</td>\n",
       "      <td>-0.136106</td>\n",
       "      <td>0.672648</td>\n",
       "      <td>-0.881246</td>\n",
       "      <td>0.286066</td>\n",
       "      <td>-0.545320</td>\n",
       "      <td>-0.169566</td>\n",
       "      <td>-0.138191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.219217 -1.462922  1.178301 -1.158433 -0.310762 -0.971497 -0.755831   \n",
       "1   -1.074831 -0.975247 -1.117501  1.467582  0.600116  0.752940 -0.501049   \n",
       "2   -1.709315 -1.138903 -1.021033 -0.682275 -0.695543  0.764778 -0.214868   \n",
       "3    1.663753 -0.627343  0.902401  0.203277 -1.282240  0.595241 -0.127063   \n",
       "4   -0.700241 -0.873808 -0.584979  1.089007  0.456954  1.263407  1.250353   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491  0.215505  1.415952  1.284548 -1.906232  1.640753  0.031301  0.003976   \n",
       "492 -1.855585  1.815472  1.542833 -0.925159  0.407913 -0.316808  0.891505   \n",
       "493 -0.344175  2.714250 -0.050140  1.376521 -0.163887  0.344485  1.132100   \n",
       "494  1.359647  2.496757 -0.654457 -0.348960 -0.545734  0.588113  0.224513   \n",
       "495  1.151745  2.187918 -0.498737 -0.924619  0.119817 -0.005517 -0.762025   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -0.230518 -0.494264  0.538112 -0.219431  0.110541  0.489966  0.258522   \n",
       "1    0.335687 -0.131784  0.433012  0.971562 -0.671462  0.304404  0.032035   \n",
       "2   -0.390358  0.666524 -0.413437 -0.139482 -0.195169 -1.277626  0.280918   \n",
       "3   -0.537176 -0.238251  0.242232  0.642603 -0.382570 -0.783190  0.615173   \n",
       "4   -0.869443  0.494251 -0.086577  0.916465  0.613792 -0.102432  0.155376   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491 -0.619236 -0.205285  0.207232 -0.327709 -0.513953 -0.226027  0.608667   \n",
       "492 -0.587438 -0.011707 -0.090071  0.917244 -0.334305 -0.091007 -0.375539   \n",
       "493  0.093943  1.117169  0.132252 -0.223988  0.390860 -0.403981 -0.299661   \n",
       "494  0.563510  1.070476  0.663969 -0.192287  0.316345  0.036057 -0.203978   \n",
       "495  0.464947  0.333831  0.880404  0.345708 -0.569200 -0.029051  0.585565   \n",
       "\n",
       "           14        15        16        17        18        19        20  \\\n",
       "0   -0.183544 -0.996880 -0.325106  0.077066  0.605346  0.326053 -0.370117   \n",
       "1    0.698974  0.178367  0.673246 -0.525624 -0.297949  0.272397 -0.351474   \n",
       "2   -0.169060  0.222243 -0.186811  0.107063 -0.120785 -0.109009 -0.166495   \n",
       "3   -0.315056 -0.221828  0.419174 -0.571787 -0.581266  0.398642 -0.176412   \n",
       "4   -0.608668 -0.055980 -0.542087  0.043258  0.271773 -0.561970  0.263052   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "491 -0.341245  0.302062 -0.176278  0.025857 -0.183068  0.119498 -0.056718   \n",
       "492  0.028622 -0.060973  0.260019 -0.619008  0.185277 -0.507739  0.072617   \n",
       "493  0.802703 -0.494383  0.226498 -0.423929 -0.091523  0.236133 -0.281697   \n",
       "494  0.410482 -0.595080  0.101182 -0.634113 -0.960526 -0.359390  0.367505   \n",
       "495  0.245967 -0.217486 -0.136106  0.672648 -0.881246  0.286066 -0.545320   \n",
       "\n",
       "           21        22  \n",
       "0   -0.036833 -0.773960  \n",
       "1    0.338810  0.609654  \n",
       "2    0.097733 -0.190754  \n",
       "3   -0.606962  0.271133  \n",
       "4    0.130627 -0.436842  \n",
       "..        ...       ...  \n",
       "491  0.063401  0.425254  \n",
       "492 -0.163648 -0.072349  \n",
       "493  0.132193 -0.145807  \n",
       "494  0.000487  0.150783  \n",
       "495 -0.169566 -0.138191  \n",
       "\n",
       "[496 rows x 23 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a07cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked function to handle numpy arrays\n",
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_np(df, window_size=5):\n",
    "  df_as_np = df.to_numpy()\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size):\n",
    "    row = [r for r in df_as_np[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = df_as_np[i+window_size][0] # the y will get thrown out, but for some reason the function refuses to work without it there\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f9b0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_1, _ = df_to_X_np(train_X_0_1)\n",
    "val_X_0_1, _ = df_to_X_np(val_X_0_1)\n",
    "test_X_0_1, _ = df_to_X_np(test_X_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "958cb837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 14ms/step - loss: 0.7046 - accuracy: 0.4990 - val_loss: 0.7015 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.5173 - val_loss: 0.6977 - val_accuracy: 0.5109\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6889 - accuracy: 0.5132 - val_loss: 0.6964 - val_accuracy: 0.5036\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.5438 - val_loss: 0.6977 - val_accuracy: 0.5036\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6787 - accuracy: 0.5580 - val_loss: 0.6995 - val_accuracy: 0.4818\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6746 - accuracy: 0.5825 - val_loss: 0.7008 - val_accuracy: 0.4964\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6713 - accuracy: 0.5764 - val_loss: 0.7007 - val_accuracy: 0.4891\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6676 - accuracy: 0.5927 - val_loss: 0.7042 - val_accuracy: 0.4453\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6644 - accuracy: 0.6151 - val_loss: 0.7066 - val_accuracy: 0.4672\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6611 - accuracy: 0.6090 - val_loss: 0.7062 - val_accuracy: 0.4599\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6573 - accuracy: 0.6334 - val_loss: 0.7091 - val_accuracy: 0.4599\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6535 - accuracy: 0.6456 - val_loss: 0.7118 - val_accuracy: 0.4453\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6501 - accuracy: 0.6456 - val_loss: 0.7153 - val_accuracy: 0.4672\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6465 - accuracy: 0.6558 - val_loss: 0.7181 - val_accuracy: 0.4672\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6425 - accuracy: 0.6599 - val_loss: 0.7197 - val_accuracy: 0.4891\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6386 - accuracy: 0.6578 - val_loss: 0.7215 - val_accuracy: 0.4745\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.6558 - val_loss: 0.7268 - val_accuracy: 0.4672\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6305 - accuracy: 0.6680 - val_loss: 0.7281 - val_accuracy: 0.4672\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6267 - accuracy: 0.6701 - val_loss: 0.7352 - val_accuracy: 0.4599\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6225 - accuracy: 0.6741 - val_loss: 0.7341 - val_accuracy: 0.4672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154930c40>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_1.shape[1]\n",
    "n_features = train_X_0_1.shape[2]\n",
    "\n",
    "model_0_1_1 = Sequential()\n",
    "model_0_1_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_1_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_1.add(Flatten())\n",
    "model_0_1_1.add(Dense(50, activation='relu')) \n",
    "model_0_1_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_1_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_1.fit(train_X_0_1, train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "55b67373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 24ms/step - loss: 0.7158 - accuracy: 0.4847 - val_loss: 0.6905 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6768 - accuracy: 0.5784 - val_loss: 0.6878 - val_accuracy: 0.5401\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6577 - accuracy: 0.6395 - val_loss: 0.6884 - val_accuracy: 0.5474\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6365 - accuracy: 0.6517 - val_loss: 0.6904 - val_accuracy: 0.5839\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6229 - accuracy: 0.7047 - val_loss: 0.6919 - val_accuracy: 0.5547\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5982 - accuracy: 0.7617 - val_loss: 0.6963 - val_accuracy: 0.5839\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5795 - accuracy: 0.7739 - val_loss: 0.6905 - val_accuracy: 0.6058\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5580 - accuracy: 0.7923 - val_loss: 0.7018 - val_accuracy: 0.5328\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5348 - accuracy: 0.8187 - val_loss: 0.6997 - val_accuracy: 0.5985\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5132 - accuracy: 0.8208 - val_loss: 0.7046 - val_accuracy: 0.5766\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4889 - accuracy: 0.8350 - val_loss: 0.7148 - val_accuracy: 0.5547\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4639 - accuracy: 0.8656 - val_loss: 0.7204 - val_accuracy: 0.5620\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4372 - accuracy: 0.8900 - val_loss: 0.7250 - val_accuracy: 0.5766\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4120 - accuracy: 0.9022 - val_loss: 0.7334 - val_accuracy: 0.5985\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3893 - accuracy: 0.9002 - val_loss: 0.7398 - val_accuracy: 0.5912\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3611 - accuracy: 0.9165 - val_loss: 0.7629 - val_accuracy: 0.5036\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3349 - accuracy: 0.9267 - val_loss: 0.7706 - val_accuracy: 0.5912\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3132 - accuracy: 0.9246 - val_loss: 0.7970 - val_accuracy: 0.5912\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.2907 - accuracy: 0.9348 - val_loss: 0.8245 - val_accuracy: 0.5182\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2781 - accuracy: 0.9226 - val_loss: 0.8424 - val_accuracy: 0.5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154a5cbe0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_2 = Sequential()\n",
    "model_0_1_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_1_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_2.add(Flatten())\n",
    "model_0_1_2.add(Dense(50, activation='relu')) \n",
    "model_0_1_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_1_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_2.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "14b830b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.6960 - accuracy: 0.5051 - val_loss: 0.6929 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.5886 - val_loss: 0.6962 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.6619 - val_loss: 0.7003 - val_accuracy: 0.4964\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6515 - accuracy: 0.6945 - val_loss: 0.7056 - val_accuracy: 0.4891\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6371 - accuracy: 0.6884 - val_loss: 0.7131 - val_accuracy: 0.4818\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.7576 - val_loss: 0.7151 - val_accuracy: 0.4599\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5893 - accuracy: 0.7515 - val_loss: 0.7326 - val_accuracy: 0.4745\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5585 - accuracy: 0.7841 - val_loss: 0.7383 - val_accuracy: 0.4599\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5215 - accuracy: 0.8167 - val_loss: 0.7642 - val_accuracy: 0.4964\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4918 - accuracy: 0.8269 - val_loss: 0.7791 - val_accuracy: 0.5036\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.8432 - val_loss: 0.8156 - val_accuracy: 0.4964\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4032 - accuracy: 0.8635 - val_loss: 0.8320 - val_accuracy: 0.4599\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3549 - accuracy: 0.8982 - val_loss: 0.8808 - val_accuracy: 0.4672\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3114 - accuracy: 0.9124 - val_loss: 0.9734 - val_accuracy: 0.4672\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2695 - accuracy: 0.9185 - val_loss: 1.0237 - val_accuracy: 0.4745\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2333 - accuracy: 0.9389 - val_loss: 1.0668 - val_accuracy: 0.4599\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1992 - accuracy: 0.9572 - val_loss: 1.1237 - val_accuracy: 0.4526\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1704 - accuracy: 0.9695 - val_loss: 1.1809 - val_accuracy: 0.4307\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9695 - val_loss: 1.2642 - val_accuracy: 0.4672\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1227 - accuracy: 0.9817 - val_loss: 1.3129 - val_accuracy: 0.4599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154bdacd0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_3 = Sequential()\n",
    "model_0_1_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_3.add(Flatten())\n",
    "model_0_1_3.add(Dense(50, activation='relu')) \n",
    "model_0_1_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_1_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_3.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "61279601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.6942 - accuracy: 0.4745 - val_loss: 0.6970 - val_accuracy: 0.4745\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6840 - accuracy: 0.6090 - val_loss: 0.6986 - val_accuracy: 0.4964\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6718 - accuracy: 0.6599 - val_loss: 0.7046 - val_accuracy: 0.4964\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6501 - accuracy: 0.7067 - val_loss: 0.7128 - val_accuracy: 0.4599\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6189 - accuracy: 0.7108 - val_loss: 0.7341 - val_accuracy: 0.4964\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5659 - accuracy: 0.7719 - val_loss: 0.7815 - val_accuracy: 0.4453\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5008 - accuracy: 0.7984 - val_loss: 0.8290 - val_accuracy: 0.4453\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4320 - accuracy: 0.8371 - val_loss: 0.9053 - val_accuracy: 0.4380\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.8615 - val_loss: 0.9291 - val_accuracy: 0.4161\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3276 - accuracy: 0.9002 - val_loss: 1.0377 - val_accuracy: 0.4818\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2789 - accuracy: 0.9022 - val_loss: 1.1122 - val_accuracy: 0.4745\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9511 - val_loss: 1.2099 - val_accuracy: 0.4672\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9654 - val_loss: 1.3669 - val_accuracy: 0.4599\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1487 - accuracy: 0.9633 - val_loss: 1.3673 - val_accuracy: 0.4526\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9817 - val_loss: 1.5907 - val_accuracy: 0.4672\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.9939 - val_loss: 1.7091 - val_accuracy: 0.4745\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9959 - val_loss: 1.8577 - val_accuracy: 0.4745\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9980 - val_loss: 1.9299 - val_accuracy: 0.4453\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.9980 - val_loss: 2.0359 - val_accuracy: 0.4526\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.9980 - val_loss: 2.1329 - val_accuracy: 0.4526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154d6c970>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_4 = Sequential()\n",
    "model_0_1_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_1_4.add(Flatten())\n",
    "model_0_1_4.add(Dense(50, activation='relu')) \n",
    "model_0_1_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_1_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_4.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "78b60c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.6982 - accuracy: 0.5153 - val_loss: 0.6937 - val_accuracy: 0.5182\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6849 - accuracy: 0.5519 - val_loss: 0.6953 - val_accuracy: 0.4964\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6750 - accuracy: 0.5540 - val_loss: 0.6980 - val_accuracy: 0.5036\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6585 - accuracy: 0.6090 - val_loss: 0.7048 - val_accuracy: 0.4380\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6361 - accuracy: 0.6701 - val_loss: 0.7268 - val_accuracy: 0.4818\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5817 - accuracy: 0.7495 - val_loss: 0.7476 - val_accuracy: 0.4891\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5224 - accuracy: 0.7597 - val_loss: 0.8030 - val_accuracy: 0.4891\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8167 - val_loss: 0.8673 - val_accuracy: 0.4818\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3672 - accuracy: 0.8513 - val_loss: 0.9364 - val_accuracy: 0.5109\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3047 - accuracy: 0.8880 - val_loss: 1.1414 - val_accuracy: 0.4745\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8921 - val_loss: 1.1745 - val_accuracy: 0.4745\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9165 - val_loss: 1.2123 - val_accuracy: 0.4964\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9409 - val_loss: 1.3605 - val_accuracy: 0.5036\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.1074 - accuracy: 0.9756 - val_loss: 1.4681 - val_accuracy: 0.4818\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0787 - accuracy: 0.9898 - val_loss: 1.6186 - val_accuracy: 0.4818\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0759 - accuracy: 0.9837 - val_loss: 1.6753 - val_accuracy: 0.5036\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9898 - val_loss: 1.9105 - val_accuracy: 0.4964\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.9959 - val_loss: 1.9069 - val_accuracy: 0.5182\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 2.0401 - val_accuracy: 0.5182\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9980 - val_loss: 2.1287 - val_accuracy: 0.4964\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.9959 - val_loss: 2.1712 - val_accuracy: 0.5036\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 2.2649 - val_accuracy: 0.5109\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.3561 - val_accuracy: 0.4964\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.4007 - val_accuracy: 0.4964\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 2.4659 - val_accuracy: 0.4891\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 2.5053 - val_accuracy: 0.5109\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.5679 - val_accuracy: 0.4891\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.6026 - val_accuracy: 0.4964\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.6432 - val_accuracy: 0.5036\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.6900 - val_accuracy: 0.5036\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.7344 - val_accuracy: 0.4964\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.7597 - val_accuracy: 0.5036\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.8000 - val_accuracy: 0.5036\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.8323 - val_accuracy: 0.5036\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 2.8670 - val_accuracy: 0.5036\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.8981 - val_accuracy: 0.4964\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.9277 - val_accuracy: 0.5036\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.9558 - val_accuracy: 0.5036\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9833 - val_accuracy: 0.5036\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.0126 - val_accuracy: 0.5036\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.0415 - val_accuracy: 0.5036\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.5468e-04 - accuracy: 1.0000 - val_loss: 3.0685 - val_accuracy: 0.5036\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.9598e-04 - accuracy: 1.0000 - val_loss: 3.0864 - val_accuracy: 0.5036\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.3805e-04 - accuracy: 1.0000 - val_loss: 3.1157 - val_accuracy: 0.5036\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.7075e-04 - accuracy: 1.0000 - val_loss: 3.1485 - val_accuracy: 0.5036\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 7.2595e-04 - accuracy: 1.0000 - val_loss: 3.1666 - val_accuracy: 0.5036\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.8964e-04 - accuracy: 1.0000 - val_loss: 3.1935 - val_accuracy: 0.5036\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.6154e-04 - accuracy: 1.0000 - val_loss: 3.2186 - val_accuracy: 0.5036\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.0770e-04 - accuracy: 1.0000 - val_loss: 3.2379 - val_accuracy: 0.5036\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.7368e-04 - accuracy: 1.0000 - val_loss: 3.2559 - val_accuracy: 0.5036\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 5.4404e-04 - accuracy: 1.0000 - val_loss: 3.2812 - val_accuracy: 0.5036\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 5.1315e-04 - accuracy: 1.0000 - val_loss: 3.2971 - val_accuracy: 0.5036\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.8598e-04 - accuracy: 1.0000 - val_loss: 3.3229 - val_accuracy: 0.5036\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 4.6249e-04 - accuracy: 1.0000 - val_loss: 3.3426 - val_accuracy: 0.5036\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3511e-04 - accuracy: 1.0000 - val_loss: 3.3648 - val_accuracy: 0.5036\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.1440e-04 - accuracy: 1.0000 - val_loss: 3.3794 - val_accuracy: 0.5036\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.9530e-04 - accuracy: 1.0000 - val_loss: 3.3987 - val_accuracy: 0.5036\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.7490e-04 - accuracy: 1.0000 - val_loss: 3.4168 - val_accuracy: 0.5036\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.5698e-04 - accuracy: 1.0000 - val_loss: 3.4411 - val_accuracy: 0.5109\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.4081e-04 - accuracy: 1.0000 - val_loss: 3.4619 - val_accuracy: 0.5109\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 3.2349e-04 - accuracy: 1.0000 - val_loss: 3.4759 - val_accuracy: 0.5036\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3.0843e-04 - accuracy: 1.0000 - val_loss: 3.4941 - val_accuracy: 0.5036\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2.9477e-04 - accuracy: 1.0000 - val_loss: 3.5097 - val_accuracy: 0.5036\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.8149e-04 - accuracy: 1.0000 - val_loss: 3.5305 - val_accuracy: 0.5036\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.6871e-04 - accuracy: 1.0000 - val_loss: 3.5479 - val_accuracy: 0.5036\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.5840e-04 - accuracy: 1.0000 - val_loss: 3.5658 - val_accuracy: 0.5036\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.4643e-04 - accuracy: 1.0000 - val_loss: 3.5844 - val_accuracy: 0.5036\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.3595e-04 - accuracy: 1.0000 - val_loss: 3.5959 - val_accuracy: 0.5036\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.2749e-04 - accuracy: 1.0000 - val_loss: 3.6146 - val_accuracy: 0.5036\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 2.1891e-04 - accuracy: 1.0000 - val_loss: 3.6270 - val_accuracy: 0.5036\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.0782e-04 - accuracy: 1.0000 - val_loss: 3.6473 - val_accuracy: 0.5036\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.9995e-04 - accuracy: 1.0000 - val_loss: 3.6571 - val_accuracy: 0.5036\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.9225e-04 - accuracy: 1.0000 - val_loss: 3.6745 - val_accuracy: 0.5036\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.8477e-04 - accuracy: 1.0000 - val_loss: 3.6913 - val_accuracy: 0.5036\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7636e-04 - accuracy: 1.0000 - val_loss: 3.6999 - val_accuracy: 0.5036\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7089e-04 - accuracy: 1.0000 - val_loss: 3.7177 - val_accuracy: 0.5036\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.6325e-04 - accuracy: 1.0000 - val_loss: 3.7303 - val_accuracy: 0.5036\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.5807e-04 - accuracy: 1.0000 - val_loss: 3.7459 - val_accuracy: 0.5036\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.5247e-04 - accuracy: 1.0000 - val_loss: 3.7622 - val_accuracy: 0.5036\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.4659e-04 - accuracy: 1.0000 - val_loss: 3.7773 - val_accuracy: 0.5036\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.4141e-04 - accuracy: 1.0000 - val_loss: 3.7898 - val_accuracy: 0.5036\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.3668e-04 - accuracy: 1.0000 - val_loss: 3.8032 - val_accuracy: 0.5036\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3155e-04 - accuracy: 1.0000 - val_loss: 3.8185 - val_accuracy: 0.5036\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.2699e-04 - accuracy: 1.0000 - val_loss: 3.8310 - val_accuracy: 0.5036\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.2316e-04 - accuracy: 1.0000 - val_loss: 3.8447 - val_accuracy: 0.5036\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.1883e-04 - accuracy: 1.0000 - val_loss: 3.8568 - val_accuracy: 0.5036\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.1507e-04 - accuracy: 1.0000 - val_loss: 3.8700 - val_accuracy: 0.5036\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.1097e-04 - accuracy: 1.0000 - val_loss: 3.8842 - val_accuracy: 0.5036\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.0776e-04 - accuracy: 1.0000 - val_loss: 3.8955 - val_accuracy: 0.5036\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.0395e-04 - accuracy: 1.0000 - val_loss: 3.9067 - val_accuracy: 0.5036\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 3.9176 - val_accuracy: 0.5036\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 9.7449e-05 - accuracy: 1.0000 - val_loss: 3.9306 - val_accuracy: 0.5036\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 9.4311e-05 - accuracy: 1.0000 - val_loss: 3.9401 - val_accuracy: 0.5036\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 9.1203e-05 - accuracy: 1.0000 - val_loss: 3.9535 - val_accuracy: 0.5036\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 8.8338e-05 - accuracy: 1.0000 - val_loss: 3.9644 - val_accuracy: 0.5036\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.5692e-05 - accuracy: 1.0000 - val_loss: 3.9746 - val_accuracy: 0.5036\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.3111e-05 - accuracy: 1.0000 - val_loss: 3.9871 - val_accuracy: 0.5036\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 8.0811e-05 - accuracy: 1.0000 - val_loss: 3.9978 - val_accuracy: 0.5036\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.8090e-05 - accuracy: 1.0000 - val_loss: 4.0121 - val_accuracy: 0.5036\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.5888e-05 - accuracy: 1.0000 - val_loss: 4.0220 - val_accuracy: 0.5036\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.3664e-05 - accuracy: 1.0000 - val_loss: 4.0372 - val_accuracy: 0.5036\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 7.1688e-05 - accuracy: 1.0000 - val_loss: 4.0469 - val_accuracy: 0.5036\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.9522e-05 - accuracy: 1.0000 - val_loss: 4.0591 - val_accuracy: 0.5036\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.7529e-05 - accuracy: 1.0000 - val_loss: 4.0698 - val_accuracy: 0.5036\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5639e-05 - accuracy: 1.0000 - val_loss: 4.0780 - val_accuracy: 0.5036\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4055e-05 - accuracy: 1.0000 - val_loss: 4.0883 - val_accuracy: 0.5036\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2032e-05 - accuracy: 1.0000 - val_loss: 4.1031 - val_accuracy: 0.5036\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 6.0592e-05 - accuracy: 1.0000 - val_loss: 4.1088 - val_accuracy: 0.5036\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.8787e-05 - accuracy: 1.0000 - val_loss: 4.1216 - val_accuracy: 0.5036\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.7238e-05 - accuracy: 1.0000 - val_loss: 4.1321 - val_accuracy: 0.5036\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.5691e-05 - accuracy: 1.0000 - val_loss: 4.1446 - val_accuracy: 0.5036\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 5.4342e-05 - accuracy: 1.0000 - val_loss: 4.1520 - val_accuracy: 0.5036\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 5.3378e-05 - accuracy: 1.0000 - val_loss: 4.1596 - val_accuracy: 0.5036\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.1525e-05 - accuracy: 1.0000 - val_loss: 4.1729 - val_accuracy: 0.5036\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 5.0236e-05 - accuracy: 1.0000 - val_loss: 4.1835 - val_accuracy: 0.5036\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.8933e-05 - accuracy: 1.0000 - val_loss: 4.1908 - val_accuracy: 0.5036\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.7835e-05 - accuracy: 1.0000 - val_loss: 4.2004 - val_accuracy: 0.5036\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.6553e-05 - accuracy: 1.0000 - val_loss: 4.2101 - val_accuracy: 0.5036\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.5336e-05 - accuracy: 1.0000 - val_loss: 4.2208 - val_accuracy: 0.5036\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.4310e-05 - accuracy: 1.0000 - val_loss: 4.2309 - val_accuracy: 0.5036\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.3211e-05 - accuracy: 1.0000 - val_loss: 4.2382 - val_accuracy: 0.5036\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.2230e-05 - accuracy: 1.0000 - val_loss: 4.2482 - val_accuracy: 0.5036\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 4.1167e-05 - accuracy: 1.0000 - val_loss: 4.2548 - val_accuracy: 0.5036\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 4.0153e-05 - accuracy: 1.0000 - val_loss: 4.2671 - val_accuracy: 0.5036\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.9282e-05 - accuracy: 1.0000 - val_loss: 4.2765 - val_accuracy: 0.5036\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.8511e-05 - accuracy: 1.0000 - val_loss: 4.2815 - val_accuracy: 0.5036\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7460e-05 - accuracy: 1.0000 - val_loss: 4.2925 - val_accuracy: 0.5036\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.6644e-05 - accuracy: 1.0000 - val_loss: 4.3039 - val_accuracy: 0.5036\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.5834e-05 - accuracy: 1.0000 - val_loss: 4.3106 - val_accuracy: 0.5036\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.4971e-05 - accuracy: 1.0000 - val_loss: 4.3216 - val_accuracy: 0.5036\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.4168e-05 - accuracy: 1.0000 - val_loss: 4.3292 - val_accuracy: 0.5036\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.3444e-05 - accuracy: 1.0000 - val_loss: 4.3360 - val_accuracy: 0.5036\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.2696e-05 - accuracy: 1.0000 - val_loss: 4.3438 - val_accuracy: 0.5036\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.1957e-05 - accuracy: 1.0000 - val_loss: 4.3526 - val_accuracy: 0.5036\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.1425e-05 - accuracy: 1.0000 - val_loss: 4.3582 - val_accuracy: 0.5036\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0638e-05 - accuracy: 1.0000 - val_loss: 4.3687 - val_accuracy: 0.4964\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.9976e-05 - accuracy: 1.0000 - val_loss: 4.3780 - val_accuracy: 0.4964\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 2.9339e-05 - accuracy: 1.0000 - val_loss: 4.3878 - val_accuracy: 0.4964\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.8767e-05 - accuracy: 1.0000 - val_loss: 4.3929 - val_accuracy: 0.5036\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.8206e-05 - accuracy: 1.0000 - val_loss: 4.4043 - val_accuracy: 0.4964\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.7683e-05 - accuracy: 1.0000 - val_loss: 4.4089 - val_accuracy: 0.5036\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.7023e-05 - accuracy: 1.0000 - val_loss: 4.4177 - val_accuracy: 0.5036\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.6367e-05 - accuracy: 1.0000 - val_loss: 4.4279 - val_accuracy: 0.4964\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.5803e-05 - accuracy: 1.0000 - val_loss: 4.4348 - val_accuracy: 0.4964\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.5377e-05 - accuracy: 1.0000 - val_loss: 4.4446 - val_accuracy: 0.4964\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.4806e-05 - accuracy: 1.0000 - val_loss: 4.4499 - val_accuracy: 0.4964\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.4267e-05 - accuracy: 1.0000 - val_loss: 4.4570 - val_accuracy: 0.4964\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.3806e-05 - accuracy: 1.0000 - val_loss: 4.4630 - val_accuracy: 0.4964\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 2.3311e-05 - accuracy: 1.0000 - val_loss: 4.4708 - val_accuracy: 0.4964\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2861e-05 - accuracy: 1.0000 - val_loss: 4.4811 - val_accuracy: 0.4964\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.2410e-05 - accuracy: 1.0000 - val_loss: 4.4876 - val_accuracy: 0.4964\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 2.1969e-05 - accuracy: 1.0000 - val_loss: 4.4943 - val_accuracy: 0.4964\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1568e-05 - accuracy: 1.0000 - val_loss: 4.5025 - val_accuracy: 0.4964\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1153e-05 - accuracy: 1.0000 - val_loss: 4.5095 - val_accuracy: 0.4964\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 2.0751e-05 - accuracy: 1.0000 - val_loss: 4.5164 - val_accuracy: 0.4964\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 2.0355e-05 - accuracy: 1.0000 - val_loss: 4.5216 - val_accuracy: 0.4964\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.9952e-05 - accuracy: 1.0000 - val_loss: 4.5296 - val_accuracy: 0.4964\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9576e-05 - accuracy: 1.0000 - val_loss: 4.5349 - val_accuracy: 0.4964\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9228e-05 - accuracy: 1.0000 - val_loss: 4.5427 - val_accuracy: 0.4964\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8828e-05 - accuracy: 1.0000 - val_loss: 4.5504 - val_accuracy: 0.4964\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 1.8546e-05 - accuracy: 1.0000 - val_loss: 4.5591 - val_accuracy: 0.4964\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.8189e-05 - accuracy: 1.0000 - val_loss: 4.5658 - val_accuracy: 0.4964\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.7809e-05 - accuracy: 1.0000 - val_loss: 4.5709 - val_accuracy: 0.4964\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.7474e-05 - accuracy: 1.0000 - val_loss: 4.5777 - val_accuracy: 0.4964\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.7164e-05 - accuracy: 1.0000 - val_loss: 4.5846 - val_accuracy: 0.4964\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.6874e-05 - accuracy: 1.0000 - val_loss: 4.5922 - val_accuracy: 0.4964\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.6566e-05 - accuracy: 1.0000 - val_loss: 4.5979 - val_accuracy: 0.4964\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.6309e-05 - accuracy: 1.0000 - val_loss: 4.6063 - val_accuracy: 0.4964\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.5985e-05 - accuracy: 1.0000 - val_loss: 4.6124 - val_accuracy: 0.4964\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.5716e-05 - accuracy: 1.0000 - val_loss: 4.6191 - val_accuracy: 0.4964\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.5444e-05 - accuracy: 1.0000 - val_loss: 4.6266 - val_accuracy: 0.4964\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.5188e-05 - accuracy: 1.0000 - val_loss: 4.6330 - val_accuracy: 0.4964\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.4896e-05 - accuracy: 1.0000 - val_loss: 4.6402 - val_accuracy: 0.4964\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.4642e-05 - accuracy: 1.0000 - val_loss: 4.6468 - val_accuracy: 0.4964\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.4382e-05 - accuracy: 1.0000 - val_loss: 4.6518 - val_accuracy: 0.4964\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.4142e-05 - accuracy: 1.0000 - val_loss: 4.6586 - val_accuracy: 0.4964\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.3899e-05 - accuracy: 1.0000 - val_loss: 4.6644 - val_accuracy: 0.4964\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3665e-05 - accuracy: 1.0000 - val_loss: 4.6700 - val_accuracy: 0.4964\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.3435e-05 - accuracy: 1.0000 - val_loss: 4.6779 - val_accuracy: 0.4964\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3228e-05 - accuracy: 1.0000 - val_loss: 4.6850 - val_accuracy: 0.4964\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2988e-05 - accuracy: 1.0000 - val_loss: 4.6905 - val_accuracy: 0.4964\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.2771e-05 - accuracy: 1.0000 - val_loss: 4.6974 - val_accuracy: 0.4964\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.2565e-05 - accuracy: 1.0000 - val_loss: 4.7036 - val_accuracy: 0.4964\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.2392e-05 - accuracy: 1.0000 - val_loss: 4.7071 - val_accuracy: 0.4964\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.2158e-05 - accuracy: 1.0000 - val_loss: 4.7135 - val_accuracy: 0.4964\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1949e-05 - accuracy: 1.0000 - val_loss: 4.7208 - val_accuracy: 0.4964\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.1749e-05 - accuracy: 1.0000 - val_loss: 4.7290 - val_accuracy: 0.4964\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1566e-05 - accuracy: 1.0000 - val_loss: 4.7340 - val_accuracy: 0.4964\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 1.1401e-05 - accuracy: 1.0000 - val_loss: 4.7387 - val_accuracy: 0.4964\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1198e-05 - accuracy: 1.0000 - val_loss: 4.7457 - val_accuracy: 0.4964\n",
      "Epoch 191/200\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.1048e-05 - accuracy: 1.0000 - val_loss: 4.7537 - val_accuracy: 0.4964\n",
      "Epoch 192/200\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 1.0854e-05 - accuracy: 1.0000 - val_loss: 4.7569 - val_accuracy: 0.4964\n",
      "Epoch 193/200\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0676e-05 - accuracy: 1.0000 - val_loss: 4.7640 - val_accuracy: 0.4964\n",
      "Epoch 194/200\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.0521e-05 - accuracy: 1.0000 - val_loss: 4.7708 - val_accuracy: 0.4964\n",
      "Epoch 195/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0337e-05 - accuracy: 1.0000 - val_loss: 4.7773 - val_accuracy: 0.4964\n",
      "Epoch 196/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.0193e-05 - accuracy: 1.0000 - val_loss: 4.7822 - val_accuracy: 0.4964\n",
      "Epoch 197/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.0026e-05 - accuracy: 1.0000 - val_loss: 4.7880 - val_accuracy: 0.4964\n",
      "Epoch 198/200\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 9.8683e-06 - accuracy: 1.0000 - val_loss: 4.7930 - val_accuracy: 0.4964\n",
      "Epoch 199/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.7075e-06 - accuracy: 1.0000 - val_loss: 4.7972 - val_accuracy: 0.4964\n",
      "Epoch 200/200\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 9.5663e-06 - accuracy: 1.0000 - val_loss: 4.8021 - val_accuracy: 0.4964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154f37b20>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_5 = Sequential()\n",
    "model_0_1_5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_5.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_1_5.add(Flatten())\n",
    "model_0_1_5.add(Dense(50, activation='relu')) \n",
    "model_0_1_5.add(Dense(1, activation='sigmoid')) \n",
    "model_0_1_5.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_5.fit(train_X_0_1,train_f0t_tc,epochs=200,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa754d7",
   "metadata": {},
   "source": [
    "### Model_0_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bda6210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_2, _ = df_to_X_y2(train_f0[feats],train_f0t)\n",
    "val_X_0_2, _ = df_to_X_y2(val_f0[feats], val_f0t)\n",
    "test_X_0_2, _ = df_to_X_y2(test_f0[feats],test_f0t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7bcfded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 23ms/step - loss: 0.6950 - accuracy: 0.5234 - val_loss: 0.6963 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5214 - val_loss: 0.6948 - val_accuracy: 0.5036\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6902 - accuracy: 0.5397 - val_loss: 0.6942 - val_accuracy: 0.4891\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5418 - val_loss: 0.6950 - val_accuracy: 0.4891\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6881 - accuracy: 0.5255 - val_loss: 0.6963 - val_accuracy: 0.4891\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6868 - accuracy: 0.5234 - val_loss: 0.6943 - val_accuracy: 0.4672\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.5397 - val_loss: 0.6941 - val_accuracy: 0.4891\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.5519 - val_loss: 0.6931 - val_accuracy: 0.5109\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5540 - val_loss: 0.6934 - val_accuracy: 0.4964\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6824 - accuracy: 0.5540 - val_loss: 0.6939 - val_accuracy: 0.4818\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6807 - accuracy: 0.5601 - val_loss: 0.6931 - val_accuracy: 0.5474\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5764 - val_loss: 0.6927 - val_accuracy: 0.5547\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.5906 - val_loss: 0.6923 - val_accuracy: 0.5401\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6772 - accuracy: 0.5947 - val_loss: 0.6935 - val_accuracy: 0.5401\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6756 - accuracy: 0.5784 - val_loss: 0.6936 - val_accuracy: 0.5401\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6745 - accuracy: 0.5967 - val_loss: 0.6949 - val_accuracy: 0.5328\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6749 - accuracy: 0.5682 - val_loss: 0.6938 - val_accuracy: 0.5474\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.6049 - val_loss: 0.6948 - val_accuracy: 0.5401\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6678 - accuracy: 0.6090 - val_loss: 0.6957 - val_accuracy: 0.5328\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6661 - accuracy: 0.6008 - val_loss: 0.6994 - val_accuracy: 0.5328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1550ac160>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_2.shape[1]\n",
    "n_features = train_X_0_2.shape[2]\n",
    "\n",
    "model_0_2_1 = Sequential()\n",
    "model_0_2_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_2_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_1.add(Flatten())\n",
    "model_0_2_1.add(Dense(50, activation='relu')) \n",
    "model_0_2_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_2_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_1.fit(train_X_0_2, train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d4bf6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.6992 - accuracy: 0.5010 - val_loss: 0.7035 - val_accuracy: 0.5255\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5703 - val_loss: 0.7091 - val_accuracy: 0.5328\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6798 - accuracy: 0.5764 - val_loss: 0.7078 - val_accuracy: 0.5255\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6710 - accuracy: 0.6212 - val_loss: 0.7092 - val_accuracy: 0.5474\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6651 - accuracy: 0.5927 - val_loss: 0.7086 - val_accuracy: 0.5036\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.6008 - val_loss: 0.7142 - val_accuracy: 0.5401\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6498 - accuracy: 0.6151 - val_loss: 0.7342 - val_accuracy: 0.5328\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6414 - accuracy: 0.6415 - val_loss: 0.7096 - val_accuracy: 0.4818\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6293 - accuracy: 0.6599 - val_loss: 0.7788 - val_accuracy: 0.5109\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6342 - accuracy: 0.6212 - val_loss: 0.7230 - val_accuracy: 0.5328\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6156 - accuracy: 0.6599 - val_loss: 0.7136 - val_accuracy: 0.4526\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6127 - accuracy: 0.6578 - val_loss: 0.7322 - val_accuracy: 0.5401\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5867 - accuracy: 0.7271 - val_loss: 0.7465 - val_accuracy: 0.5547\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5732 - accuracy: 0.7495 - val_loss: 0.7558 - val_accuracy: 0.5401\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5627 - accuracy: 0.7393 - val_loss: 0.7647 - val_accuracy: 0.5620\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5461 - accuracy: 0.7454 - val_loss: 0.7560 - val_accuracy: 0.5255\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5500 - accuracy: 0.7210 - val_loss: 0.7758 - val_accuracy: 0.5693\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.5192 - accuracy: 0.7556 - val_loss: 0.7495 - val_accuracy: 0.4891\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5082 - accuracy: 0.7841 - val_loss: 0.7834 - val_accuracy: 0.5182\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4824 - accuracy: 0.8045 - val_loss: 0.7981 - val_accuracy: 0.5401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155276fa0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_2 = Sequential()\n",
    "model_0_2_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_2_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_2.add(Flatten())\n",
    "model_0_2_2.add(Dense(50, activation='relu')) \n",
    "model_0_2_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_2_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_2.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "41c7a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 0.7028 - accuracy: 0.4990 - val_loss: 0.6938 - val_accuracy: 0.5255\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5193 - val_loss: 0.6949 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5479 - val_loss: 0.6939 - val_accuracy: 0.5474\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6859 - accuracy: 0.5540 - val_loss: 0.6947 - val_accuracy: 0.5255\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5825 - val_loss: 0.6951 - val_accuracy: 0.5255\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5519 - val_loss: 0.6950 - val_accuracy: 0.5328\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6786 - accuracy: 0.5866 - val_loss: 0.6986 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.5886 - val_loss: 0.6954 - val_accuracy: 0.5328\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.5825 - val_loss: 0.6933 - val_accuracy: 0.5182\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6585 - accuracy: 0.6436 - val_loss: 0.6977 - val_accuracy: 0.5547\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6495 - accuracy: 0.6680 - val_loss: 0.7078 - val_accuracy: 0.4599\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6516 - accuracy: 0.5967 - val_loss: 0.7049 - val_accuracy: 0.5109\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.5784 - val_loss: 0.6997 - val_accuracy: 0.5328\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6524 - accuracy: 0.6049 - val_loss: 0.7168 - val_accuracy: 0.5255\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6212 - accuracy: 0.6884 - val_loss: 0.7039 - val_accuracy: 0.5620\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6062 - accuracy: 0.6925 - val_loss: 0.7129 - val_accuracy: 0.5109\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6006 - accuracy: 0.6843 - val_loss: 0.7205 - val_accuracy: 0.4818\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5909 - accuracy: 0.6884 - val_loss: 0.7326 - val_accuracy: 0.5036\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5866 - accuracy: 0.7067 - val_loss: 0.7270 - val_accuracy: 0.4891\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7352 - val_loss: 0.7317 - val_accuracy: 0.5109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15430ad00>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_3 = Sequential()\n",
    "model_0_2_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_2_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_3.add(Flatten())\n",
    "model_0_2_3.add(Dense(50, activation='relu')) \n",
    "model_0_2_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_2_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_3.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5d0874fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 2s 47ms/step - loss: 0.6956 - accuracy: 0.4969 - val_loss: 0.6924 - val_accuracy: 0.5182\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5214 - val_loss: 0.6926 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5234 - val_loss: 0.6921 - val_accuracy: 0.5255\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5621 - val_loss: 0.6916 - val_accuracy: 0.5401\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6839 - accuracy: 0.5662 - val_loss: 0.6920 - val_accuracy: 0.5255\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6757 - accuracy: 0.5988 - val_loss: 0.6944 - val_accuracy: 0.5328\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6749 - accuracy: 0.5866 - val_loss: 0.7040 - val_accuracy: 0.5109\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6626 - accuracy: 0.6151 - val_loss: 0.6937 - val_accuracy: 0.4745\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6455 - accuracy: 0.6538 - val_loss: 0.7002 - val_accuracy: 0.4745\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6236 - accuracy: 0.6721 - val_loss: 0.7099 - val_accuracy: 0.5182\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6063 - accuracy: 0.6762 - val_loss: 0.7314 - val_accuracy: 0.4891\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5765 - accuracy: 0.7006 - val_loss: 0.7353 - val_accuracy: 0.5401\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5518 - accuracy: 0.6965 - val_loss: 0.7447 - val_accuracy: 0.5036\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5246 - accuracy: 0.7413 - val_loss: 0.7916 - val_accuracy: 0.5036\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5327 - accuracy: 0.7434 - val_loss: 0.7996 - val_accuracy: 0.5401\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4975 - accuracy: 0.7495 - val_loss: 0.8502 - val_accuracy: 0.5401\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4479 - accuracy: 0.7963 - val_loss: 0.8822 - val_accuracy: 0.4964\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4313 - accuracy: 0.7984 - val_loss: 0.8854 - val_accuracy: 0.4599\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3798 - accuracy: 0.8513 - val_loss: 0.9349 - val_accuracy: 0.5255\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3446 - accuracy: 0.8595 - val_loss: 0.9573 - val_accuracy: 0.5255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15570dcd0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_4 = Sequential()\n",
    "model_0_2_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_2_4.add(Flatten())\n",
    "model_0_2_4.add(Dense(50, activation='relu')) \n",
    "model_0_2_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_2_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_4.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb55cb",
   "metadata": {},
   "source": [
    "Model_0_2_4 best, and most consistent so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5aada2",
   "metadata": {},
   "source": [
    "### Model_0_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b367f40",
   "metadata": {},
   "source": [
    "# Model_0_1 final data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e15e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 23 components to explain 85% of variance\n",
    "sklearn_pca = PCA(n_components=8)\n",
    "train_X_0_3 = pd.DataFrame(sklearn_pca.fit_transform(train_f0[feats]))\n",
    "val_X_0_3 = pd.DataFrame(sklearn_pca.transform(val_f0[feats]))\n",
    "test_X_0_3 = pd.DataFrame(sklearn_pca.transform(test_f0[feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ab93c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_0_3, _ = df_to_X_np(train_X_0_3)\n",
    "val_X_0_3, _ = df_to_X_np(val_X_0_3)\n",
    "test_X_0_3, _ = df_to_X_np(test_X_0_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "252b2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.6962 - accuracy: 0.5214 - val_loss: 0.6982 - val_accuracy: 0.4599\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6910 - accuracy: 0.5193 - val_loss: 0.7000 - val_accuracy: 0.5474\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5377 - val_loss: 0.7001 - val_accuracy: 0.5109\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6882 - accuracy: 0.5356 - val_loss: 0.6999 - val_accuracy: 0.5109\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6865 - accuracy: 0.5397 - val_loss: 0.7002 - val_accuracy: 0.5036\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6851 - accuracy: 0.5377 - val_loss: 0.7005 - val_accuracy: 0.4964\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6840 - accuracy: 0.5397 - val_loss: 0.7010 - val_accuracy: 0.4891\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.5499 - val_loss: 0.7016 - val_accuracy: 0.5036\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6816 - accuracy: 0.5723 - val_loss: 0.7024 - val_accuracy: 0.5036\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6801 - accuracy: 0.5784 - val_loss: 0.7027 - val_accuracy: 0.5036\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6797 - accuracy: 0.5866 - val_loss: 0.7035 - val_accuracy: 0.5036\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6779 - accuracy: 0.5967 - val_loss: 0.7033 - val_accuracy: 0.4672\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6775 - accuracy: 0.5825 - val_loss: 0.7033 - val_accuracy: 0.4672\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6760 - accuracy: 0.5927 - val_loss: 0.7044 - val_accuracy: 0.4964\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6749 - accuracy: 0.5967 - val_loss: 0.7050 - val_accuracy: 0.4745\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6732 - accuracy: 0.6029 - val_loss: 0.7062 - val_accuracy: 0.4891\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6722 - accuracy: 0.6029 - val_loss: 0.7068 - val_accuracy: 0.4818\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6714 - accuracy: 0.6130 - val_loss: 0.7079 - val_accuracy: 0.4745\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6698 - accuracy: 0.6110 - val_loss: 0.7070 - val_accuracy: 0.4891\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6681 - accuracy: 0.6151 - val_loss: 0.7081 - val_accuracy: 0.4891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1568a4fa0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_3.shape[1]\n",
    "n_features = train_X_0_3.shape[2]\n",
    "\n",
    "model_0_3_1 = Sequential()\n",
    "model_0_3_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_3_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_1.add(Flatten())\n",
    "model_0_3_1.add(Dense(50, activation='relu')) \n",
    "model_0_3_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_3_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_1.fit(train_X_0_3, train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7327a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 0.7017 - accuracy: 0.4929 - val_loss: 0.6886 - val_accuracy: 0.5328\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6802 - accuracy: 0.5540 - val_loss: 0.6909 - val_accuracy: 0.5036\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6691 - accuracy: 0.5886 - val_loss: 0.6876 - val_accuracy: 0.5255\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6587 - accuracy: 0.6619 - val_loss: 0.6885 - val_accuracy: 0.5109\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6495 - accuracy: 0.6884 - val_loss: 0.6877 - val_accuracy: 0.5182\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6431 - accuracy: 0.6538 - val_loss: 0.6887 - val_accuracy: 0.5109\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6301 - accuracy: 0.7251 - val_loss: 0.6878 - val_accuracy: 0.5109\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6203 - accuracy: 0.7291 - val_loss: 0.6897 - val_accuracy: 0.5401\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6082 - accuracy: 0.7332 - val_loss: 0.6890 - val_accuracy: 0.4964\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5945 - accuracy: 0.7597 - val_loss: 0.6918 - val_accuracy: 0.5328\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5888 - accuracy: 0.7413 - val_loss: 0.6991 - val_accuracy: 0.4891\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5770 - accuracy: 0.7434 - val_loss: 0.6959 - val_accuracy: 0.5182\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5612 - accuracy: 0.7719 - val_loss: 0.6984 - val_accuracy: 0.5109\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5478 - accuracy: 0.7719 - val_loss: 0.6984 - val_accuracy: 0.5401\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.7658 - val_loss: 0.7034 - val_accuracy: 0.5547\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5173 - accuracy: 0.8024 - val_loss: 0.7074 - val_accuracy: 0.5255\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5045 - accuracy: 0.8086 - val_loss: 0.6982 - val_accuracy: 0.5328\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4871 - accuracy: 0.8330 - val_loss: 0.7171 - val_accuracy: 0.5109\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.4766 - accuracy: 0.8248 - val_loss: 0.7047 - val_accuracy: 0.5620\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.8391 - val_loss: 0.7037 - val_accuracy: 0.5474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156426d00>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_2 = Sequential()\n",
    "model_0_3_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_3_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_2.add(Flatten())\n",
    "model_0_3_2.add(Dense(50, activation='relu')) \n",
    "model_0_3_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_3_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_2.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3ca8a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 0.6994 - accuracy: 0.4501 - val_loss: 0.6913 - val_accuracy: 0.5401\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6872 - accuracy: 0.5255 - val_loss: 0.6907 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6815 - accuracy: 0.5642 - val_loss: 0.6906 - val_accuracy: 0.5109\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.6293 - val_loss: 0.6900 - val_accuracy: 0.5182\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6662 - accuracy: 0.6497 - val_loss: 0.6910 - val_accuracy: 0.5182\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6574 - accuracy: 0.6619 - val_loss: 0.6909 - val_accuracy: 0.5620\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6442 - accuracy: 0.6721 - val_loss: 0.6931 - val_accuracy: 0.5255\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6298 - accuracy: 0.6823 - val_loss: 0.6957 - val_accuracy: 0.4891\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6138 - accuracy: 0.7047 - val_loss: 0.7046 - val_accuracy: 0.5182\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5979 - accuracy: 0.6904 - val_loss: 0.7148 - val_accuracy: 0.4526\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5769 - accuracy: 0.7352 - val_loss: 0.7179 - val_accuracy: 0.5109\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.5520 - accuracy: 0.7393 - val_loss: 0.7263 - val_accuracy: 0.4891\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5225 - accuracy: 0.7617 - val_loss: 0.7441 - val_accuracy: 0.4453\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4910 - accuracy: 0.7902 - val_loss: 0.7485 - val_accuracy: 0.5109\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4618 - accuracy: 0.8065 - val_loss: 0.7735 - val_accuracy: 0.5182\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4401 - accuracy: 0.8248 - val_loss: 0.7938 - val_accuracy: 0.4526\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4128 - accuracy: 0.8473 - val_loss: 0.8118 - val_accuracy: 0.4307\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.3780 - accuracy: 0.8534 - val_loss: 0.8522 - val_accuracy: 0.5474\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3694 - accuracy: 0.8473 - val_loss: 0.9073 - val_accuracy: 0.5766\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.3378 - accuracy: 0.8819 - val_loss: 0.8998 - val_accuracy: 0.5547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156729fd0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_3 = Sequential()\n",
    "model_0_3_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_3_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_3.add(Flatten())\n",
    "model_0_3_3.add(Dense(50, activation='relu')) \n",
    "model_0_3_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_3_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_3.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b778221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 17ms/step - loss: 0.6938 - accuracy: 0.5275 - val_loss: 0.6917 - val_accuracy: 0.5328\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6848 - accuracy: 0.5519 - val_loss: 0.6909 - val_accuracy: 0.5182\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6765 - accuracy: 0.5723 - val_loss: 0.6878 - val_accuracy: 0.4964\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6620 - accuracy: 0.6415 - val_loss: 0.6847 - val_accuracy: 0.5401\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6401 - accuracy: 0.6680 - val_loss: 0.6877 - val_accuracy: 0.5109\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6120 - accuracy: 0.7006 - val_loss: 0.6953 - val_accuracy: 0.5401\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.6013 - accuracy: 0.6843 - val_loss: 0.7202 - val_accuracy: 0.5620\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5464 - accuracy: 0.7393 - val_loss: 0.6966 - val_accuracy: 0.5474\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4889 - accuracy: 0.8187 - val_loss: 0.7590 - val_accuracy: 0.4745\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4413 - accuracy: 0.8310 - val_loss: 0.7481 - val_accuracy: 0.5255\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.3741 - accuracy: 0.8778 - val_loss: 0.7694 - val_accuracy: 0.5109\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3098 - accuracy: 0.9104 - val_loss: 0.8297 - val_accuracy: 0.5547\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2739 - accuracy: 0.9165 - val_loss: 0.8608 - val_accuracy: 0.5766\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2104 - accuracy: 0.9430 - val_loss: 0.8955 - val_accuracy: 0.5547\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.1710 - accuracy: 0.9654 - val_loss: 0.9665 - val_accuracy: 0.5547\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.1345 - accuracy: 0.9695 - val_loss: 1.1143 - val_accuracy: 0.5547\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0998 - accuracy: 0.9878 - val_loss: 1.1922 - val_accuracy: 0.5255\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0852 - accuracy: 0.9796 - val_loss: 1.2850 - val_accuracy: 0.5547\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0632 - accuracy: 0.9939 - val_loss: 1.2911 - val_accuracy: 0.5401\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.0453 - accuracy: 0.9959 - val_loss: 1.3738 - val_accuracy: 0.5693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156a2e3d0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_4 = Sequential()\n",
    "model_0_3_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_3_4.add(Flatten())\n",
    "model_0_3_4.add(Dense(50, activation='relu')) \n",
    "model_0_3_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_3_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_4.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18708c",
   "metadata": {},
   "source": [
    "Generally the data from Model_0_3 has been the best. Keeping High NaN variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fc2b0",
   "metadata": {},
   "source": [
    "## Modeling with Ford_1, high NaN Vairables Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "24d92000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Features     Score\n",
      "30         Google_total  9.697339\n",
      "0                  Ford  7.392857\n",
      "59     Google_Disparity  6.904345\n",
      "62         Google_ROC_s  6.703454\n",
      "55    Google_Moment_1_s  6.703454\n",
      "56    Google_Moment_2_s  6.371212\n",
      "2         Ford Bronco_x  6.361728\n",
      "54      Google_Moment_2  5.181274\n",
      "58        Google_MAvg_s  4.651411\n",
      "69      Google_EMA_Move  4.573195\n",
      "53      Google_Moment_1  4.475422\n",
      "61           Google_ROC  4.475422\n",
      "64           Google_EMA  4.299831\n",
      "65          Google_diff  4.255284\n",
      "60   Google_Disparity_s  3.956600\n",
      "67     Google_MAvg_Move  3.270358\n",
      "80            Stock_ROC  3.180742\n",
      "72       Stock_Moment_1  3.180742\n",
      "3        Ford Mustang_x  3.119942\n",
      "18              Ford GT  2.737796\n",
      "57          Google_MAvg  2.602780\n",
      "91         Nas_Moment_1  2.536525\n",
      "99              Nas_ROC  2.536525\n",
      "110        Dow_Moment_1  2.524855\n",
      "118             Dow_ROC  2.524855\n",
      "73       Stock_Moment_2  2.190038\n",
      "86      Stock_MAvg_Move  2.124926\n",
      "111        Dow_Moment_2  2.012198\n",
      "83            Stock_EMA  2.003751\n",
      "121             Dow_EMA  1.935215\n",
      "76           Stock_MAvg  1.918076\n",
      "129            target_3  1.896191\n",
      "88       Stock_EMA_Move  1.868945\n",
      "92         Nas_Moment_2  1.846341\n",
      "114            Dow_MAvg  1.742268\n",
      "77         Stock_MAvg_s  1.719772\n",
      "7                   Low  1.712666\n",
      "115          Dow_MAvg_s  1.694505\n",
      "5                  Open  1.669424\n",
      "66          Google_Move  1.669346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  correlation_coefficient /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 40 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=40)\n",
    "best_fit = bestfeatures.fit(train_f1, train_f1t)\n",
    "best_scores = pd.DataFrame(best_fit.scores_)\n",
    "best_columns = pd.DataFrame(Ford_1.columns)\n",
    "\n",
    "# concatenate the dataframes for better visualization\n",
    "features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "print(features_score.nlargest(40, 'Score'))  # print the top 40 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "683d2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_1 = list(features_score.nlargest(40, 'Score')['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "048d7967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 16.98316578,  27.34516601,  36.70636765,  45.24164289,\n",
       "        52.98779207,  58.58736929,  62.65046152,  65.98630289,\n",
       "        69.00818095,  71.88462111,  74.30738492,  76.60388819,\n",
       "        78.32273091,  79.9721231 ,  81.42187884,  82.70002996,\n",
       "        83.92393946,  85.05094905,  86.12073991,  87.09685162,\n",
       "        88.03650244,  88.86005707,  89.66252715,  90.40823802,\n",
       "        91.07053522,  91.72757079,  92.35530657,  92.93604888,\n",
       "        93.50975167,  94.03841633,  94.51725671,  94.97270045,\n",
       "        95.41110121,  95.83705963,  96.23255204,  96.59667955,\n",
       "        96.90883224,  97.1921651 ,  97.4572665 ,  97.69354652,\n",
       "        97.91995976,  98.11848802,  98.31219318,  98.49686805,\n",
       "        98.66770333,  98.80566854,  98.93588768,  99.05649548,\n",
       "        99.15845596,  99.23906433,  99.31004431,  99.37373898,\n",
       "        99.43236925,  99.48218315,  99.5287526 ,  99.57210852,\n",
       "        99.61519771,  99.64945729,  99.67893466,  99.70614872,\n",
       "        99.7297222 ,  99.75282075,  99.77429121,  99.79276735,\n",
       "        99.81086682,  99.82859354,  99.84573123,  99.86136484,\n",
       "        99.8762125 ,  99.89024442,  99.90385013,  99.91669627,\n",
       "        99.92861041,  99.93931094,  99.94871753,  99.95540807,\n",
       "        99.96139753,  99.96693087,  99.97185998,  99.97610807,\n",
       "        99.97964956,  99.98254297,  99.98480729,  99.98673238,\n",
       "        99.98854193,  99.99012853,  99.9916836 ,  99.99308491,\n",
       "        99.99399188,  99.99483913,  99.99545786,  99.99606967,\n",
       "        99.99662533,  99.99712888,  99.99762604,  99.99807555,\n",
       "        99.99850341,  99.99879366,  99.9990365 ,  99.99923519,\n",
       "        99.9994001 ,  99.99952558,  99.99961632,  99.9997023 ,\n",
       "        99.99977828,  99.99984011,  99.9998914 ,  99.99993112,\n",
       "        99.99995247,  99.99997062,  99.99998825,  99.99999457,\n",
       "        99.9999986 , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        ])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA().fit(train_f1)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "# reach 85% variance explained with 18 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e186f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 25.42893388,  44.55868689,  57.15851333,  68.09462646,\n",
       "        76.42832011,  82.40706939,  86.34121795,  90.01691532,\n",
       "        92.49323818,  94.22049683,  95.48830206,  96.61452751,\n",
       "        97.53812002,  98.2507896 ,  98.88612876,  99.13023468,\n",
       "        99.34714087,  99.5377854 ,  99.70227494,  99.79522553,\n",
       "        99.84894844,  99.88436472,  99.91695221,  99.94736141,\n",
       "        99.96580457,  99.97551172,  99.98295633,  99.98690184,\n",
       "        99.99038945,  99.993438  ,  99.99602455,  99.99765392,\n",
       "        99.9990055 ,  99.99965228, 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA with 40 best components\n",
    "pca_1 = PCA().fit(train_f1[feats_1])\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca_1.explained_variance_ratio_) * 100\n",
    "# can use 7 principle compents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b2cc0",
   "metadata": {},
   "source": [
    "Keeping score: Will test 4 models with Ford_1 data variations,\n",
    "- Model_1_0 = All varaibles,\n",
    "- Model_1_1 = PCA All varaibles\n",
    "- Model_1_2 = 40 best k score\n",
    "- Model_1_3 = PCA of 40 Best K Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f78ca9",
   "metadata": {},
   "source": [
    "### Model_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6208918c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18 is out of bounds for axis 0 with size 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/v484103954dd6vtd4swk8hd40000gn/T/ipykernel_95818/140805083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1t_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1t_tc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1t_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jx/v484103954dd6vtd4swk8hd40000gn/T/ipykernel_95818/1494985560.py\u001b[0m in \u001b[0;36mdf_to_X_y2\u001b[0;34m(df, target, window_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_as_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# grabs row i and all rows above within the window size length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creates 3 dimentional array, (# obseravtions, # rows in window, # features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# pulls the target variable after the window, target varible needs to be column zero in this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns (N,) martix of targets i+window_length time periods away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18 is out of bounds for axis 0 with size 18"
     ]
    }
   ],
   "source": [
    "# Model_1_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_1_0, train_f1t_tc = df_to_X_y2(train_f1,train_f1t)\n",
    "val_X_1_0, val_f1t_tc= df_to_X_y2(val_f1, val_f1t)\n",
    "test_X_1_0, test_f1t_tc = df_to_X_y2(test_f1,test_f1t) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

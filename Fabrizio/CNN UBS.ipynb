{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515730c1-45ed-49e9-86b8-dbd07f2dbd0e",
   "metadata": {},
   "source": [
    "# CNN Model using full UBS data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23263a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, InputLayer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, model_selection\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb74ca8-4ea0-46a4-afa0-b61173feef12",
   "metadata": {},
   "source": [
    "## Part 1: Clean and Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e760be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb5de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UBS_x</th>\n",
       "      <th>UBS Financial Services Inc.</th>\n",
       "      <th>UBS Investment Bank</th>\n",
       "      <th>UBS Global Wealth Management</th>\n",
       "      <th>UBS Asset Management</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>UBS_y</th>\n",
       "      <th>Union Bank of Switzerland</th>\n",
       "      <th>UBS tax evasion controversies</th>\n",
       "      <th>Banking in Switzerland</th>\n",
       "      <th>dow_open</th>\n",
       "      <th>dow_high</th>\n",
       "      <th>dow_low</th>\n",
       "      <th>dow_close</th>\n",
       "      <th>dow_vol</th>\n",
       "      <th>nas_open</th>\n",
       "      <th>nas_high</th>\n",
       "      <th>nas_low</th>\n",
       "      <th>nas_close</th>\n",
       "      <th>nas_vol</th>\n",
       "      <th>Wiki_total</th>\n",
       "      <th>Google_total</th>\n",
       "      <th>Stock_total</th>\n",
       "      <th>Nas_total</th>\n",
       "      <th>Dow_total</th>\n",
       "      <th>Wiki_Moment_1</th>\n",
       "      <th>Wiki_Moment_2</th>\n",
       "      <th>Wiki_Moment_1_s</th>\n",
       "      <th>Wiki_Moment_2_s</th>\n",
       "      <th>Wiki_MAvg</th>\n",
       "      <th>Wiki_MAvg_s</th>\n",
       "      <th>Wiki_MAvg_s_5</th>\n",
       "      <th>Wiki_MAvg_s_6</th>\n",
       "      <th>Wiki_Disparity</th>\n",
       "      <th>Wiki_Disparity_s</th>\n",
       "      <th>Wiki_Disparity_s_5</th>\n",
       "      <th>Wiki_ROC</th>\n",
       "      <th>Wiki_ROC_s</th>\n",
       "      <th>Wiki_Rocp</th>\n",
       "      <th>Wiki_EMA</th>\n",
       "      <th>Wiki_EMA_5</th>\n",
       "      <th>Wiki_diff</th>\n",
       "      <th>Wiki_gain</th>\n",
       "      <th>Wiki_loss</th>\n",
       "      <th>Wiki_avg_gain</th>\n",
       "      <th>Wiki_avg_loss</th>\n",
       "      <th>Wiki_rs</th>\n",
       "      <th>Wiki_RSI</th>\n",
       "      <th>Stoch_Oscillator_3</th>\n",
       "      <th>Stoch_Oscillator_14</th>\n",
       "      <th>Change_Close</th>\n",
       "      <th>Change_Google</th>\n",
       "      <th>Wiki_Move</th>\n",
       "      <th>Wiki_MAvg_Move</th>\n",
       "      <th>Wiki_MAvg_s_Move</th>\n",
       "      <th>Wiki_ROC_Move</th>\n",
       "      <th>Wiki_EMA_Move</th>\n",
       "      <th>Wiki_EMA_Move_5</th>\n",
       "      <th>Wiki_Disparity_Move</th>\n",
       "      <th>Wiki_Disparity_s_Move</th>\n",
       "      <th>Wiki_RSI_Move</th>\n",
       "      <th>Google_Moment_1</th>\n",
       "      <th>Google_Moment_2</th>\n",
       "      <th>Google_Moment_1_s</th>\n",
       "      <th>Google_Moment_2_s</th>\n",
       "      <th>Google_MAvg</th>\n",
       "      <th>Google_MAvg_s</th>\n",
       "      <th>Google_MAvg_s_5</th>\n",
       "      <th>Google_MAvg_s_6</th>\n",
       "      <th>Google_Disparity</th>\n",
       "      <th>Google_Disparity_s</th>\n",
       "      <th>Google_Disparity_s_5</th>\n",
       "      <th>Google_ROC</th>\n",
       "      <th>Google_ROC_s</th>\n",
       "      <th>Google_Rocp</th>\n",
       "      <th>Google_EMA</th>\n",
       "      <th>Google_EMA_5</th>\n",
       "      <th>Google_diff</th>\n",
       "      <th>Google_gain</th>\n",
       "      <th>Google_loss</th>\n",
       "      <th>Google_avg_gain</th>\n",
       "      <th>Google_avg_loss</th>\n",
       "      <th>Google_rs</th>\n",
       "      <th>Google_RSI</th>\n",
       "      <th>Google_Move</th>\n",
       "      <th>Google_MAvg_Move</th>\n",
       "      <th>Google_MAvg_s_Move</th>\n",
       "      <th>Google_ROC_Move</th>\n",
       "      <th>Google_EMA_Move</th>\n",
       "      <th>Google_EMA_Move_5</th>\n",
       "      <th>Google_Disparity_Move</th>\n",
       "      <th>Google_Disparity_s_Move</th>\n",
       "      <th>Google_RSI_Move</th>\n",
       "      <th>Stock_Moment_1</th>\n",
       "      <th>Stock_Moment_2</th>\n",
       "      <th>Stock_Moment_1_s</th>\n",
       "      <th>Stock_Moment_2_s</th>\n",
       "      <th>Stock_MAvg</th>\n",
       "      <th>Stock_MAvg_s</th>\n",
       "      <th>Stock_MAvg_s_5</th>\n",
       "      <th>Stock_MAvg_s_6</th>\n",
       "      <th>Stock_Disparity</th>\n",
       "      <th>Stock_Disparity_s</th>\n",
       "      <th>Stock_Disparity_s_5</th>\n",
       "      <th>Stock_ROC</th>\n",
       "      <th>Stock_ROC_s</th>\n",
       "      <th>Stock_Rocp</th>\n",
       "      <th>Stock_EMA</th>\n",
       "      <th>Stock_EMA_5</th>\n",
       "      <th>Stock_diff</th>\n",
       "      <th>Stock_gain</th>\n",
       "      <th>Stock_loss</th>\n",
       "      <th>Stock_avg_gain</th>\n",
       "      <th>Stock_avg_loss</th>\n",
       "      <th>Stock_rs</th>\n",
       "      <th>Stock_RSI</th>\n",
       "      <th>Stock_Move</th>\n",
       "      <th>Stock_MAvg_Move</th>\n",
       "      <th>Stock_MAvg_s_Move</th>\n",
       "      <th>Stock_ROC_Move</th>\n",
       "      <th>Stock_EMA_Move</th>\n",
       "      <th>Stock_EMA_Move_5</th>\n",
       "      <th>Stock_Disparity_Move</th>\n",
       "      <th>Stock_Disparity_s_Move</th>\n",
       "      <th>Stock_RSI_Move</th>\n",
       "      <th>Nas_Moment_1</th>\n",
       "      <th>Nas_Moment_2</th>\n",
       "      <th>Nas_Moment_1_s</th>\n",
       "      <th>Nas_Moment_2_s</th>\n",
       "      <th>Nas_MAvg</th>\n",
       "      <th>Nas_MAvg_s</th>\n",
       "      <th>Nas_MAvg_s_5</th>\n",
       "      <th>Nas_MAvg_s_6</th>\n",
       "      <th>Nas_Disparity</th>\n",
       "      <th>Nas_Disparity_s</th>\n",
       "      <th>Nas_Disparity_s_5</th>\n",
       "      <th>Nas_ROC</th>\n",
       "      <th>Nas_ROC_s</th>\n",
       "      <th>Nas_Rocp</th>\n",
       "      <th>Nas_EMA</th>\n",
       "      <th>Nas_EMA_5</th>\n",
       "      <th>Nas_diff</th>\n",
       "      <th>Nas_gain</th>\n",
       "      <th>Nas_loss</th>\n",
       "      <th>Nas_avg_gain</th>\n",
       "      <th>Nas_avg_loss</th>\n",
       "      <th>Nas_rs</th>\n",
       "      <th>Nas_RSI</th>\n",
       "      <th>Nas_Move</th>\n",
       "      <th>Nas_MAvg_Move</th>\n",
       "      <th>Nas_MAvg_s_Move</th>\n",
       "      <th>Nas_ROC_Move</th>\n",
       "      <th>Nas_EMA_Move</th>\n",
       "      <th>Nas_EMA_Move_5</th>\n",
       "      <th>Nas_Disparity_Move</th>\n",
       "      <th>Nas_Disparity_s_Move</th>\n",
       "      <th>Nas_RSI_Move</th>\n",
       "      <th>Dow_Moment_1</th>\n",
       "      <th>Dow_Moment_2</th>\n",
       "      <th>Dow_Moment_1_s</th>\n",
       "      <th>Dow_Moment_2_s</th>\n",
       "      <th>Dow_MAvg</th>\n",
       "      <th>Dow_MAvg_s</th>\n",
       "      <th>Dow_MAvg_s_5</th>\n",
       "      <th>Dow_MAvg_s_6</th>\n",
       "      <th>Dow_Disparity</th>\n",
       "      <th>Dow_Disparity_s</th>\n",
       "      <th>Dow_Disparity_s_5</th>\n",
       "      <th>Dow_ROC</th>\n",
       "      <th>Dow_ROC_s</th>\n",
       "      <th>Dow_Rocp</th>\n",
       "      <th>Dow_EMA</th>\n",
       "      <th>Dow_EMA_5</th>\n",
       "      <th>Dow_diff</th>\n",
       "      <th>Dow_gain</th>\n",
       "      <th>Dow_loss</th>\n",
       "      <th>Dow_avg_gain</th>\n",
       "      <th>Dow_avg_loss</th>\n",
       "      <th>Dow_rs</th>\n",
       "      <th>Dow_RSI</th>\n",
       "      <th>Dow_Move</th>\n",
       "      <th>Dow_MAvg_Move</th>\n",
       "      <th>Dow_MAvg_s_Move</th>\n",
       "      <th>Dow_ROC_Move</th>\n",
       "      <th>Dow_EMA_Move</th>\n",
       "      <th>Dow_EMA_Move_5</th>\n",
       "      <th>Dow_Disparity_Move</th>\n",
       "      <th>Dow_Disparity_s_Move</th>\n",
       "      <th>Dow_RSI_Move</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UBS_x  UBS Financial Services Inc.  UBS Investment Bank  \\\n",
       "0     18                           18                   18   \n",
       "\n",
       "   UBS Global Wealth Management  UBS Asset Management  Open  High  Low  Close  \\\n",
       "0                            18                    18     0     0    0      0   \n",
       "\n",
       "   Volume  Dividends  Stock Splits  UBS_y  Union Bank of Switzerland  \\\n",
       "0       0          0             0      0                          0   \n",
       "\n",
       "   UBS tax evasion controversies  Banking in Switzerland  dow_open  dow_high  \\\n",
       "0                              0                       0         0         0   \n",
       "\n",
       "   dow_low  dow_close  dow_vol  nas_open  nas_high  nas_low  nas_close  \\\n",
       "0        0          0        0         0         0        0          0   \n",
       "\n",
       "   nas_vol  Wiki_total  Google_total  Stock_total  Nas_total  Dow_total  \\\n",
       "0        0          18            18            0          0          0   \n",
       "\n",
       "   Wiki_Moment_1  Wiki_Moment_2  Wiki_Moment_1_s  Wiki_Moment_2_s  Wiki_MAvg  \\\n",
       "0             23             23               21               21         15   \n",
       "\n",
       "   Wiki_MAvg_s  Wiki_MAvg_s_5  Wiki_MAvg_s_6  Wiki_Disparity  \\\n",
       "0           16             15             15              18   \n",
       "\n",
       "   Wiki_Disparity_s  Wiki_Disparity_s_5  Wiki_ROC  Wiki_ROC_s  Wiki_Rocp  \\\n",
       "0                18                  18        23          21         23   \n",
       "\n",
       "   Wiki_EMA  Wiki_EMA_5  Wiki_diff  Wiki_gain  Wiki_loss  Wiki_avg_gain  \\\n",
       "0        19          23         19         19         19             32   \n",
       "\n",
       "   Wiki_avg_loss  Wiki_rs  Wiki_RSI  Stoch_Oscillator_3  Stoch_Oscillator_14  \\\n",
       "0             32       32        32                   1                    3   \n",
       "\n",
       "   Change_Close  Change_Google  Wiki_Move  Wiki_MAvg_Move  Wiki_MAvg_s_Move  \\\n",
       "0             1             20          0               0                 0   \n",
       "\n",
       "   Wiki_ROC_Move  Wiki_EMA_Move  Wiki_EMA_Move_5  Wiki_Disparity_Move  \\\n",
       "0              0              0                0                    0   \n",
       "\n",
       "   Wiki_Disparity_s_Move  Wiki_RSI_Move  Google_Moment_1  Google_Moment_2  \\\n",
       "0                      0              0               23               23   \n",
       "\n",
       "   Google_Moment_1_s  Google_Moment_2_s  Google_MAvg  Google_MAvg_s  \\\n",
       "0                 21                 21           15             16   \n",
       "\n",
       "   Google_MAvg_s_5  Google_MAvg_s_6  Google_Disparity  Google_Disparity_s  \\\n",
       "0               15               15                18                  18   \n",
       "\n",
       "   Google_Disparity_s_5  Google_ROC  Google_ROC_s  Google_Rocp  Google_EMA  \\\n",
       "0                    18          23            21           23          19   \n",
       "\n",
       "   Google_EMA_5  Google_diff  Google_gain  Google_loss  Google_avg_gain  \\\n",
       "0            23           19           19           19               32   \n",
       "\n",
       "   Google_avg_loss  Google_rs  Google_RSI  Google_Move  Google_MAvg_Move  \\\n",
       "0               32         32          32            0                 0   \n",
       "\n",
       "   Google_MAvg_s_Move  Google_ROC_Move  Google_EMA_Move  Google_EMA_Move_5  \\\n",
       "0                   0                0                0                  0   \n",
       "\n",
       "   Google_Disparity_Move  Google_Disparity_s_Move  Google_RSI_Move  \\\n",
       "0                      0                        0                0   \n",
       "\n",
       "   Stock_Moment_1  Stock_Moment_2  Stock_Moment_1_s  Stock_Moment_2_s  \\\n",
       "0               0               0                 0                 0   \n",
       "\n",
       "   Stock_MAvg  Stock_MAvg_s  Stock_MAvg_s_5  Stock_MAvg_s_6  Stock_Disparity  \\\n",
       "0           0             0               0               0                0   \n",
       "\n",
       "   Stock_Disparity_s  Stock_Disparity_s_5  Stock_ROC  Stock_ROC_s  Stock_Rocp  \\\n",
       "0                  0                    0          0            0           0   \n",
       "\n",
       "   Stock_EMA  Stock_EMA_5  Stock_diff  Stock_gain  Stock_loss  Stock_avg_gain  \\\n",
       "0          0            0           0           0           0               0   \n",
       "\n",
       "   Stock_avg_loss  Stock_rs  Stock_RSI  Stock_Move  Stock_MAvg_Move  \\\n",
       "0               0         0          0           0                0   \n",
       "\n",
       "   Stock_MAvg_s_Move  Stock_ROC_Move  Stock_EMA_Move  Stock_EMA_Move_5  \\\n",
       "0                  0               0               0                 0   \n",
       "\n",
       "   Stock_Disparity_Move  Stock_Disparity_s_Move  Stock_RSI_Move  Nas_Moment_1  \\\n",
       "0                     0                       0               0             0   \n",
       "\n",
       "   Nas_Moment_2  Nas_Moment_1_s  Nas_Moment_2_s  Nas_MAvg  Nas_MAvg_s  \\\n",
       "0             0               0               0         0           0   \n",
       "\n",
       "   Nas_MAvg_s_5  Nas_MAvg_s_6  Nas_Disparity  Nas_Disparity_s  \\\n",
       "0             0             0              0                0   \n",
       "\n",
       "   Nas_Disparity_s_5  Nas_ROC  Nas_ROC_s  Nas_Rocp  Nas_EMA  Nas_EMA_5  \\\n",
       "0                  0        0          0         0        0          0   \n",
       "\n",
       "   Nas_diff  Nas_gain  Nas_loss  Nas_avg_gain  Nas_avg_loss  Nas_rs  Nas_RSI  \\\n",
       "0         0         0         0             0             0       0        0   \n",
       "\n",
       "   Nas_Move  Nas_MAvg_Move  Nas_MAvg_s_Move  Nas_ROC_Move  Nas_EMA_Move  \\\n",
       "0         0              0                0             0             0   \n",
       "\n",
       "   Nas_EMA_Move_5  Nas_Disparity_Move  Nas_Disparity_s_Move  Nas_RSI_Move  \\\n",
       "0               0                   0                     0             0   \n",
       "\n",
       "   Dow_Moment_1  Dow_Moment_2  Dow_Moment_1_s  Dow_Moment_2_s  Dow_MAvg  \\\n",
       "0             0             0               0               0         0   \n",
       "\n",
       "   Dow_MAvg_s  Dow_MAvg_s_5  Dow_MAvg_s_6  Dow_Disparity  Dow_Disparity_s  \\\n",
       "0           0             0             0              0                0   \n",
       "\n",
       "   Dow_Disparity_s_5  Dow_ROC  Dow_ROC_s  Dow_Rocp  Dow_EMA  Dow_EMA_5  \\\n",
       "0                  0        0          0         0        0          0   \n",
       "\n",
       "   Dow_diff  Dow_gain  Dow_loss  Dow_avg_gain  Dow_avg_loss  Dow_rs  Dow_RSI  \\\n",
       "0         0         0         0             0             0       0        0   \n",
       "\n",
       "   Dow_Move  Dow_MAvg_Move  Dow_MAvg_s_Move  Dow_ROC_Move  Dow_EMA_Move  \\\n",
       "0         0              0                0             0             0   \n",
       "\n",
       "   Dow_EMA_Move_5  Dow_Disparity_Move  Dow_Disparity_s_Move  Dow_RSI_Move  \\\n",
       "0               0                   0                     0             0   \n",
       "\n",
       "   target_1  target_2  target_3  target_4  target_5  \n",
       "0         0         0         0         0         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"UBS_Cleaned_Date.csv\")\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df.set_index(\"date\")\n",
    "df = df.iloc[14:, :] # to remove first 14 days that include NaNs due to some calculations\n",
    "pd.DataFrame(df.isna().sum()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8736c8",
   "metadata": {},
   "source": [
    "We see from above that some varaibles contain a lot of NaN's so they either might not be useful, or they're going to lead us into eliminating a lot of data points.\n",
    "\n",
    "To Solve NaN problem we will create two initial data sets, one removing high NaN values, one not, and see which produced better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997c5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop([['Ford', 'Ford_Bronco_x', 'Ford_Stock', 'F-150', 'Ford_Bronco_y', 'Ford Motor Company', 'Ford F Series', 'Lincoln Navigator', 'Lincoln Aviator']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c58691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((748, 200), (762, 165))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High NaN varaibles included will be df_0\n",
    "df_0 = df.dropna()\n",
    "df_0 = df_0[~(df_0.isin([np.inf, -np.inf]).any(axis=1))] # to remove inf\n",
    "\n",
    "# df_1 will remove the high NaN columns\n",
    "df_1 = df[df.columns.drop(list(df.filter(regex='gain')))]\n",
    "df_1 = df_1[df_1.columns.drop(list(df_1.filter(regex='loss')))]\n",
    "df_1 = df_1[df_1.columns.drop(list(df_1.filter(regex='RSI')))]\n",
    "df_1 = df_1[df_1.columns.drop(list(df_1.filter(regex='_rs')))]\n",
    "df_1 = df_1.dropna()\n",
    "df_1 = df_1[~(df_1.isin([np.inf, -np.inf]).any(axis=1))]\n",
    "\n",
    "df_0.shape,df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a68ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Chosen is target_3\n",
    "df_0 = df_0.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "df_1 = df_1.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "\n",
    "target_3_0 = df_0[\"target_3\"]\n",
    "target_3_1 = df_0[\"target_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262d44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training sets \n",
    "column_indices = {name: i for i, name in enumerate(df_0.columns)}\n",
    "\n",
    "n = len(df_0)\n",
    "train_f0 = df_0[0:int(n*0.7)]\n",
    "val_f0 = df_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0 = df_0[int(n*0.9):]\n",
    "\n",
    "train_f0t = target_3_0[0:int(n*0.7)]\n",
    "val_f0t = target_3_0[int(n*0.7):int(n*0.9)]\n",
    "test_f0t = target_3_0[int(n*0.9):]\n",
    "\n",
    "# now with df_1\n",
    "n = len(df_1)\n",
    "train_f1 = df_1[0:int(n*0.7)]\n",
    "val_f1 = df_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1 = df_1[int(n*0.9):]\n",
    "\n",
    "train_f1t = target_3_1[0:int(n*0.7)]\n",
    "val_f1t = target_3_1[int(n*0.7):int(n*0.9)]\n",
    "test_f1t = target_3_1[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e31954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preporocessing and standardizing the data\n",
    "Mscaler = MinMaxScaler() # keeps binarys at zero and 1 :)\n",
    "\n",
    "train_f0 = pd.DataFrame(Mscaler.fit_transform(train_f0), columns = df_0.columns)\n",
    "val_f0 = pd.DataFrame(Mscaler.fit_transform(val_f0), columns = df_0.columns)\n",
    "test_f0 = pd.DataFrame(Mscaler.fit_transform(test_f0), columns = df_0.columns)\n",
    "\n",
    "train_f1 = pd.DataFrame(Mscaler.fit_transform(train_f1), columns = df_1.columns)\n",
    "val_f1 = pd.DataFrame(Mscaler.fit_transform(val_f1), columns = df_1.columns)\n",
    "test_f1 = pd.DataFrame(Mscaler.fit_transform(test_f1), columns = df_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2159c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_y2(df, target, window_size=5):\n",
    "  df_as_np = df.to_numpy() # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df_as_np[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "    label = target[i+window_size] # pulls the target variable after the window, target varible needs to be column zero in this \n",
    "    y.append(label) # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b42ae",
   "metadata": {},
   "source": [
    "## Switching Focus to Just df_0, High NaN Varibles included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eedd7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Features       Score\n",
      "56                   Change_Close  499.866139\n",
      "49                      Wiki_loss    3.537357\n",
      "59                 Wiki_MAvg_Move    2.913996\n",
      "53                       Wiki_RSI    2.646081\n",
      "66                  Wiki_RSI_Move    2.633565\n",
      "112                    Stock_Rocp    2.628453\n",
      "124             Stock_MAvg_s_Move    2.354298\n",
      "154                      Nas_Move    2.135456\n",
      "179                      Dow_diff    1.798386\n",
      "80                    Google_Rocp    1.721439\n",
      "60               Wiki_MAvg_s_Move    1.638488\n",
      "161          Nas_Disparity_s_Move    1.605203\n",
      "176                      Dow_Rocp    1.592464\n",
      "2             UBS Investment Bank    1.591836\n",
      "69              Google_Moment_1_s    1.571602\n",
      "79                   Google_ROC_s    1.571602\n",
      "116                    Stock_gain    1.494766\n",
      "70              Google_Moment_2_s    1.478365\n",
      "184                        Dow_rs    1.469674\n",
      "166                Dow_Moment_2_s    1.308038\n",
      "10                      Dividends    1.269835\n",
      "94                Google_EMA_Move    1.212841\n",
      "95              Google_EMA_Move_5    1.212841\n",
      "126                Stock_EMA_Move    1.198972\n",
      "127              Stock_EMA_Move_5    1.198972\n",
      "193          Dow_Disparity_s_Move    1.195191\n",
      "157                  Nas_ROC_Move    1.193377\n",
      "181                      Dow_loss    1.125189\n",
      "175                     Dow_ROC_s    1.103559\n",
      "165                Dow_Moment_1_s    1.103559\n",
      "180                      Dow_gain    1.085749\n",
      "115                    Stock_diff    1.065938\n",
      "55            Stoch_Oscillator_14    1.039901\n",
      "92             Google_MAvg_s_Move    1.027950\n",
      "82                   Google_EMA_5    1.025887\n",
      "3    UBS Global Wealth Management    1.003839\n",
      "121                     Stock_RSI    0.996176\n",
      "185                       Dow_RSI    0.981739\n",
      "67                Google_Moment_1    0.936986\n",
      "78                     Google_ROC    0.936986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 40 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=40)\n",
    "best_fit = bestfeatures.fit(train_f0, train_f0t)\n",
    "best_scores = pd.DataFrame(best_fit.scores_)\n",
    "best_columns = pd.DataFrame(df_0.columns)\n",
    "\n",
    "# concatenate the dataframes for better visualization\n",
    "features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "print(features_score.nlargest(40, 'Score'))  # print the top 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683d2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(features_score.nlargest(40, 'Score')['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048d7967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 15.83397769,  27.60003601,  37.50858322,  43.88833547,\n",
       "        48.88917189,  53.2672383 ,  56.5996076 ,  59.14963451,\n",
       "        61.45222625,  63.5737241 ,  65.63009239,  67.62225812,\n",
       "        69.48354866,  71.28702006,  72.95707323,  74.5189978 ,\n",
       "        76.01283478,  77.44447619,  78.80779229,  80.07205881,\n",
       "        81.24098137,  82.35245441,  83.43631352,  84.4341931 ,\n",
       "        85.36970224,  86.27434305,  87.10701614,  87.91715349,\n",
       "        88.68724553,  89.39270875,  90.04532372,  90.65695026,\n",
       "        91.24060505,  91.81745313,  92.33268   ,  92.8361845 ,\n",
       "        93.32600745,  93.78240789,  94.20835914,  94.62749284,\n",
       "        95.01012962,  95.38509474,  95.74556645,  96.05160644,\n",
       "        96.34738857,  96.63727549,  96.89788316,  97.14730102,\n",
       "        97.3788915 ,  97.60377075,  97.80437089,  97.99371146,\n",
       "        98.14706294,  98.27551567,  98.39257788,  98.49614067,\n",
       "        98.59764575,  98.69101369,  98.771267  ,  98.84526401,\n",
       "        98.91110121,  98.9737899 ,  99.02808751,  99.08068051,\n",
       "        99.12822247,  99.17550979,  99.21905239,  99.25969304,\n",
       "        99.29907845,  99.33499928,  99.37053428,  99.40335911,\n",
       "        99.43404028,  99.46371308,  99.49132355,  99.51813148,\n",
       "        99.5439303 ,  99.56834909,  99.59212467,  99.61405301,\n",
       "        99.63435022,  99.65410992,  99.67286453,  99.69041928,\n",
       "        99.70708519,  99.72329057,  99.738684  ,  99.75328583,\n",
       "        99.76775378,  99.78158571,  99.79475179,  99.80724069,\n",
       "        99.81955179,  99.8311655 ,  99.84226274,  99.85319251,\n",
       "        99.86377808,  99.87410581,  99.8840818 ,  99.89372298,\n",
       "        99.90301571,  99.91164756,  99.92022037,  99.92819788,\n",
       "        99.93539201,  99.94176197,  99.94730702,  99.95256292,\n",
       "        99.95749391,  99.9620069 ,  99.96644661,  99.9705513 ,\n",
       "        99.97357332,  99.97607686,  99.97839646,  99.98055694,\n",
       "        99.98254201,  99.98430377,  99.98599265,  99.98746372,\n",
       "        99.98887509,  99.9902077 ,  99.99146916,  99.99250908,\n",
       "        99.99347999,  99.99438784,  99.99520168,  99.9958543 ,\n",
       "        99.99638492,  99.99687292,  99.99735749,  99.99771881,\n",
       "        99.9980422 ,  99.998292  ,  99.99853534,  99.99876694,\n",
       "        99.99893208,  99.99908318,  99.99922881,  99.99935939,\n",
       "        99.99947533,  99.99956348,  99.99964391,  99.99971125,\n",
       "        99.99975816,  99.99980135,  99.99984077,  99.99987283,\n",
       "        99.99990002,  99.99992358,  99.99994599,  99.9999589 ,\n",
       "        99.99996988,  99.99997847,  99.99998515,  99.99999044,\n",
       "        99.99999388,  99.99999678,  99.99999907, 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA().fit(train_f0)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "# reach 85% variance explained with 25 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e186f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnvElEQVR4nO3deZxddX3/8dd79iSThZBJgCwkhLBEAgSHqAUFQTFCBdxaaKmiFqwSteIGtUVKa1uxdak/XBARtQJStRohgAtQtAokJCEkYUkICZMQMtmTSTLr/fz+OGfCzTjJ3Ay5c2bmvp+Px33cs3zPuZ97kjmfe77ne75fRQRmZla6yrIOwMzMsuVEYGZW4pwIzMxKnBOBmVmJcyIwMytxFVkHcLDGjBkTkydPzjoMM7MB5fHHH98UEXXdrRtwiWDy5MksWLAg6zDMzAYUSWv2t85VQ2ZmJc6JwMysxDkRmJmVOCcCM7MS50RgZlbiipYIJN0qqVHS0v2sl6T/lLRS0hJJpxUrFjMz279iXhHcBsw+wPq3AtPS15XAN4oYi5mZ7UfRniOIiIclTT5AkYuA70fSD/YjkkZJOjIi1hcrJjPrnYigIxe0p6+OjqA9l9tnvi2XS8rkreuc78il6zvi5eW5HLl4eX17LvbOR/qZuQgiIBeQS7vMz+WS9Ulc0DnX2aN+7Bt4Xx2iPnHuieM4ZeKoQ77fLB8oGw805M2vTZf9USKQdCXJVQOTJk3qk+DMstTanqO5vYPmtg5a2nK0tHfQnL4n812Wtef2lkvW5Whp66C1I13e8fL61vYcrR052juCto70ZN6Ro63zBJ53sm7ryO09SQ9UUtYRHDpjR9QMukRQsIi4GbgZoL6+fuD+j7RBqbU9x66WdprS166Wdnam78nyjrzpl5ftaWtnd2sHe1o72NOWvqfTr+TEK0F1RRnVFeVUV5RRVVGWvr88X1tdQUWZqCgvo7JcVJSVUVEuKtP3znXJuygve3m6oiyZrywX5WVp2b3blVFeRvKe7q987z6S+bIyqCwvo0yd+3p5P53TkigTe9/L0rN5mYQEIlkHndOd330QnfX7UJaJYB0wMW9+QrrMrE+0tufYtqeVnc3tNDW3s7O5nZ3NbexsbmdHc1tyYm9++QSffxJvamljV0sHTc3ttHbkCvq8qooyhldXMKzzVVVObXUFdbXVDK0qZ0hVOUMqKxhSVcbQqgqqK8qoqUxO3tWV5dSk79XpiT1/Xeey6opyKsvlE6IdlCwTwVxgjqQ7gdcA231/wA6FXS3tNGzdTcOWPby0fQ+bmlrZvKuFTTuT981NrWxqamFHc3uP+xpWVc6w6gpqqyuoralgWFUFEw6rorZ6OMOqy6mtrqS2unzvyT3/RF9bXZGWSeYry91a2/qnoiUCSXcAZwNjJK0FPgdUAkTEN4F5wPnASmA38L5ixWKDS3tHjnXb9rBm825e2LKbhq27WbtlD2u37qZh6x627Grdp7wEhw2t4vBhVRxeW8WJR42grraaw4dVMWpoJcNrKhleU5H3nkzXVldQXuZf1jb4FbPV0KU9rA/gqmJ9vg1s7R051mzZzcrGJtZs3rX3pL9m827WbdtDR14demW5GD9qCBNHD+UtR41k4ughTDxsKBNHD+WokTWMHlZFhX+Nm+3XgLhZbINX5wl/xYadrNjQxLONTazYsJNVG3ftU/c+ckglRx8+lJMnjORtpxzJ0aOHMenwoUwaPZQjRtRQ5l/uZr3mRGB9qqmlnYVrtrJg9Rbmr97KooatNLe9fMKfcNgQpo2t5azj6pg2bjjHjq1lyuHDGDm0MsOozQY3JwIrqk1NLTy6agvzV29hwZotLH9xB7mAMsH0o0ZwyemTOGn8SI4bV8vUulqGVfu/pFlf81+dHVIRwbIXd/Cbpxp54JlGnmjYBkBNZRkzJx7GnDcey+lTRjNz0mHU+qRv1i/4L9FesV0t7fxu5SYefLqRB55upHFnCxKcMmEUV7/5OF4/bQwnjR/p5pNm/ZQTgfXall2tfOHep/mfReto7cgxvLqC1x83hnNOGMfZx9cxprY66xDNrABOBHbQIoIfP76Wf5n3FDub27lk1kTOn3Ek9UePpqrCv/rNBhonAjsoKzbs5LM/W8pjz2+h/ujD+Oe3n8QJR4zIOiwzewWcCKwge1o7+NoDK7j54VXU1lTwhXfO4N2vnuj2+2aDgBOB9ejBZxq57udLadiyh3eeNoG/O/8EDnf9v9mg4URg+7VxZwv/+Itl3L1kPVPrhnHHFa/ldVMPzzosMzvEnAjsj0QE//34Wj5/z1Psae3g6jcfx9+cNdU3gs0GKScC28eazbu49qdP8vvnNjNr8mj+5R0zOHZsbdZhmVkROREYkHT+dsvvnufLv3qWqvIyPv/2k7j09Em+GWxWApwIjKXrtvOZnyxh2Ys7OG/6OG646CSOGFmTdVhm1kecCErczxev4+q7nmD0sCq+edlpzD7pyKxDMrM+VtS7f5JmS3pG0kpJ13Sz/mhJv5G0RNJDkiYUMx7b1y+eeJGP/2gxp08+jF9//CwnAbMSVbREIKkcuAl4KzAduFTS9C7F/h34fkScDNwA/Gux4rF93bNkPX/7o8XUTx7NrZef7v7+zUpYMa8IZgErI2JVRLQCdwIXdSkzHXggnX6wm/VWBPc+uZ6P3rmIV086jO9efjpDq1xDaFbKipkIxgMNefNr02X5ngDekU6/HRgu6Y+eWJJ0paQFkhZs3LixKMGWivuWrucjdyxi5sRR3Pq+0z0QjJkV9x5BAT4JnCVpEXAWsA7o6FooIm6OiPqIqK+rq+vrGAeN+5e9xJzbF3HKxFHc9v5ZHhjGzIDithpaB0zMm5+QLtsrIl4kvSKQVAu8MyK2FTGmkvXLZS9x1Q8XMmPCSG573+lOAma2VzGvCOYD0yRNkVQFXALMzS8gaYykzhiuBW4tYjwl69fLN3DV7Qt51fiRfO/9sxhe4xvDZvayoiWCiGgH5gD3A08Bd0XEMkk3SLowLXY28IykZ4FxwOeLFU+penTVZj78w4VMP3IE33//LEY4CZhZF4qIrGM4KPX19bFgwYKswxgQVjbu5B1f/z11w6v5yYf+hFFDq7IOycwyIunxiKjvbl3WN4utSBp3NPPeW+dTVVHObe+b5SRgZvvlRDAI7Wpp5/3fm8+WXa189/LTmTh6aNYhmVk/5kQwyLR35Jhz+0KWv7iDm/5yJjMmjMw6JDPr59yGcBCJCP7h50t58JmN/MvbZ3DOCeOyDsnMBgBfEQwiX3/oOe54rIEPnz2Vv3jNpKzDMbMBwolgkPifRWv54v3PcNGpR/GptxyfdThmNoA4EQwCv1+5iU//eAmvPWY0N77rZCSPKmZmhXMiGOAatuzmQz9cyOTDh/Gty+qprijPOiQzG2CcCAaw5rYOPvzDheQi+PZ76j2mgJn1ilsNDWA33L2cJ9dt5+a/ejWTxwzLOhwzG6B8RTBA/eTxtdz+6At88KxjOO9VR2QdjpkNYE4EA9DTL+3gsz97ktdMGc2nznMLITN7ZZwIBpgdzW186L8WMqKmkq/9xUwqyv1PaGavjO8RDCARwaf/ewkvbNnNHVe8lrHDa7IOycwGAf+cHEBu+e3z3LfsJa6ZfQKzpozOOhwzGyScCAaIx57fwr/d9zSzX3UEf/36KVmHY2aDSFETgaTZkp6RtFLSNd2snyTpQUmLJC2RdH4x4xmoGnc2M+f2hUw8bAg3vttPDpvZoVW0RCCpHLgJeCswHbhU0vQuxf6eZAjLmSRjGn+9WPEMVBHBJ+56gh3NbXzjsld7qEkzO+SKeUUwC1gZEasiohW4E7ioS5kARqTTI4EXixjPgPRfj6zhtys28dkLpnPikSN63sDM7CAVMxGMBxry5temy/JdD1wmaS0wD/hIdzuSdKWkBZIWbNy4sRix9kvPb9rF5+c9xRuOq+MydyttZkWS9c3iS4HbImICcD7wA0l/FFNE3BwR9RFRX1dX1+dBZqG9I8cn7lpMVXkZN77T9wXMrHiKmQjWARPz5ieky/J9ALgLICL+ANQAY4oY04DxrYdXsfCFbfzTxSdxxEg/L2BmxVPMRDAfmCZpiqQqkpvBc7uUeQE4F0DSiSSJoHTqfvZj+Ys7+Mqvn+X8GUdw4SlHZR2OmQ1yRUsEEdEOzAHuB54iaR20TNINki5Mi30CuELSE8AdwOUREcWKaSBoae/g6rsWM3JIFf988QxXCZlZ0RW1i4mImEdyEzh/2XV508uBM4oZw0DzlV+v4OmXdvKd99YzelhV1uGYWQnI+max5Xl8zRa+9b/P8Wf1Ezj3xHFZh2NmJcKJoJ/Y3drO1Xc9wZEjh/APf9r1uTszs+Jx76P9xL/Oe5o1m5NeRYf76WEz60O+IugH/m/lJn7wyBref8YUXjf18KzDMbMS40SQsfaOHNfPXcak0UP59GyPNmZmfc+JIGN3zG9gRWMTf3f+CdRUlmcdjpmVICeCDO1obuPLv3qWWVNG8xYPQG9mGXEiyNBND6xk6+5W/uGC6X5wzMwy40SQkRc27+a7/7ead8ycwIwJI7MOx8xKmBNBRv7tvqcoLxOfeotvEJtZtpwIMvDY81uY9+RLfPCsY9yzqJllzomgj+VywT/dvZwjRtRw5RuOyTocMzMngr72s8XreHLddj49+3iGVvnBbjPLnhNBH9rd2s6N9z3DyRNGcvGpXUftNDPLRo8/SSVNIBlU5vXAUcAeYClwD3BvROSKGuEgcvPDq3hpRzP/eelMysrcXNTM+ocDJgJJ3yUZcP5u4AtAI8koYscBs4HPSromIh4udqAD3Uvbm/nW/67i/BlHMGvK6KzDMTPbq6crgv+IiKXdLF8K/DQdgnLS/jaWNBv4KlAO3BIR/9Zl/ZeBN6azQ4GxETGqwNgHlC/e/wwdueCa2SdmHYqZ2T4OmAi6SwKSpgJDI+LJiGgFVna3raRy4CbgzcBaYL6kuemoZJ37/3he+Y8AM3v1Lfq5p9bv4CcL1/LBs45h0uFDsw7HzGwfB9VsRdLfAccCOUnVEfFXByg+C1gZEavSbe8ELgKW76f8pcDnDiaegeLbD69iaFU5Hz7r2KxDMTP7IwdsNSTpo+kv+06nRMT7I+KvgVN62Pd4oCFvfm26rLvPORqYAjywn/VXSlogacHGjRt7+Nj+Zf32Pcx94kX+/PSJjBzqAWfMrP/pqfnoZuA+SRem87+UdJ+kXwL3H8I4LgF+HBEd3a2MiJsjoj4i6uvq6g7hxxbfbb9fTS6C958xJetQzMy6dcBEEBE/BN4GnCxpLvA48A7g3RHxqR72vQ6YmDc/IV3WnUuAOwqKeABpamnn9kdf4K0zjmTiaN8bMLP+qZAHyqYCdwFXAleRtAIaUsB284FpkqakrYsuAeZ2LSTpBOAw4A+FBj1Q/Gh+Azub27ni9e5Kwsz6r56eI7gNaCNp2rkuIq6QNBP4tqT5EXHD/raNiHZJc0iqkMqBWyNimaQbgAUR0ZkULgHujIg4BN+n32jvyHHr755n1uTRnDpxVNbhmJntV0+thmZGxCkAkhYBRMQi4G2SLupp5xExD5jXZdl1XeavP5iAB4p7l77Eum17+NzbpmcdipnZAfWUCO6TdD9QCdyevyIifl60qAa4iOCW365iyphhvOnEcVmHY2Z2QD09UPYZSSOAXEQ09VFMA9781Vt5Yu12/vnik9ynkJn1ez09R3AZ0LS/JCBpqqQzixLZAHbzw6s4bGgl7zxtQtahmJn1qKeqocOBRZIeJ2k6upGk07ljgbOATcA1RY1wgHluYxO/eXoDHzlnGkOqynvewMwsYz1VDX1V0v8DzgHOAE4m6Yb6KeCvIuKF4oc4sHznd89TWV7Ge153dNahmJkVpMe+htKnfX+VvuwANje18JPH1/LO08YzprY663DMzAriEcoOoR88soaW9hwfONMPkJnZwOFEcIg0t3Xwgz+s4ZwTxnLs2NqswzEzK5gTwSHy04Xr2Lyr1d1JmNmAU1AikDRO0nck3ZvOT5f0geKGNnDkcsEtv1vFSeNH8NpjPAylmQ0shV4R3EbSZ9BR6fyzwN8WIZ4BaVHDNlZt3MV7XzcZyQ+QmdnAUmgiGBMRdwE5SDqUA7odO6AU3bNkPVXlZbzlpCOyDsXM7KAVmgh2STocCABJrwW2Fy2qASSXC+Y9uZ6zjq9jRI1HIDOzgafQMYuvJhlLYKqk/wPqgHcVLaoB5PEXtvLSjmauPfmErEMxM+uVghJBRCyUdBZwPCDgmYhoK2pkA8Q9S9ZTXVHGue5l1MwGqEJbDV0F1EbEsohYCtRK+nBxQ+v/OtJqoTceP5ba6kIvrszM+pdC7xFcERHbOmciYitwRU8bSZot6RlJKyV12zmdpD+TtFzSMkm3d1emv1qweguNO1u44OQjsw7FzKzXCv0ZWy5JncNJSioHqg60QVrmJuDNwFpgvqS5EbE8r8w04FrgjIjYKmlsb75EVu5esp6ayjLOOWFAhW1mto9CrwjuA34k6VxJ5wJ3pMsOZBawMiJWRUQrcCfQdXjLK4Cb0isMIqKx8NCz1ZEL7l26nnNOGMswVwuZ2QBW6BnsM8AHgQ+l878Cbulhm/FAQ978WuA1XcocB5C2RCoHro+IP0owkq4ErgSYNGlSgSEX16PPb2ZTUyt/evJRPRc2M+vHCm01lAO+kb4O9edPA84GJgAPS5qRfz8i/fybgZsB6uvr4xDH0Cv3LFnPkMpy3ni8q4XMbGArtNXQGZJ+JelZSaskPS9pVQ+brQMm5s1PSJflWwvMjYi2iHiepOuKaYUGn5X2jhz3LX2Jc08c61HIzGzAK7Rq6DvAx0mGqyy0a4n5wDRJU0gSwCXAX3Qp8zPgUuC7ksaQVBX1lGAy98iqLWze1cqfurWQmQ0ChSaC7RFx78HsOCLaJc0h6ayuHLg1IpZJugFYEBFz03XnSVpOkmA+FRGbD+ZzsnDPky8yrKqcs10tZGaDQKGJ4EFJXwR+CrR0LoyIhQfaKCLmAfO6LLsubzpIuq+4utCAs9aWVgu9afo4aipdLWRmA1+hiaCztU993rIgGdS+pPzhuc1s3d3GBTNcLWRmg0OhrYbeWOxABoq7l7zI8OoK3nBcXdahmJkdEgU/CSXpAuBVQE3nsoi4oRhB9Vet7TnuX7aBN7tayMwGkUKbj34T+HPgIyS9j74bOLqIcfVL//fcJrbvaXPfQmY2qBTaxcSfRMR7gK0R8Y/A60ifCi4l9yxZz/CaCs6cNibrUMzMDplCE8Ge9H23pKOANqCkfha3tHdw/7KXeMurjqC6wtVCZjZ4FHqP4G5Jo4AvAgtJWgz11NfQoPK7FZvY2dzuaiEzG3QKbTX0T+nkTyTdDdREREmNWXzPkvWMHFLJGVNdLWRmg8sBE4GkcyLiAUnv6GYdEfHT4oXWf0QEDz27kXNPGEtVRaG1aWZmA0NPVwRnAQ8Ab+tmXZA8aTzovbBlN1t2tfLqyYdlHYqZ2SF3wEQQEZ+TVAbcGxF39VFM/c7ihm0AnDpxVKZxmJkVQ4/1HOlYBJ/ug1j6rUUvbGNIZTnHjxuedShmZodcoRXev5b0SUkTJY3ufBU1sn5kccM2ZowfSUW57w+Y2eBTaPPRP0/fr8pbFsAxhzac/qelvYPlL+7g8jMmZx2KmVlRFNp8dEqxA+mvnlq/k9aOnO8PmNmgdTCdzp0ETGffTue+X4yg+pPFL2wFfKPYzAavQjud+xzwtfT1RuBG4MICtpst6RlJKyVd0836yyVtlLQ4ff31QcZfdIsbtjF2eDVHjqzpubCZ2QBU6BXBu4BTgEUR8T5J44D/OtAGksqBm4A3kwxSP1/S3IhY3qXojyJizkHG3WcWN2zj1ImjkJR1KGZmRVFwp3NpM9J2SSOARmBiD9vMAlZGxKqIaAXuBC7qfah9b+uuVlZv3s2pk0ZlHYqZWdEUmggWpJ3OfRt4nKTjuT/0sM14oCFvfm26rKt3Sloi6ceSekoufWrx2m2A7w+Y2eB2wEQg6SZJZ0TEhyNiW0R8k6Sq570R8b5D8Pm/ACZHxMnAr4Dv7SeOKyUtkLRg48aNh+BjC7P4hW1IcPKEUX32mWZmfa2nK4JngX+XtFrSjZJmRsTqiFhSwL7XsW/10YR02V4RsTkiWtLZW4BXd7ejiLg5Iuojor6uru/GCl7csI3jxw2ntrrgxlVmZgPOARNBRHw1Il5H0vncZuBWSU9L+pyknkYomw9MkzRFUhVwCTA3v4Ck/M79LwSeOuhvUCQRwRNrt7layMwGvYLuEUTEmoj4QkTMBC4FLqaHk3ZEtANzgPvTsndFxDJJN0jqbHr6UUnLJD0BfBS4vHdf49BbvXk323a3ORGY2aBXUJ2HpArgrSS/6s8FHgKu72m7iJgHzOuy7Lq86WuBawuOtg8t6nyQzC2GzGyQ62lgmjeTXAGcDzxG0gT0yojY1QexZWpxwzaGVZUzbax7HDWzwa2nK4JrgduBT0TE1j6Ip99Y3LCNGRNGUl7mB8nMbHDraWCac/oqkP6kua2Dp9bv4ANnDvrOVc3MCn6grKQse3EHbR3hG8VmVhKcCLrROTTlTN8oNrMS4ETQjcUN2zhyZA3jRrjHUTMb/JwIurG4YaurhcysZDgRdLG5qYWGLXucCMysZDgRdNF5f8CJwMxKhRNBF4sbtlFeJmZMGJl1KGZmfcKJoIvFDds4btxwhla5x1EzKw1OBHlyudg7NKWZWalwIsizatMudja3M9OJwMxKiBNBHvc4amalyIkgz+KGbdRWVzC1rjbrUMzM+owTQZ7FDds42T2OmlmJKWoikDRb0jOSVkq65gDl3ikpJNUXM54D2dPawdMv7fSNYjMrOUVLBJLKgZtIRjabDlwqaXo35YYDHwMeLVYshVj64nY6cu5x1MxKTzGvCGYBKyNiVUS0koxudlE35f4J+ALQXMRYerT4hW2AbxSbWekpZiIYDzTkza9Nl+0l6TRgYkTcU8Q4CrJk3XbGjxrC2OHucdTMSktmN4sllQFfAj5RQNkrJS2QtGDjxo1FiefZl3ZywhEen9jMSk8xE8E6YGLe/IR0WafhwEnAQ5JWA68F5nZ3wzgibo6I+oior6urO+SBtnfkWLWpiWPHudmomZWeYiaC+cA0SVMkVQGXAHM7V0bE9ogYExGTI2Iy8AhwYUQsKGJM3VqzZTdtHcG0sb4iMLPSU7REEBHtwBzgfuAp4K6IWCbpBkkXFutze2PFhiYApo31FYGZlZ6idrEZEfOAeV2WXbefsmcXM5YDWdm4E4CpTgRmVoL8ZDGworGJ8aOGUFvtrqfNrPQ4EQDPbmhimm8Um1mJKvlE0JELntvY5PsDZlaySj4RNGzZTWt7zi2GzKxklXwiWNGYtBjyMwRmVqqcCNIWQ8e6asjMSpQTwYYmjhxZw4iayqxDMTPLhBNB405fDZhZSSvpRJDLBSsbm3yj2MxKWkkngnXb9tDclvMzBGZW0ko6EXTeKPYzBGZWyko7EeztbM5VQ2ZWuko6ETy7oYmxw6sZOdQthsysdJV0IljZuNP3B8ys5JVsIogIVrjFkJlZ6SaCF7c3s7u1w88QmFnJK2oikDRb0jOSVkq6ppv1fyPpSUmLJf1O0vRixpNvxYakxdBx43xFYGalrWiJQFI5cBPwVmA6cGk3J/rbI2JGRJwK3Ah8qVjxdOXhKc3MEsW8IpgFrIyIVRHRCtwJXJRfICJ25M0OA6KI8exjReNOxtRWcdiwqr76SDOzfqmYYzOOBxry5tcCr+laSNJVwNVAFXBOEePZx4rGJt8fMDOjH9wsjoibImIq8Bng77srI+lKSQskLdi4ceOh+ExWbnCLITMzKG4iWAdMzJufkC7bnzuBi7tbERE3R0R9RNTX1dW94sA27GhhZ0s7x/kZAjOzoiaC+cA0SVMkVQGXAHPzC0ialjd7AbCiiPHs9fJgNL4iMDMr2j2CiGiXNAe4HygHbo2IZZJuABZExFxgjqQ3AW3AVuC9xYon37OdLYZ8RWBmVtSbxUTEPGBel2XX5U1/rJifvz8rG3dy2NBKDneLITOz7G8WZ2FFeqNYUtahmJllruQSwd4+hlwtZGYGlGAi2NjUwvY9bX6i2MwsVXKJYG/XEu5jyMwMKMlE4OEpzczylV4iaGxiRE0FdcOrsw7FzKxfKMlEcNw4txgyM+tUcolgpVsMmZnto6QSweamFrbsanXXEmZmeUoqETzrwWjMzP5ISSWClWlnc64aMjN7WUklghWNTQyvruCIETVZh2Jm1m+UViLY0MSx42rdYsjMLE9pJYLGJt8fMDPromQSwdZdrWxqavHwlGZmXZRMIljRmLQYOtY3is3M9lHURCBptqRnJK2UdE0366+WtFzSEkm/kXR0sWLpHJ7yOHc2Z2a2j6IlAknlwE3AW4HpwKWSpncptgioj4iTgR8DNxYrnrraas6bPo6jRrrFkJlZvmIOVTkLWBkRqwAk3QlcBCzvLBARD+aVfwS4rFjBnPeqIzjvVUcUa/dmZgNWMauGxgMNefNr02X78wHg3iLGY2Zm3Sjq4PWFknQZUA+ctZ/1VwJXAkyaNKkPIzMzG/yKeUWwDpiYNz8hXbYPSW8CPgtcGBEt3e0oIm6OiPqIqK+rqytKsGZmpaqYiWA+ME3SFElVwCXA3PwCkmYC3yJJAo1FjMXMzPajaIkgItqBOcD9wFPAXRGxTNINki5Mi30RqAX+W9JiSXP3szszMyuSot4jiIh5wLwuy67Lm35TMT/fzMx6VjJPFpuZWfecCMzMSpwiIusYDoqkjcCaXm4+Bth0CMM5lBxb7zi23nFsvTOQYzs6IrptdjngEsErIWlBRNRnHUd3HFvvOLbecWy9M1hjc9WQmVmJcyIwMytxpZYIbs46gANwbL3j2HrHsfXOoIytpO4RmJnZHyu1KwIzM+vCicDMrMSVTCLoadjMLElaLenJtL+lBRnHcqukRklL85aNlvQrSSvS98P6UWzXS1qXHrvFks7PKLaJkh5Mh15dJulj6fLMj90BYsv82EmqkfSYpCfS2P4xXT5F0qPp3+uP0o4r+0tst0l6Pu+4ndrXseXFWC5pkaS70/neHbeIGPQvoBx4DjgGqAKeAKZnHVdefKuBMVnHkcbyBuA0YGneshuBa9Lpa4Av9KPYrgc+2Q+O25HAaen0cOBZkiFaMz92B4gt82MHCKhNpyuBR4HXAncBl6TLvwl8qB/Fdhvwrqz/z6VxXQ3cDtydzvfquJXKFcHeYTMjohXoHDbTuoiIh4EtXRZfBHwvnf4ecHFfxtRpP7H1CxGxPiIWptM7SXrcHU8/OHYHiC1zkWhKZyvTVwDnkIxjDtkdt/3F1i9ImgBcANySzoteHrdSSQQHO2xmXwvgl5IeT0dj62/GRcT6dPolYFyWwXRjjqQladVRJtVW+SRNBmaS/ILsV8euS2zQD45dWr2xGGgEfkVy9b4tkq7sIcO/166xRUTncft8ety+LKk6i9iArwCfBnLp/OH08riVSiLo786MiNOAtwJXSXpD1gHtTyTXnP3mVxHwDWAqcCqwHviPLIORVAv8BPjbiNiRvy7rY9dNbP3i2EVER0ScSjKK4SzghCzi6E7X2CSdBFxLEuPpwGjgM30dl6Q/BRoj4vFDsb9SSQQFDZuZlYhYl743Av9D8sfQn2yQdCRA+t5vRpOLiA3pH2sO+DYZHjtJlSQn2h9GxE/Txf3i2HUXW386dmk824AHgdcBoyR1jpeS+d9rXmyz06q2iGRo3e+SzXE7A7hQ0mqSqu5zgK/Sy+NWKomgx2EzsyJpmKThndPAecDSA2/V5+YC702n3wv8PMNY9tF5kk29nYyOXVo/+x3gqYj4Ut6qzI/d/mLrD8dOUp2kUen0EODNJPcwHgTelRbL6rh1F9vTeYldJHXwfX7cIuLaiJgQEZNJzmcPRMRf0tvjlvVd7756AeeTtJZ4Dvhs1vHkxXUMSSumJ4BlWccG3EFSTdBGUsf4AZK6x98AK4BfA6P7UWw/AJ4ElpCcdI/MKLYzSap9lgCL09f5/eHYHSC2zI8dcDKwKI1hKXBduvwY4DFgJfDfQHU/iu2B9LgtBf6LtGVRVi/gbF5uNdSr4+YuJszMSlypVA2Zmdl+OBGYmZU4JwIzsxLnRGBmVuKcCMzMSpwTgRWdpJD0H3nzn5R0/SHa922S3tVzyVf8Oe+W9JSkB7tZd5ykeWkPowsl3SWpv3XDcVAkXSxpetZxWN9wIrC+0AK8Q9KYrAPJl/cEZiE+AFwREW/sso8a4B7gGxExLZKuQr4O1B26SDNxMUkPpVYCnAisL7STjKf68a4ruv6il9SUvp8t6X8l/VzSKkn/Jukv0/7hn5Q0NW83b5K0QNKzaR8snZ2FfVHS/LRzsA/m7fe3kuYCy7uJ59J0/0slfSFddh3JQ1nfkfTFLpv8BfCHiPhF54KIeCgilqb92X833d8iSW9M93e5pJ8pGZ9gtaQ5kq5OyzwiaXRa7iFJX1XS5/1SSbPS5aPT7Zek5U9Ol1+fdh73UHrMPpr3vS5Lj91iSd+SVN55vCV9Xkmf+49IGifpT4ALgS+m5adK+qiS8QyWSLqzkH90G0CyfCLOr9J4AU3ACJJxF0YCnwSuT9fdRl7f7kBT+n42sI2kL/1qkj5T/jFd9zHgK3nb30fyo2YayRPHNcCVwN+nZaqBBcCUdL+7gCndxHkU8ALJr/kKkidIL07XPQTUd7PNl4CP7ed7fwK4NZ0+Id13DXA5yZOfw9PP2g78TVruyySdwnV+5rfT6TeQjsMAfA34XDp9DrA4nb4e+H36fccAm0m6Tj4R+AVQmZb7OvCedDqAt6XTN+Yds67/Li+SPqUKjMr6/5Rfh/blKwLrE5H0dvl94KM9lc0zP5IOvlpIugb5Zbr8SWByXrm7IiIXESuAVSQn3fOA9yjpQvhRkq4epqXlH4uI57v5vNOBhyJiYyRd+f6Q5ATcW2eSdEFARDwNrAGOS9c9GBE7I2IjSSLovKLo+t3uSLd/GBiR9n1zJkn3EETEA8Dhkkak5e+JiJaI2ETSwd044Fzg1cD89HicS9IVAUArcHc6/XiXz863BPihpMtIrvBsEDmYOlKzV+orwEKSHhs7tZNWUUoqIxlBrlNL3nQubz7Hvv93u/aTEiSjS30kIu7PXyHpbJIrgkNlGXBWL7Z7Jd+t0P12pPsS8L2IuLab8m0REV3Kd+cCkqT4NuCzkmbEy/3e2wDnKwLrMxGxhWQovQ/kLV5N8msVknrpyl7s+t2SytL7BscAzwD3Ax9S0v1yZ8ueYT3s5zHgLElj0jr0S4H/7WGb24E/kXRB5wJJb1DSb/1vgb/s/HxgUhrbwfjzdPszge0Rsb3Lfs8GNkWXsQ+6+A3wLklj021GSzq6h8/dSVJ11ZmgJ0bEgyR9748Eag/ye1g/5isC62v/AczJm/828HNJT5DU9ffm1/oLJCfxESR17c2SbiGp5lgoScBGehi2LyLWS7qGpCtfkVSzHLAb34jYk96g/oqkr5D0jLqE5D7G14FvSHqS5Mrn8ohoScIpWLOkRSQJ8v3psuuBWyUtAXbzcjfX+4txuaS/JxkFryyN8SqSqqr9uRP4dnrD+RKSG+UjSY7Lf0bSP78NEu591KyfkvQQyeDyC7KOxQY3Vw2ZmZU4XxGYmZU4XxGYmZU4JwIzsxLnRGBmVuKcCMzMSpwTgZlZifv/++mN1rUUGjAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 21.09317012,  40.00020565,  54.48458811,  63.73249966,\n",
       "        70.6397164 ,  76.15278804,  80.70030185,  84.74212683,\n",
       "        88.2952194 ,  91.43695872,  93.43065458,  95.3563954 ,\n",
       "        96.27547839,  97.00889466,  97.52806546,  97.95192943,\n",
       "        98.35234205,  98.62303134,  98.80382722,  98.96751124,\n",
       "        99.12924168,  99.28730436,  99.41525631,  99.51662289,\n",
       "        99.61596873,  99.70779084,  99.76673092,  99.82010665,\n",
       "        99.87243229,  99.92385075,  99.96961088,  99.98941855,\n",
       "        99.99795272, 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA with 40 best components\n",
    "pca_1 = PCA().fit(train_f0[feats])\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca_1.explained_variance_ratio_) * 100\n",
    "# can use 9 principle compents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b2cc0",
   "metadata": {},
   "source": [
    "Keeping score: Will test 5 models with df_0 data variations,\n",
    "- Model_0_0 = All varaibles,\n",
    "- Model_0_1 = PCA All varaibles\n",
    "- Model_0_2 = 40 best k score\n",
    "- Model_0_3 = PCA of 40 Best K Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a449c60",
   "metadata": {},
   "source": [
    "### Model_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ef8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_0, train_f0t_tc = df_to_X_y2(train_f0,train_f0t)\n",
    "val_X_0_0, val_f0t_tc= df_to_X_y2(val_f0, val_f0t)\n",
    "test_X_0_0, test_f0t_tc = df_to_X_y2(test_f0,test_f0t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c108603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(523, 518)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_f0t), len(train_X_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "602a57f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_0.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0375a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 15ms/step - loss: 0.6992 - accuracy: 0.4749 - val_loss: 0.6995 - val_accuracy: 0.4759\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5405 - val_loss: 0.6978 - val_accuracy: 0.5034\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5560 - val_loss: 0.6992 - val_accuracy: 0.4828\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.5541 - val_loss: 0.6988 - val_accuracy: 0.4759\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5811 - val_loss: 0.7030 - val_accuracy: 0.4621\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6767 - accuracy: 0.6042 - val_loss: 0.7022 - val_accuracy: 0.4621\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6730 - accuracy: 0.5830 - val_loss: 0.7009 - val_accuracy: 0.4552\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.5985 - val_loss: 0.7022 - val_accuracy: 0.4897\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6601 - accuracy: 0.6332 - val_loss: 0.7088 - val_accuracy: 0.4483\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6531 - accuracy: 0.6313 - val_loss: 0.7091 - val_accuracy: 0.4897\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6616 - accuracy: 0.5946 - val_loss: 0.7152 - val_accuracy: 0.4690\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6518 - accuracy: 0.6100 - val_loss: 0.7204 - val_accuracy: 0.4483\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6416 - accuracy: 0.6351 - val_loss: 0.7145 - val_accuracy: 0.4621\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6303 - accuracy: 0.6564 - val_loss: 0.7234 - val_accuracy: 0.4759\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6247 - accuracy: 0.6757 - val_loss: 0.7251 - val_accuracy: 0.4552\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6129 - accuracy: 0.6757 - val_loss: 0.7257 - val_accuracy: 0.4759\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6058 - accuracy: 0.6834 - val_loss: 0.7463 - val_accuracy: 0.4828\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6008 - accuracy: 0.6815 - val_loss: 0.7593 - val_accuracy: 0.4897\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5897 - accuracy: 0.7278 - val_loss: 0.7482 - val_accuracy: 0.4690\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5799 - accuracy: 0.6892 - val_loss: 0.7674 - val_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f4e2aa00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_0.shape[1]\n",
    "n_features = train_X_0_0.shape[2]\n",
    "\n",
    "model_0_0_1 = Sequential()\n",
    "model_0_0_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_1.add(Flatten())\n",
    "model_0_0_1.add(Dense(50, activation='relu')) \n",
    "model_0_0_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_0_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_1.fit(train_X_0_0, train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc060af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 2s 22ms/step - loss: 0.7082 - accuracy: 0.4942 - val_loss: 0.6889 - val_accuracy: 0.4966\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6863 - accuracy: 0.5193 - val_loss: 0.6922 - val_accuracy: 0.5241\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6718 - accuracy: 0.5714 - val_loss: 0.7078 - val_accuracy: 0.5034\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6664 - accuracy: 0.5695 - val_loss: 0.7075 - val_accuracy: 0.5034\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6542 - accuracy: 0.6216 - val_loss: 0.7103 - val_accuracy: 0.4966\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6346 - accuracy: 0.6525 - val_loss: 0.7115 - val_accuracy: 0.4690\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6297 - accuracy: 0.6486 - val_loss: 0.7151 - val_accuracy: 0.4483\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6157 - accuracy: 0.6757 - val_loss: 0.7246 - val_accuracy: 0.4828\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5897 - accuracy: 0.7124 - val_loss: 0.7319 - val_accuracy: 0.4897\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5675 - accuracy: 0.7568 - val_loss: 0.7517 - val_accuracy: 0.4690\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5528 - accuracy: 0.7413 - val_loss: 0.7523 - val_accuracy: 0.5034\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5238 - accuracy: 0.7664 - val_loss: 0.9200 - val_accuracy: 0.4690\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5309 - accuracy: 0.7568 - val_loss: 0.7655 - val_accuracy: 0.4690\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5059 - accuracy: 0.7954 - val_loss: 0.8280 - val_accuracy: 0.5103\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4836 - accuracy: 0.7896 - val_loss: 0.8249 - val_accuracy: 0.4483\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4668 - accuracy: 0.7876 - val_loss: 0.8675 - val_accuracy: 0.4345\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4293 - accuracy: 0.8571 - val_loss: 0.8413 - val_accuracy: 0.4759\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4014 - accuracy: 0.8591 - val_loss: 0.9490 - val_accuracy: 0.4276\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3892 - accuracy: 0.8610 - val_loss: 0.8731 - val_accuracy: 0.4621\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3515 - accuracy: 0.8996 - val_loss: 0.8949 - val_accuracy: 0.4966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f2d0a040>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_2 = Sequential()\n",
    "model_0_0_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_0_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_2.add(Flatten())\n",
    "model_0_0_2.add(Dense(50, activation='relu')) \n",
    "model_0_0_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_0_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_2.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38d44d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 0.7022 - accuracy: 0.4749 - val_loss: 0.6930 - val_accuracy: 0.5310\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5386 - val_loss: 0.6982 - val_accuracy: 0.4138\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6885 - accuracy: 0.5367 - val_loss: 0.6984 - val_accuracy: 0.4621\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.5502 - val_loss: 0.7226 - val_accuracy: 0.4690\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.5521 - val_loss: 0.7082 - val_accuracy: 0.4828\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.5598 - val_loss: 0.7028 - val_accuracy: 0.5310\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6804 - accuracy: 0.5618 - val_loss: 0.7029 - val_accuracy: 0.4483\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.6622 - val_loss: 0.7458 - val_accuracy: 0.4621\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6782 - accuracy: 0.5618 - val_loss: 0.7201 - val_accuracy: 0.4690\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.5927 - val_loss: 0.7153 - val_accuracy: 0.5310\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6485 - accuracy: 0.6158 - val_loss: 0.7161 - val_accuracy: 0.4759\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6194 - accuracy: 0.6911 - val_loss: 0.7358 - val_accuracy: 0.4414\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5936 - accuracy: 0.7124 - val_loss: 0.7680 - val_accuracy: 0.4276\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5830 - accuracy: 0.7104 - val_loss: 0.7711 - val_accuracy: 0.4828\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5428 - accuracy: 0.7645 - val_loss: 0.9512 - val_accuracy: 0.4414\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5975 - accuracy: 0.6525 - val_loss: 0.7832 - val_accuracy: 0.4897\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5380 - accuracy: 0.7741 - val_loss: 0.8026 - val_accuracy: 0.4828\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5099 - accuracy: 0.7703 - val_loss: 0.8306 - val_accuracy: 0.4759\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4883 - accuracy: 0.7954 - val_loss: 0.8839 - val_accuracy: 0.5034\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.4806 - accuracy: 0.7838 - val_loss: 0.8988 - val_accuracy: 0.4345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f634c5b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_3 = Sequential()\n",
    "model_0_0_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_3.add(Flatten())\n",
    "model_0_0_3.add(Dense(50, activation='relu')) \n",
    "model_0_0_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_0_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_3.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79a0e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 18ms/step - loss: 0.6993 - accuracy: 0.5174 - val_loss: 0.6949 - val_accuracy: 0.5034\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6965 - accuracy: 0.4826 - val_loss: 0.6964 - val_accuracy: 0.4483\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6968 - accuracy: 0.5019 - val_loss: 0.6933 - val_accuracy: 0.5241\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5212 - val_loss: 0.6942 - val_accuracy: 0.5103\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6957 - accuracy: 0.5000 - val_loss: 0.6938 - val_accuracy: 0.5241\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.4981 - val_loss: 0.6916 - val_accuracy: 0.5034\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5676 - val_loss: 0.6923 - val_accuracy: 0.4897\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6836 - accuracy: 0.5792 - val_loss: 0.6941 - val_accuracy: 0.5034\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5695 - val_loss: 0.6954 - val_accuracy: 0.5034\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5135 - val_loss: 0.6975 - val_accuracy: 0.4759\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6856 - accuracy: 0.5637 - val_loss: 0.7104 - val_accuracy: 0.4897\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6658 - accuracy: 0.5907 - val_loss: 0.7112 - val_accuracy: 0.4552\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6420 - accuracy: 0.6448 - val_loss: 0.7347 - val_accuracy: 0.5034\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6452 - accuracy: 0.6178 - val_loss: 0.7295 - val_accuracy: 0.4759\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6299 - accuracy: 0.6486 - val_loss: 0.7311 - val_accuracy: 0.5103\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6164 - accuracy: 0.6911 - val_loss: 0.7606 - val_accuracy: 0.4759\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5896 - accuracy: 0.7008 - val_loss: 0.7574 - val_accuracy: 0.5172\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5904 - accuracy: 0.6795 - val_loss: 0.7618 - val_accuracy: 0.5103\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5540 - accuracy: 0.7046 - val_loss: 0.8305 - val_accuracy: 0.5034\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5911 - accuracy: 0.6988 - val_loss: 0.8109 - val_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f5e02cd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_4 = Sequential()\n",
    "model_0_0_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_0_4.add(Flatten())\n",
    "model_0_0_4.add(Dense(50, activation='relu')) \n",
    "model_0_0_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_0_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_4.fit(train_X_0_0,train_f0t_tc,epochs=20,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19af56ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 [==============================] - 1s 15ms/step - loss: 0.6993 - accuracy: 0.4807 - val_loss: 0.6936 - val_accuracy: 0.4552\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6940 - accuracy: 0.4961 - val_loss: 0.6926 - val_accuracy: 0.4966\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6941 - accuracy: 0.5193 - val_loss: 0.6945 - val_accuracy: 0.4828\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5135 - val_loss: 0.6931 - val_accuracy: 0.4759\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6937 - accuracy: 0.4942 - val_loss: 0.6925 - val_accuracy: 0.5310\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.4826 - val_loss: 0.6927 - val_accuracy: 0.5517\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6916 - accuracy: 0.5598 - val_loss: 0.6925 - val_accuracy: 0.5241\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5116 - val_loss: 0.6924 - val_accuracy: 0.5172\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5483 - val_loss: 0.6973 - val_accuracy: 0.4897\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5502 - val_loss: 0.6921 - val_accuracy: 0.5310\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6866 - accuracy: 0.5135 - val_loss: 0.6966 - val_accuracy: 0.4897\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6806 - accuracy: 0.5888 - val_loss: 0.6919 - val_accuracy: 0.5448\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6856 - accuracy: 0.5347 - val_loss: 0.6941 - val_accuracy: 0.5379\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6725 - accuracy: 0.6023 - val_loss: 0.7475 - val_accuracy: 0.4621\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6752 - accuracy: 0.5965 - val_loss: 0.7014 - val_accuracy: 0.5103\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6537 - accuracy: 0.6409 - val_loss: 0.7389 - val_accuracy: 0.4552\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6446 - accuracy: 0.6236 - val_loss: 0.7330 - val_accuracy: 0.5103\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6218 - accuracy: 0.6718 - val_loss: 0.8165 - val_accuracy: 0.5034\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6420 - accuracy: 0.6062 - val_loss: 0.7589 - val_accuracy: 0.4966\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6023 - accuracy: 0.6911 - val_loss: 0.7782 - val_accuracy: 0.4828\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5911 - accuracy: 0.6776 - val_loss: 0.8247 - val_accuracy: 0.4414\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.7066 - val_loss: 0.8499 - val_accuracy: 0.4759\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5710 - accuracy: 0.7278 - val_loss: 0.8646 - val_accuracy: 0.4690\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5208 - accuracy: 0.7510 - val_loss: 0.8954 - val_accuracy: 0.4828\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4908 - accuracy: 0.7664 - val_loss: 0.9370 - val_accuracy: 0.4759\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4651 - accuracy: 0.7799 - val_loss: 0.9431 - val_accuracy: 0.4897\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.4437 - accuracy: 0.8012 - val_loss: 0.9820 - val_accuracy: 0.4828\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4226 - accuracy: 0.8089 - val_loss: 1.1126 - val_accuracy: 0.5034\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.4078 - accuracy: 0.8224 - val_loss: 1.0734 - val_accuracy: 0.4690\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3732 - accuracy: 0.8494 - val_loss: 1.2397 - val_accuracy: 0.4966\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3803 - accuracy: 0.8282 - val_loss: 1.2664 - val_accuracy: 0.5172\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3614 - accuracy: 0.8224 - val_loss: 1.2315 - val_accuracy: 0.4897\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.8649 - val_loss: 1.2222 - val_accuracy: 0.4897\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3279 - accuracy: 0.8552 - val_loss: 1.5527 - val_accuracy: 0.4828\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2839 - accuracy: 0.8842 - val_loss: 1.4551 - val_accuracy: 0.5103\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2511 - accuracy: 0.8919 - val_loss: 1.5301 - val_accuracy: 0.4966\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2168 - accuracy: 0.9228 - val_loss: 1.6746 - val_accuracy: 0.5103\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2484 - accuracy: 0.8996 - val_loss: 1.7422 - val_accuracy: 0.4897\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2195 - accuracy: 0.9035 - val_loss: 1.6588 - val_accuracy: 0.5103\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2609 - accuracy: 0.8977 - val_loss: 1.7056 - val_accuracy: 0.4828\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2002 - accuracy: 0.9208 - val_loss: 1.9073 - val_accuracy: 0.5034\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2085 - accuracy: 0.8958 - val_loss: 1.9195 - val_accuracy: 0.5172\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1514 - accuracy: 0.9517 - val_loss: 1.9408 - val_accuracy: 0.5103\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.1573 - accuracy: 0.9363 - val_loss: 2.0128 - val_accuracy: 0.5103\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9633 - val_loss: 2.2904 - val_accuracy: 0.5241\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0921 - accuracy: 0.9768 - val_loss: 2.2393 - val_accuracy: 0.5241\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.1041 - accuracy: 0.9710 - val_loss: 2.2532 - val_accuracy: 0.5448\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9923 - val_loss: 2.4660 - val_accuracy: 0.5241\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 0.9981 - val_loss: 2.6659 - val_accuracy: 0.5379\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9942 - val_loss: 2.6983 - val_accuracy: 0.5310\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.9768 - val_loss: 2.6234 - val_accuracy: 0.4966\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9923 - val_loss: 2.9605 - val_accuracy: 0.5241\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9961 - val_loss: 3.0767 - val_accuracy: 0.4966\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0371 - accuracy: 1.0000 - val_loss: 3.0155 - val_accuracy: 0.5379\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9788 - val_loss: 3.2627 - val_accuracy: 0.5034\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9749 - val_loss: 3.2757 - val_accuracy: 0.5172\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9942 - val_loss: 3.2327 - val_accuracy: 0.4759\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.2753 - val_accuracy: 0.4897\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 3.3004 - val_accuracy: 0.5103\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 3.4241 - val_accuracy: 0.5241\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 3.4998 - val_accuracy: 0.5241\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 3.6050 - val_accuracy: 0.5241\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 3.6809 - val_accuracy: 0.5241\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7340 - val_accuracy: 0.5172\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.7813 - val_accuracy: 0.5241\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.8216 - val_accuracy: 0.5241\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 3.8675 - val_accuracy: 0.5172\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.9229 - val_accuracy: 0.5310\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 3.9462 - val_accuracy: 0.5241\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 4.0025 - val_accuracy: 0.5310\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.0199 - val_accuracy: 0.5310\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.0501 - val_accuracy: 0.5241\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.0800 - val_accuracy: 0.5172\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 4.1186 - val_accuracy: 0.5310\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.1702 - val_accuracy: 0.5310\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.1845 - val_accuracy: 0.5310\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.2294 - val_accuracy: 0.5310\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.2510 - val_accuracy: 0.5241\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 4.2786 - val_accuracy: 0.5241\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.3016 - val_accuracy: 0.5241\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3252 - val_accuracy: 0.5241\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.3522 - val_accuracy: 0.5241\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.3865 - val_accuracy: 0.5241\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.3980 - val_accuracy: 0.5241\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.4217 - val_accuracy: 0.5241\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.9145e-04 - accuracy: 1.0000 - val_loss: 4.4391 - val_accuracy: 0.5241\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 9.5202e-04 - accuracy: 1.0000 - val_loss: 4.4480 - val_accuracy: 0.5310\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 9.1585e-04 - accuracy: 1.0000 - val_loss: 4.4803 - val_accuracy: 0.5310\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 9.0029e-04 - accuracy: 1.0000 - val_loss: 4.5043 - val_accuracy: 0.5310\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.6079e-04 - accuracy: 1.0000 - val_loss: 4.5304 - val_accuracy: 0.5241\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 8.2460e-04 - accuracy: 1.0000 - val_loss: 4.5512 - val_accuracy: 0.5241\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.9458e-04 - accuracy: 1.0000 - val_loss: 4.5614 - val_accuracy: 0.5241\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.8013e-04 - accuracy: 1.0000 - val_loss: 4.5842 - val_accuracy: 0.5241\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.4285e-04 - accuracy: 1.0000 - val_loss: 4.5977 - val_accuracy: 0.5241\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 7.2029e-04 - accuracy: 1.0000 - val_loss: 4.6142 - val_accuracy: 0.5241\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.8943e-04 - accuracy: 1.0000 - val_loss: 4.6345 - val_accuracy: 0.5241\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.6376e-04 - accuracy: 1.0000 - val_loss: 4.6565 - val_accuracy: 0.5310\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.5859e-04 - accuracy: 1.0000 - val_loss: 4.6626 - val_accuracy: 0.5310\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 6.2672e-04 - accuracy: 1.0000 - val_loss: 4.6909 - val_accuracy: 0.5241\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.0575e-04 - accuracy: 1.0000 - val_loss: 4.7045 - val_accuracy: 0.5241\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 5.9124e-04 - accuracy: 1.0000 - val_loss: 4.7221 - val_accuracy: 0.5241\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.6960e-04 - accuracy: 1.0000 - val_loss: 4.7379 - val_accuracy: 0.5241\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.5804e-04 - accuracy: 1.0000 - val_loss: 4.7577 - val_accuracy: 0.5241\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.4651e-04 - accuracy: 1.0000 - val_loss: 4.7799 - val_accuracy: 0.5241\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 5.3070e-04 - accuracy: 1.0000 - val_loss: 4.7921 - val_accuracy: 0.5241\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 5.0265e-04 - accuracy: 1.0000 - val_loss: 4.8200 - val_accuracy: 0.5172\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4.9277e-04 - accuracy: 1.0000 - val_loss: 4.8284 - val_accuracy: 0.5241\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 4.7838e-04 - accuracy: 1.0000 - val_loss: 4.8423 - val_accuracy: 0.5241\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4.6603e-04 - accuracy: 1.0000 - val_loss: 4.8590 - val_accuracy: 0.5241\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 4.5129e-04 - accuracy: 1.0000 - val_loss: 4.8720 - val_accuracy: 0.5241\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 4.4023e-04 - accuracy: 1.0000 - val_loss: 4.8937 - val_accuracy: 0.5241\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 4.2985e-04 - accuracy: 1.0000 - val_loss: 4.9109 - val_accuracy: 0.5241\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 4.2010e-04 - accuracy: 1.0000 - val_loss: 4.9217 - val_accuracy: 0.5241\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4.1099e-04 - accuracy: 1.0000 - val_loss: 4.9321 - val_accuracy: 0.5241\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3.9568e-04 - accuracy: 1.0000 - val_loss: 4.9467 - val_accuracy: 0.5241\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3.8720e-04 - accuracy: 1.0000 - val_loss: 4.9600 - val_accuracy: 0.5241\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 3.7754e-04 - accuracy: 1.0000 - val_loss: 4.9658 - val_accuracy: 0.5310\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.6866e-04 - accuracy: 1.0000 - val_loss: 4.9950 - val_accuracy: 0.5241\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.5981e-04 - accuracy: 1.0000 - val_loss: 5.0078 - val_accuracy: 0.5241\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3.4963e-04 - accuracy: 1.0000 - val_loss: 5.0214 - val_accuracy: 0.5241\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.4305e-04 - accuracy: 1.0000 - val_loss: 5.0385 - val_accuracy: 0.5379\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3.3752e-04 - accuracy: 1.0000 - val_loss: 5.0492 - val_accuracy: 0.5241\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3.2643e-04 - accuracy: 1.0000 - val_loss: 5.0640 - val_accuracy: 0.5241\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 3.1610e-04 - accuracy: 1.0000 - val_loss: 5.0716 - val_accuracy: 0.5310\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3.0939e-04 - accuracy: 1.0000 - val_loss: 5.0992 - val_accuracy: 0.5310\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 3.0300e-04 - accuracy: 1.0000 - val_loss: 5.0987 - val_accuracy: 0.5310\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 2.9430e-04 - accuracy: 1.0000 - val_loss: 5.1193 - val_accuracy: 0.5241\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2.8913e-04 - accuracy: 1.0000 - val_loss: 5.1319 - val_accuracy: 0.5310\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.8218e-04 - accuracy: 1.0000 - val_loss: 5.1460 - val_accuracy: 0.5241\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.7609e-04 - accuracy: 1.0000 - val_loss: 5.1589 - val_accuracy: 0.5310\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2.6931e-04 - accuracy: 1.0000 - val_loss: 5.1804 - val_accuracy: 0.5310\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 2.6379e-04 - accuracy: 1.0000 - val_loss: 5.1817 - val_accuracy: 0.5310\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2.5717e-04 - accuracy: 1.0000 - val_loss: 5.1952 - val_accuracy: 0.5310\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.5501e-04 - accuracy: 1.0000 - val_loss: 5.2175 - val_accuracy: 0.5310\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2.4675e-04 - accuracy: 1.0000 - val_loss: 5.2210 - val_accuracy: 0.5241\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.4115e-04 - accuracy: 1.0000 - val_loss: 5.2349 - val_accuracy: 0.5241\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2.3673e-04 - accuracy: 1.0000 - val_loss: 5.2476 - val_accuracy: 0.5379\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.3256e-04 - accuracy: 1.0000 - val_loss: 5.2521 - val_accuracy: 0.5379\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 2.2751e-04 - accuracy: 1.0000 - val_loss: 5.2641 - val_accuracy: 0.5310\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.2088e-04 - accuracy: 1.0000 - val_loss: 5.2834 - val_accuracy: 0.5310\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2.1729e-04 - accuracy: 1.0000 - val_loss: 5.2905 - val_accuracy: 0.5241\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 2.1280e-04 - accuracy: 1.0000 - val_loss: 5.2984 - val_accuracy: 0.5241\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 2.0708e-04 - accuracy: 1.0000 - val_loss: 5.3120 - val_accuracy: 0.5241\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 2.0297e-04 - accuracy: 1.0000 - val_loss: 5.3261 - val_accuracy: 0.5310\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 2.0204e-04 - accuracy: 1.0000 - val_loss: 5.3405 - val_accuracy: 0.5310\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.9703e-04 - accuracy: 1.0000 - val_loss: 5.3469 - val_accuracy: 0.5241\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.9303e-04 - accuracy: 1.0000 - val_loss: 5.3592 - val_accuracy: 0.5241\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.9369e-04 - accuracy: 1.0000 - val_loss: 5.3557 - val_accuracy: 0.5241\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.8884e-04 - accuracy: 1.0000 - val_loss: 5.3805 - val_accuracy: 0.5241\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.7970e-04 - accuracy: 1.0000 - val_loss: 5.3816 - val_accuracy: 0.5310\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.8095e-04 - accuracy: 1.0000 - val_loss: 5.3951 - val_accuracy: 0.5310\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.7413e-04 - accuracy: 1.0000 - val_loss: 5.4120 - val_accuracy: 0.5241\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.7004e-04 - accuracy: 1.0000 - val_loss: 5.4229 - val_accuracy: 0.5241\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 1.7044e-04 - accuracy: 1.0000 - val_loss: 5.4342 - val_accuracy: 0.5310\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.6351e-04 - accuracy: 1.0000 - val_loss: 5.4361 - val_accuracy: 0.5310\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 1.6053e-04 - accuracy: 1.0000 - val_loss: 5.4526 - val_accuracy: 0.5241\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.5767e-04 - accuracy: 1.0000 - val_loss: 5.4616 - val_accuracy: 0.5241\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.5445e-04 - accuracy: 1.0000 - val_loss: 5.4689 - val_accuracy: 0.5310\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.5294e-04 - accuracy: 1.0000 - val_loss: 5.4854 - val_accuracy: 0.5310\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.4989e-04 - accuracy: 1.0000 - val_loss: 5.4956 - val_accuracy: 0.5241\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 1.4653e-04 - accuracy: 1.0000 - val_loss: 5.5085 - val_accuracy: 0.5241\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.4595e-04 - accuracy: 1.0000 - val_loss: 5.5199 - val_accuracy: 0.5310\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.4247e-04 - accuracy: 1.0000 - val_loss: 5.5187 - val_accuracy: 0.5310\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.3808e-04 - accuracy: 1.0000 - val_loss: 5.5450 - val_accuracy: 0.5310\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.3605e-04 - accuracy: 1.0000 - val_loss: 5.5502 - val_accuracy: 0.5310\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.3398e-04 - accuracy: 1.0000 - val_loss: 5.5618 - val_accuracy: 0.5379\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.3134e-04 - accuracy: 1.0000 - val_loss: 5.5608 - val_accuracy: 0.5310\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2925e-04 - accuracy: 1.0000 - val_loss: 5.5755 - val_accuracy: 0.5241\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.2686e-04 - accuracy: 1.0000 - val_loss: 5.5785 - val_accuracy: 0.5310\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.2477e-04 - accuracy: 1.0000 - val_loss: 5.5925 - val_accuracy: 0.5241\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.2261e-04 - accuracy: 1.0000 - val_loss: 5.6063 - val_accuracy: 0.5310\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 1.2383e-04 - accuracy: 1.0000 - val_loss: 5.6070 - val_accuracy: 0.5310\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.1809e-04 - accuracy: 1.0000 - val_loss: 5.6247 - val_accuracy: 0.5310\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.1733e-04 - accuracy: 1.0000 - val_loss: 5.6330 - val_accuracy: 0.5241\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.1426e-04 - accuracy: 1.0000 - val_loss: 5.6404 - val_accuracy: 0.5310\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.1200e-04 - accuracy: 1.0000 - val_loss: 5.6588 - val_accuracy: 0.5310\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.1069e-04 - accuracy: 1.0000 - val_loss: 5.6598 - val_accuracy: 0.5310\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.0828e-04 - accuracy: 1.0000 - val_loss: 5.6672 - val_accuracy: 0.5310\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 1.0678e-04 - accuracy: 1.0000 - val_loss: 5.6762 - val_accuracy: 0.5310\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.0578e-04 - accuracy: 1.0000 - val_loss: 5.6818 - val_accuracy: 0.5241\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 1.0428e-04 - accuracy: 1.0000 - val_loss: 5.6852 - val_accuracy: 0.5310\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 1.0172e-04 - accuracy: 1.0000 - val_loss: 5.7036 - val_accuracy: 0.5310\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 5.7154 - val_accuracy: 0.5241\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.7910e-05 - accuracy: 1.0000 - val_loss: 5.7220 - val_accuracy: 0.5310\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.6567e-05 - accuracy: 1.0000 - val_loss: 5.7318 - val_accuracy: 0.5310\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 9.5323e-05 - accuracy: 1.0000 - val_loss: 5.7330 - val_accuracy: 0.5310\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.3112e-05 - accuracy: 1.0000 - val_loss: 5.7495 - val_accuracy: 0.5310\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.2102e-05 - accuracy: 1.0000 - val_loss: 5.7577 - val_accuracy: 0.5310\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 9.0814e-05 - accuracy: 1.0000 - val_loss: 5.7626 - val_accuracy: 0.5310\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 8.9693e-05 - accuracy: 1.0000 - val_loss: 5.7755 - val_accuracy: 0.5310\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 8.7641e-05 - accuracy: 1.0000 - val_loss: 5.7818 - val_accuracy: 0.5310\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 8.6542e-05 - accuracy: 1.0000 - val_loss: 5.7913 - val_accuracy: 0.5310\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 8.4925e-05 - accuracy: 1.0000 - val_loss: 5.7982 - val_accuracy: 0.5310\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 8.4303e-05 - accuracy: 1.0000 - val_loss: 5.8043 - val_accuracy: 0.5310\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 8.2905e-05 - accuracy: 1.0000 - val_loss: 5.8147 - val_accuracy: 0.5310\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 8.1343e-05 - accuracy: 1.0000 - val_loss: 5.8284 - val_accuracy: 0.5310\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 7.9992e-05 - accuracy: 1.0000 - val_loss: 5.8400 - val_accuracy: 0.5310\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 7.8506e-05 - accuracy: 1.0000 - val_loss: 5.8497 - val_accuracy: 0.5241\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 7.7060e-05 - accuracy: 1.0000 - val_loss: 5.8553 - val_accuracy: 0.5310\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 7.6240e-05 - accuracy: 1.0000 - val_loss: 5.8653 - val_accuracy: 0.5310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f3031730>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_0_5 = Sequential()\n",
    "model_0_0_5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_0_5.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_0_5.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_0_5.add(Flatten())\n",
    "model_0_0_5.add(Dense(50, activation='relu')) \n",
    "model_0_0_5.add(Dense(1, activation='sigmoid')) \n",
    "model_0_0_5.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_0_5.fit(train_X_0_0,train_f0t_tc,epochs=200,  validation_data=(val_X_0_0, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614b3b8",
   "metadata": {},
   "source": [
    "Model_0_0_3 appears to be an initial best using all the data. Will use this one with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0ade9",
   "metadata": {},
   "source": [
    "### Model_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f47487ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_1 final data prep \n",
    "\n",
    "# PCA with 23 components to explain 85% of variance\n",
    "sklearn_pca = PCA(n_components=23)\n",
    "train_X_0_1 = pd.DataFrame(sklearn_pca.fit_transform(train_f0))\n",
    "val_X_0_1 = pd.DataFrame(sklearn_pca.transform(val_f0))\n",
    "test_X_0_1 = pd.DataFrame(sklearn_pca.transform(test_f0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7c40f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.416422</td>\n",
       "      <td>-0.552812</td>\n",
       "      <td>0.131181</td>\n",
       "      <td>0.955582</td>\n",
       "      <td>0.054797</td>\n",
       "      <td>-0.020998</td>\n",
       "      <td>-1.592313</td>\n",
       "      <td>-0.441373</td>\n",
       "      <td>-0.557078</td>\n",
       "      <td>0.159648</td>\n",
       "      <td>-0.284534</td>\n",
       "      <td>0.413303</td>\n",
       "      <td>-0.125948</td>\n",
       "      <td>0.453747</td>\n",
       "      <td>-0.290805</td>\n",
       "      <td>0.545576</td>\n",
       "      <td>-0.413868</td>\n",
       "      <td>0.397605</td>\n",
       "      <td>-0.119679</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>0.096271</td>\n",
       "      <td>-0.375248</td>\n",
       "      <td>-0.225443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.131354</td>\n",
       "      <td>-0.855773</td>\n",
       "      <td>1.585749</td>\n",
       "      <td>0.277939</td>\n",
       "      <td>-0.939847</td>\n",
       "      <td>-0.395470</td>\n",
       "      <td>-0.263718</td>\n",
       "      <td>-0.916463</td>\n",
       "      <td>0.100369</td>\n",
       "      <td>0.197542</td>\n",
       "      <td>-1.024802</td>\n",
       "      <td>0.068028</td>\n",
       "      <td>-0.454410</td>\n",
       "      <td>-1.442645</td>\n",
       "      <td>1.027229</td>\n",
       "      <td>-0.009694</td>\n",
       "      <td>0.796664</td>\n",
       "      <td>-0.479693</td>\n",
       "      <td>-0.706045</td>\n",
       "      <td>-0.516156</td>\n",
       "      <td>0.131170</td>\n",
       "      <td>0.190230</td>\n",
       "      <td>-0.317794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.897192</td>\n",
       "      <td>-1.635107</td>\n",
       "      <td>-0.421177</td>\n",
       "      <td>-0.404517</td>\n",
       "      <td>-0.592611</td>\n",
       "      <td>0.447623</td>\n",
       "      <td>1.004420</td>\n",
       "      <td>-0.467284</td>\n",
       "      <td>-0.141209</td>\n",
       "      <td>0.407040</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>0.595495</td>\n",
       "      <td>0.871430</td>\n",
       "      <td>-0.449192</td>\n",
       "      <td>0.242426</td>\n",
       "      <td>-0.352698</td>\n",
       "      <td>-0.095071</td>\n",
       "      <td>0.331351</td>\n",
       "      <td>0.435195</td>\n",
       "      <td>0.147930</td>\n",
       "      <td>0.147314</td>\n",
       "      <td>-0.449256</td>\n",
       "      <td>-0.752451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.802775</td>\n",
       "      <td>-1.014436</td>\n",
       "      <td>-1.492656</td>\n",
       "      <td>1.105121</td>\n",
       "      <td>0.010416</td>\n",
       "      <td>-1.161346</td>\n",
       "      <td>-0.260919</td>\n",
       "      <td>0.495378</td>\n",
       "      <td>-0.180611</td>\n",
       "      <td>0.354332</td>\n",
       "      <td>-0.639869</td>\n",
       "      <td>-0.881045</td>\n",
       "      <td>0.955714</td>\n",
       "      <td>-0.484634</td>\n",
       "      <td>0.331929</td>\n",
       "      <td>0.932466</td>\n",
       "      <td>-0.137698</td>\n",
       "      <td>-0.088760</td>\n",
       "      <td>0.172485</td>\n",
       "      <td>-0.181844</td>\n",
       "      <td>-0.280038</td>\n",
       "      <td>0.160299</td>\n",
       "      <td>-0.501305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.227701</td>\n",
       "      <td>-1.150176</td>\n",
       "      <td>1.192468</td>\n",
       "      <td>0.116273</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>-1.313552</td>\n",
       "      <td>-0.450934</td>\n",
       "      <td>-0.257704</td>\n",
       "      <td>-1.475189</td>\n",
       "      <td>0.416207</td>\n",
       "      <td>0.280944</td>\n",
       "      <td>-1.263506</td>\n",
       "      <td>-0.542511</td>\n",
       "      <td>0.059127</td>\n",
       "      <td>-0.144306</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.132372</td>\n",
       "      <td>0.203615</td>\n",
       "      <td>0.523380</td>\n",
       "      <td>-0.556507</td>\n",
       "      <td>0.565145</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.057834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>-1.405010</td>\n",
       "      <td>2.413021</td>\n",
       "      <td>-1.302346</td>\n",
       "      <td>-1.454188</td>\n",
       "      <td>-1.071259</td>\n",
       "      <td>-1.011963</td>\n",
       "      <td>0.360309</td>\n",
       "      <td>0.418295</td>\n",
       "      <td>-0.443522</td>\n",
       "      <td>-0.920593</td>\n",
       "      <td>0.135602</td>\n",
       "      <td>-0.436350</td>\n",
       "      <td>0.596886</td>\n",
       "      <td>-1.110165</td>\n",
       "      <td>0.819904</td>\n",
       "      <td>0.142605</td>\n",
       "      <td>0.158831</td>\n",
       "      <td>0.040089</td>\n",
       "      <td>0.376807</td>\n",
       "      <td>0.554216</td>\n",
       "      <td>-0.054865</td>\n",
       "      <td>0.106447</td>\n",
       "      <td>0.042179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>-0.350725</td>\n",
       "      <td>2.705242</td>\n",
       "      <td>0.145452</td>\n",
       "      <td>-0.643345</td>\n",
       "      <td>1.667632</td>\n",
       "      <td>-0.533822</td>\n",
       "      <td>0.754887</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.280290</td>\n",
       "      <td>-0.543913</td>\n",
       "      <td>0.974087</td>\n",
       "      <td>0.841912</td>\n",
       "      <td>0.526558</td>\n",
       "      <td>0.049939</td>\n",
       "      <td>-0.601751</td>\n",
       "      <td>-0.228424</td>\n",
       "      <td>-0.437358</td>\n",
       "      <td>0.394419</td>\n",
       "      <td>0.313277</td>\n",
       "      <td>-1.242686</td>\n",
       "      <td>0.251948</td>\n",
       "      <td>0.312916</td>\n",
       "      <td>0.119278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>-0.014649</td>\n",
       "      <td>3.025190</td>\n",
       "      <td>0.624679</td>\n",
       "      <td>-0.347438</td>\n",
       "      <td>-1.288627</td>\n",
       "      <td>-0.457380</td>\n",
       "      <td>1.237701</td>\n",
       "      <td>-0.423323</td>\n",
       "      <td>-1.149698</td>\n",
       "      <td>-0.241622</td>\n",
       "      <td>-0.104638</td>\n",
       "      <td>-0.340139</td>\n",
       "      <td>-0.787915</td>\n",
       "      <td>-0.033955</td>\n",
       "      <td>-0.164886</td>\n",
       "      <td>0.695858</td>\n",
       "      <td>0.131704</td>\n",
       "      <td>-0.527047</td>\n",
       "      <td>-0.547125</td>\n",
       "      <td>0.333059</td>\n",
       "      <td>-0.168167</td>\n",
       "      <td>0.064260</td>\n",
       "      <td>0.046469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.028352</td>\n",
       "      <td>2.755075</td>\n",
       "      <td>-1.781529</td>\n",
       "      <td>0.560739</td>\n",
       "      <td>0.518674</td>\n",
       "      <td>1.804232</td>\n",
       "      <td>0.127990</td>\n",
       "      <td>0.419765</td>\n",
       "      <td>-0.191832</td>\n",
       "      <td>-0.557436</td>\n",
       "      <td>1.632630</td>\n",
       "      <td>-0.220480</td>\n",
       "      <td>-0.065870</td>\n",
       "      <td>-0.516325</td>\n",
       "      <td>-0.070414</td>\n",
       "      <td>-0.039190</td>\n",
       "      <td>0.523172</td>\n",
       "      <td>-0.405879</td>\n",
       "      <td>-0.051170</td>\n",
       "      <td>-0.280987</td>\n",
       "      <td>-0.161524</td>\n",
       "      <td>0.176334</td>\n",
       "      <td>-0.259446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>-1.980907</td>\n",
       "      <td>2.691795</td>\n",
       "      <td>0.684960</td>\n",
       "      <td>-0.728747</td>\n",
       "      <td>-0.968568</td>\n",
       "      <td>0.335603</td>\n",
       "      <td>0.018450</td>\n",
       "      <td>-0.541076</td>\n",
       "      <td>-0.039944</td>\n",
       "      <td>-0.478036</td>\n",
       "      <td>-1.079727</td>\n",
       "      <td>0.738774</td>\n",
       "      <td>-0.191886</td>\n",
       "      <td>0.354994</td>\n",
       "      <td>-0.483782</td>\n",
       "      <td>0.035258</td>\n",
       "      <td>0.725959</td>\n",
       "      <td>-0.460997</td>\n",
       "      <td>-0.192947</td>\n",
       "      <td>-0.147811</td>\n",
       "      <td>-0.695709</td>\n",
       "      <td>-0.970268</td>\n",
       "      <td>0.342696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>523 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    2.416422 -0.552812  0.131181  0.955582  0.054797 -0.020998 -1.592313   \n",
       "1    0.131354 -0.855773  1.585749  0.277939 -0.939847 -0.395470 -0.263718   \n",
       "2   -1.897192 -1.635107 -0.421177 -0.404517 -0.592611  0.447623  1.004420   \n",
       "3    0.802775 -1.014436 -1.492656  1.105121  0.010416 -1.161346 -0.260919   \n",
       "4   -0.227701 -1.150176  1.192468  0.116273  0.863749 -1.313552 -0.450934   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "518 -1.405010  2.413021 -1.302346 -1.454188 -1.071259 -1.011963  0.360309   \n",
       "519 -0.350725  2.705242  0.145452 -0.643345  1.667632 -0.533822  0.754887   \n",
       "520 -0.014649  3.025190  0.624679 -0.347438 -1.288627 -0.457380  1.237701   \n",
       "521  0.028352  2.755075 -1.781529  0.560739  0.518674  1.804232  0.127990   \n",
       "522 -1.980907  2.691795  0.684960 -0.728747 -0.968568  0.335603  0.018450   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0   -0.441373 -0.557078  0.159648 -0.284534  0.413303 -0.125948  0.453747   \n",
       "1   -0.916463  0.100369  0.197542 -1.024802  0.068028 -0.454410 -1.442645   \n",
       "2   -0.467284 -0.141209  0.407040  0.018325  0.595495  0.871430 -0.449192   \n",
       "3    0.495378 -0.180611  0.354332 -0.639869 -0.881045  0.955714 -0.484634   \n",
       "4   -0.257704 -1.475189  0.416207  0.280944 -1.263506 -0.542511  0.059127   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "518  0.418295 -0.443522 -0.920593  0.135602 -0.436350  0.596886 -1.110165   \n",
       "519 -0.994972 -1.280290 -0.543913  0.974087  0.841912  0.526558  0.049939   \n",
       "520 -0.423323 -1.149698 -0.241622 -0.104638 -0.340139 -0.787915 -0.033955   \n",
       "521  0.419765 -0.191832 -0.557436  1.632630 -0.220480 -0.065870 -0.516325   \n",
       "522 -0.541076 -0.039944 -0.478036 -1.079727  0.738774 -0.191886  0.354994   \n",
       "\n",
       "           14        15        16        17        18        19        20  \\\n",
       "0   -0.290805  0.545576 -0.413868  0.397605 -0.119679  0.778553  0.096271   \n",
       "1    1.027229 -0.009694  0.796664 -0.479693 -0.706045 -0.516156  0.131170   \n",
       "2    0.242426 -0.352698 -0.095071  0.331351  0.435195  0.147930  0.147314   \n",
       "3    0.331929  0.932466 -0.137698 -0.088760  0.172485 -0.181844 -0.280038   \n",
       "4   -0.144306  0.593525  0.132372  0.203615  0.523380 -0.556507  0.565145   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "518  0.819904  0.142605  0.158831  0.040089  0.376807  0.554216 -0.054865   \n",
       "519 -0.601751 -0.228424 -0.437358  0.394419  0.313277 -1.242686  0.251948   \n",
       "520 -0.164886  0.695858  0.131704 -0.527047 -0.547125  0.333059 -0.168167   \n",
       "521 -0.070414 -0.039190  0.523172 -0.405879 -0.051170 -0.280987 -0.161524   \n",
       "522 -0.483782  0.035258  0.725959 -0.460997 -0.192947 -0.147811 -0.695709   \n",
       "\n",
       "           21        22  \n",
       "0   -0.375248 -0.225443  \n",
       "1    0.190230 -0.317794  \n",
       "2   -0.449256 -0.752451  \n",
       "3    0.160299 -0.501305  \n",
       "4    0.001044  0.057834  \n",
       "..        ...       ...  \n",
       "518  0.106447  0.042179  \n",
       "519  0.312916  0.119278  \n",
       "520  0.064260  0.046469  \n",
       "521  0.176334 -0.259446  \n",
       "522 -0.970268  0.342696  \n",
       "\n",
       "[523 rows x 23 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a07cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reworked function to handle numpy arrays\n",
    "# Time series data modifier, will be used later\n",
    "\n",
    "def df_to_X_np(df, window_size=5):\n",
    "  df_as_np = df.to_numpy()\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df_as_np)-window_size):\n",
    "    row = [r for r in df_as_np[i:i+window_size]]\n",
    "    X.append(row)\n",
    "    label = df_as_np[i+window_size][0] # the y will get thrown out, but for some reason the function refuses to work without it there\n",
    "    y.append(label)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f9b0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_1, _ = df_to_X_np(train_X_0_1)\n",
    "val_X_0_1, _ = df_to_X_np(val_X_0_1)\n",
    "test_X_0_1, _ = df_to_X_np(test_X_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "958cb837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 11ms/step - loss: 0.7137 - accuracy: 0.4788 - val_loss: 0.6906 - val_accuracy: 0.5448\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.7031 - accuracy: 0.4788 - val_loss: 0.6902 - val_accuracy: 0.5448\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.4942 - val_loss: 0.6901 - val_accuracy: 0.5310\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5039 - val_loss: 0.6931 - val_accuracy: 0.4828\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5270 - val_loss: 0.6937 - val_accuracy: 0.4621\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6859 - accuracy: 0.5425 - val_loss: 0.6936 - val_accuracy: 0.4690\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6832 - accuracy: 0.5425 - val_loss: 0.6927 - val_accuracy: 0.4897\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.5541 - val_loss: 0.6917 - val_accuracy: 0.5448\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6778 - accuracy: 0.5695 - val_loss: 0.6967 - val_accuracy: 0.4552\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6778 - accuracy: 0.5541 - val_loss: 0.6962 - val_accuracy: 0.4690\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.5772 - val_loss: 0.6952 - val_accuracy: 0.4690\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.5598 - val_loss: 0.6984 - val_accuracy: 0.4552\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6707 - accuracy: 0.5849 - val_loss: 0.6972 - val_accuracy: 0.4414\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.5869 - val_loss: 0.6945 - val_accuracy: 0.4828\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.5965 - val_loss: 0.7009 - val_accuracy: 0.4483\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6648 - accuracy: 0.5888 - val_loss: 0.7028 - val_accuracy: 0.4414\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.5830 - val_loss: 0.7017 - val_accuracy: 0.4483\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6598 - accuracy: 0.6100 - val_loss: 0.6998 - val_accuracy: 0.4552\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6581 - accuracy: 0.6023 - val_loss: 0.6985 - val_accuracy: 0.4621\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.6120 - val_loss: 0.7008 - val_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f649ed00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_1.shape[1]\n",
    "n_features = train_X_0_1.shape[2]\n",
    "\n",
    "model_0_1_1 = Sequential()\n",
    "model_0_1_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_1_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_1.add(Flatten())\n",
    "model_0_1_1.add(Dense(50, activation='relu')) \n",
    "model_0_1_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_1_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_1.fit(train_X_0_1, train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55b67373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 0.7015 - accuracy: 0.5270 - val_loss: 0.6948 - val_accuracy: 0.5448\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.6081 - val_loss: 0.7055 - val_accuracy: 0.4690\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6330 - accuracy: 0.6795 - val_loss: 0.7016 - val_accuracy: 0.4966\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6159 - accuracy: 0.6834 - val_loss: 0.7015 - val_accuracy: 0.5310\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5966 - accuracy: 0.7104 - val_loss: 0.7108 - val_accuracy: 0.4759\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5661 - accuracy: 0.7973 - val_loss: 0.7104 - val_accuracy: 0.5172\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5462 - accuracy: 0.7934 - val_loss: 0.7213 - val_accuracy: 0.5034\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5222 - accuracy: 0.8243 - val_loss: 0.7362 - val_accuracy: 0.4828\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5056 - accuracy: 0.8205 - val_loss: 0.7256 - val_accuracy: 0.5172\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4737 - accuracy: 0.8552 - val_loss: 0.7530 - val_accuracy: 0.5034\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.8591 - val_loss: 0.7516 - val_accuracy: 0.5172\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4318 - accuracy: 0.8726 - val_loss: 0.7627 - val_accuracy: 0.5310\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4044 - accuracy: 0.8764 - val_loss: 0.7710 - val_accuracy: 0.5310\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3819 - accuracy: 0.8938 - val_loss: 0.7760 - val_accuracy: 0.5448\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3569 - accuracy: 0.9073 - val_loss: 0.8080 - val_accuracy: 0.4966\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.9054 - val_loss: 0.8775 - val_accuracy: 0.5310\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3034 - accuracy: 0.9305 - val_loss: 0.8409 - val_accuracy: 0.5241\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.9266 - val_loss: 0.8816 - val_accuracy: 0.5310\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.9517 - val_loss: 0.8881 - val_accuracy: 0.5379\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2363 - accuracy: 0.9633 - val_loss: 0.9628 - val_accuracy: 0.5379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f66d2ee0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_2 = Sequential()\n",
    "model_0_1_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_1_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_2.add(Flatten())\n",
    "model_0_1_2.add(Dense(50, activation='relu')) \n",
    "model_0_1_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_1_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_2.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14b830b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 14ms/step - loss: 0.7123 - accuracy: 0.4826 - val_loss: 0.6960 - val_accuracy: 0.5379\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5772 - val_loss: 0.7043 - val_accuracy: 0.4138\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6682 - accuracy: 0.6448 - val_loss: 0.7067 - val_accuracy: 0.4414\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.6911 - val_loss: 0.7095 - val_accuracy: 0.4552\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6351 - accuracy: 0.7085 - val_loss: 0.7157 - val_accuracy: 0.4552\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6163 - accuracy: 0.7432 - val_loss: 0.7265 - val_accuracy: 0.4069\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5924 - accuracy: 0.7510 - val_loss: 0.7331 - val_accuracy: 0.4276\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5623 - accuracy: 0.8012 - val_loss: 0.7509 - val_accuracy: 0.4345\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5279 - accuracy: 0.8069 - val_loss: 0.7639 - val_accuracy: 0.4414\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.8243 - val_loss: 0.8124 - val_accuracy: 0.4345\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4531 - accuracy: 0.8417 - val_loss: 0.8178 - val_accuracy: 0.4483\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4059 - accuracy: 0.8707 - val_loss: 0.8758 - val_accuracy: 0.4276\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3752 - accuracy: 0.8803 - val_loss: 0.9078 - val_accuracy: 0.4552\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3376 - accuracy: 0.8900 - val_loss: 1.0308 - val_accuracy: 0.4207\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3024 - accuracy: 0.9015 - val_loss: 1.0343 - val_accuracy: 0.4207\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2748 - accuracy: 0.9151 - val_loss: 1.0608 - val_accuracy: 0.4207\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2327 - accuracy: 0.9459 - val_loss: 1.0976 - val_accuracy: 0.4483\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2100 - accuracy: 0.9421 - val_loss: 1.1645 - val_accuracy: 0.4483\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.1894 - accuracy: 0.9614 - val_loss: 1.2353 - val_accuracy: 0.4759\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9672 - val_loss: 1.3056 - val_accuracy: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f72ec9a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_3 = Sequential()\n",
    "model_0_1_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_3.add(Flatten())\n",
    "model_0_1_3.add(Dense(50, activation='relu')) \n",
    "model_0_1_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_1_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_3.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61279601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 2s 25ms/step - loss: 0.6951 - accuracy: 0.4749 - val_loss: 0.6966 - val_accuracy: 0.4138\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.6875 - accuracy: 0.5772 - val_loss: 0.6974 - val_accuracy: 0.4345\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6801 - accuracy: 0.6004 - val_loss: 0.6995 - val_accuracy: 0.4276\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6667 - accuracy: 0.6757 - val_loss: 0.7005 - val_accuracy: 0.4828\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6447 - accuracy: 0.6969 - val_loss: 0.7161 - val_accuracy: 0.4345\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6150 - accuracy: 0.7239 - val_loss: 0.7201 - val_accuracy: 0.4759\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5687 - accuracy: 0.7529 - val_loss: 0.7530 - val_accuracy: 0.4207\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5033 - accuracy: 0.7992 - val_loss: 0.8057 - val_accuracy: 0.4345\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4286 - accuracy: 0.8243 - val_loss: 0.8364 - val_accuracy: 0.4690\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.3516 - accuracy: 0.8707 - val_loss: 0.8988 - val_accuracy: 0.4552\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.8977 - val_loss: 0.9801 - val_accuracy: 0.4690\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2849 - accuracy: 0.8764 - val_loss: 1.0759 - val_accuracy: 0.4828\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.2391 - accuracy: 0.9170 - val_loss: 1.1220 - val_accuracy: 0.4207\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.1772 - accuracy: 0.9595 - val_loss: 1.1235 - val_accuracy: 0.4483\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.1262 - accuracy: 0.9788 - val_loss: 1.1447 - val_accuracy: 0.4966\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.0889 - accuracy: 0.9942 - val_loss: 1.2890 - val_accuracy: 0.4552\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9981 - val_loss: 1.3374 - val_accuracy: 0.4828\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0463 - accuracy: 0.9981 - val_loss: 1.5877 - val_accuracy: 0.4828\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 1.0000 - val_loss: 1.5456 - val_accuracy: 0.5103\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 1.6326 - val_accuracy: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f8f90190>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_4 = Sequential()\n",
    "model_0_1_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_1_4.add(Flatten())\n",
    "model_0_1_4.add(Dense(50, activation='relu')) \n",
    "model_0_1_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_1_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_4.fit(train_X_0_1,train_f0t_tc,epochs=20,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78b60c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 0.6966 - accuracy: 0.4749 - val_loss: 0.6926 - val_accuracy: 0.4759\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.6236 - val_loss: 0.6934 - val_accuracy: 0.4828\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.6197 - val_loss: 0.6965 - val_accuracy: 0.5103\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6501 - accuracy: 0.7239 - val_loss: 0.7023 - val_accuracy: 0.4759\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.7587 - val_loss: 0.7197 - val_accuracy: 0.4759\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5609 - accuracy: 0.7548 - val_loss: 0.7575 - val_accuracy: 0.5034\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4922 - accuracy: 0.7992 - val_loss: 0.8079 - val_accuracy: 0.4483\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4145 - accuracy: 0.8494 - val_loss: 0.8277 - val_accuracy: 0.4897\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8784 - val_loss: 0.9246 - val_accuracy: 0.4414\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.2727 - accuracy: 0.8938 - val_loss: 0.9766 - val_accuracy: 0.4897\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2089 - accuracy: 0.9402 - val_loss: 1.0884 - val_accuracy: 0.4552\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9595 - val_loss: 1.1319 - val_accuracy: 0.4828\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.1363 - accuracy: 0.9614 - val_loss: 1.2639 - val_accuracy: 0.4897\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0945 - accuracy: 0.9788 - val_loss: 1.2797 - val_accuracy: 0.4897\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9923 - val_loss: 1.4393 - val_accuracy: 0.5172\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9981 - val_loss: 1.5696 - val_accuracy: 0.4966\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9981 - val_loss: 1.5840 - val_accuracy: 0.5103\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 1.6010 - val_accuracy: 0.5034\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.7001 - val_accuracy: 0.5034\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.7439 - val_accuracy: 0.5103\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 1.8851 - val_accuracy: 0.4966\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.8669 - val_accuracy: 0.4966\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.9210 - val_accuracy: 0.4828\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.9715 - val_accuracy: 0.5103\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.0132 - val_accuracy: 0.5034\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.0284 - val_accuracy: 0.5103\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.0607 - val_accuracy: 0.5103\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.0839 - val_accuracy: 0.5172\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.1324 - val_accuracy: 0.5034\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 2.1802 - val_accuracy: 0.4828\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.1805 - val_accuracy: 0.5103\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.2249 - val_accuracy: 0.4966\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.2592 - val_accuracy: 0.4966\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2764 - val_accuracy: 0.5034\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.2900 - val_accuracy: 0.5034\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.3127 - val_accuracy: 0.4966\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.3192 - val_accuracy: 0.5034\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.3957 - val_accuracy: 0.4897\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.8640e-04 - accuracy: 1.0000 - val_loss: 2.4210 - val_accuracy: 0.4966\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.1227e-04 - accuracy: 1.0000 - val_loss: 2.4467 - val_accuracy: 0.4966\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.4580e-04 - accuracy: 1.0000 - val_loss: 2.5012 - val_accuracy: 0.4966\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.6618e-04 - accuracy: 1.0000 - val_loss: 2.5171 - val_accuracy: 0.4966\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.0667e-04 - accuracy: 1.0000 - val_loss: 2.5409 - val_accuracy: 0.4966\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.5811e-04 - accuracy: 1.0000 - val_loss: 2.6110 - val_accuracy: 0.4966\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.2105e-04 - accuracy: 1.0000 - val_loss: 2.5751 - val_accuracy: 0.5241\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.9949e-04 - accuracy: 1.0000 - val_loss: 2.6516 - val_accuracy: 0.4966\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.1933e-04 - accuracy: 1.0000 - val_loss: 2.6458 - val_accuracy: 0.5103\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.9053e-04 - accuracy: 1.0000 - val_loss: 2.6903 - val_accuracy: 0.4966\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.5915e-04 - accuracy: 1.0000 - val_loss: 2.7071 - val_accuracy: 0.5103\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.3102e-04 - accuracy: 1.0000 - val_loss: 2.7453 - val_accuracy: 0.4966\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 3.0489e-04 - accuracy: 1.0000 - val_loss: 2.7767 - val_accuracy: 0.4966\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.8345e-04 - accuracy: 1.0000 - val_loss: 2.7822 - val_accuracy: 0.5034\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.6157e-04 - accuracy: 1.0000 - val_loss: 2.8189 - val_accuracy: 0.4966\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.4991e-04 - accuracy: 1.0000 - val_loss: 2.8140 - val_accuracy: 0.5172\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.2969e-04 - accuracy: 1.0000 - val_loss: 2.8736 - val_accuracy: 0.4966\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.1065e-04 - accuracy: 1.0000 - val_loss: 2.8685 - val_accuracy: 0.5172\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.9952e-04 - accuracy: 1.0000 - val_loss: 2.9068 - val_accuracy: 0.4966\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.8544e-04 - accuracy: 1.0000 - val_loss: 2.9245 - val_accuracy: 0.5034\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.7404e-04 - accuracy: 1.0000 - val_loss: 2.9449 - val_accuracy: 0.5034\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.6317e-04 - accuracy: 1.0000 - val_loss: 2.9632 - val_accuracy: 0.5034\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.5439e-04 - accuracy: 1.0000 - val_loss: 2.9775 - val_accuracy: 0.5103\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.4414e-04 - accuracy: 1.0000 - val_loss: 3.0094 - val_accuracy: 0.5034\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.3648e-04 - accuracy: 1.0000 - val_loss: 3.0154 - val_accuracy: 0.5103\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3037e-04 - accuracy: 1.0000 - val_loss: 3.0407 - val_accuracy: 0.4966\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2245e-04 - accuracy: 1.0000 - val_loss: 3.0539 - val_accuracy: 0.5034\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.1651e-04 - accuracy: 1.0000 - val_loss: 3.0643 - val_accuracy: 0.5103\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.1008e-04 - accuracy: 1.0000 - val_loss: 3.0853 - val_accuracy: 0.5034\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.0542e-04 - accuracy: 1.0000 - val_loss: 3.1070 - val_accuracy: 0.5034\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.9453e-05 - accuracy: 1.0000 - val_loss: 3.1179 - val_accuracy: 0.5034\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.5480e-05 - accuracy: 1.0000 - val_loss: 3.1422 - val_accuracy: 0.5034\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.1925e-05 - accuracy: 1.0000 - val_loss: 3.1347 - val_accuracy: 0.5103\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.9198e-05 - accuracy: 1.0000 - val_loss: 3.1751 - val_accuracy: 0.4897\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.2604e-05 - accuracy: 1.0000 - val_loss: 3.1788 - val_accuracy: 0.5034\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.7908e-05 - accuracy: 1.0000 - val_loss: 3.1980 - val_accuracy: 0.5034\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.4631e-05 - accuracy: 1.0000 - val_loss: 3.2076 - val_accuracy: 0.5034\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.1443e-05 - accuracy: 1.0000 - val_loss: 3.2256 - val_accuracy: 0.5034\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.8403e-05 - accuracy: 1.0000 - val_loss: 3.2516 - val_accuracy: 0.5034\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.5587e-05 - accuracy: 1.0000 - val_loss: 3.2502 - val_accuracy: 0.5034\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.2924e-05 - accuracy: 1.0000 - val_loss: 3.2731 - val_accuracy: 0.5034\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.0991e-05 - accuracy: 1.0000 - val_loss: 3.2767 - val_accuracy: 0.5034\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.7943e-05 - accuracy: 1.0000 - val_loss: 3.2863 - val_accuracy: 0.5103\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.5798e-05 - accuracy: 1.0000 - val_loss: 3.3063 - val_accuracy: 0.5034\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.3659e-05 - accuracy: 1.0000 - val_loss: 3.3188 - val_accuracy: 0.5034\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.1532e-05 - accuracy: 1.0000 - val_loss: 3.3170 - val_accuracy: 0.5103\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 5.0211e-05 - accuracy: 1.0000 - val_loss: 3.3440 - val_accuracy: 0.5034\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4.8043e-05 - accuracy: 1.0000 - val_loss: 3.3467 - val_accuracy: 0.5034\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4.6336e-05 - accuracy: 1.0000 - val_loss: 3.3697 - val_accuracy: 0.5034\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 4.4798e-05 - accuracy: 1.0000 - val_loss: 3.3704 - val_accuracy: 0.5103\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.3061e-05 - accuracy: 1.0000 - val_loss: 3.3926 - val_accuracy: 0.5034\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.1623e-05 - accuracy: 1.0000 - val_loss: 3.3972 - val_accuracy: 0.5034\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.0425e-05 - accuracy: 1.0000 - val_loss: 3.4026 - val_accuracy: 0.5103\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.8981e-05 - accuracy: 1.0000 - val_loss: 3.4288 - val_accuracy: 0.4966\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.7812e-05 - accuracy: 1.0000 - val_loss: 3.4282 - val_accuracy: 0.5103\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.6436e-05 - accuracy: 1.0000 - val_loss: 3.4493 - val_accuracy: 0.5034\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.5378e-05 - accuracy: 1.0000 - val_loss: 3.4462 - val_accuracy: 0.5103\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.4289e-05 - accuracy: 1.0000 - val_loss: 3.4672 - val_accuracy: 0.5034\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.3121e-05 - accuracy: 1.0000 - val_loss: 3.4693 - val_accuracy: 0.5103\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.2429e-05 - accuracy: 1.0000 - val_loss: 3.4706 - val_accuracy: 0.5103\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.1280e-05 - accuracy: 1.0000 - val_loss: 3.4981 - val_accuracy: 0.4966\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.0390e-05 - accuracy: 1.0000 - val_loss: 3.5016 - val_accuracy: 0.5034\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.9542e-05 - accuracy: 1.0000 - val_loss: 3.5127 - val_accuracy: 0.5034\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.8537e-05 - accuracy: 1.0000 - val_loss: 3.5091 - val_accuracy: 0.5103\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.7798e-05 - accuracy: 1.0000 - val_loss: 3.5338 - val_accuracy: 0.4966\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.7109e-05 - accuracy: 1.0000 - val_loss: 3.5386 - val_accuracy: 0.5034\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.6295e-05 - accuracy: 1.0000 - val_loss: 3.5531 - val_accuracy: 0.4966\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.5502e-05 - accuracy: 1.0000 - val_loss: 3.5486 - val_accuracy: 0.5103\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.4910e-05 - accuracy: 1.0000 - val_loss: 3.5606 - val_accuracy: 0.4966\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.4220e-05 - accuracy: 1.0000 - val_loss: 3.5719 - val_accuracy: 0.4966\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3561e-05 - accuracy: 1.0000 - val_loss: 3.5845 - val_accuracy: 0.4966\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.2947e-05 - accuracy: 1.0000 - val_loss: 3.5943 - val_accuracy: 0.4966\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.2387e-05 - accuracy: 1.0000 - val_loss: 3.6055 - val_accuracy: 0.4966\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.1941e-05 - accuracy: 1.0000 - val_loss: 3.6065 - val_accuracy: 0.4966\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.1241e-05 - accuracy: 1.0000 - val_loss: 3.6226 - val_accuracy: 0.4966\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.0683e-05 - accuracy: 1.0000 - val_loss: 3.6193 - val_accuracy: 0.5034\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.0232e-05 - accuracy: 1.0000 - val_loss: 3.6285 - val_accuracy: 0.5034\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.9723e-05 - accuracy: 1.0000 - val_loss: 3.6414 - val_accuracy: 0.4966\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.9255e-05 - accuracy: 1.0000 - val_loss: 3.6535 - val_accuracy: 0.4966\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.8806e-05 - accuracy: 1.0000 - val_loss: 3.6580 - val_accuracy: 0.4966\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.8408e-05 - accuracy: 1.0000 - val_loss: 3.6558 - val_accuracy: 0.5034\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.7940e-05 - accuracy: 1.0000 - val_loss: 3.6728 - val_accuracy: 0.4966\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.7507e-05 - accuracy: 1.0000 - val_loss: 3.6861 - val_accuracy: 0.4966\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.7091e-05 - accuracy: 1.0000 - val_loss: 3.6877 - val_accuracy: 0.4966\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.6929e-05 - accuracy: 1.0000 - val_loss: 3.6862 - val_accuracy: 0.5034\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.6352e-05 - accuracy: 1.0000 - val_loss: 3.7047 - val_accuracy: 0.4966\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.5990e-05 - accuracy: 1.0000 - val_loss: 3.7028 - val_accuracy: 0.4966\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.5676e-05 - accuracy: 1.0000 - val_loss: 3.7144 - val_accuracy: 0.4966\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.5099e-05 - accuracy: 1.0000 - val_loss: 3.7360 - val_accuracy: 0.4966\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.3898e-05 - accuracy: 1.0000 - val_loss: 3.7434 - val_accuracy: 0.5103\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2890e-05 - accuracy: 1.0000 - val_loss: 3.7802 - val_accuracy: 0.5034\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.1646e-05 - accuracy: 1.0000 - val_loss: 3.8137 - val_accuracy: 0.5034\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.0448e-05 - accuracy: 1.0000 - val_loss: 3.8556 - val_accuracy: 0.5034\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.3952e-06 - accuracy: 1.0000 - val_loss: 3.8825 - val_accuracy: 0.5034\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.3337e-06 - accuracy: 1.0000 - val_loss: 3.9209 - val_accuracy: 0.5034\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.4750e-06 - accuracy: 1.0000 - val_loss: 3.9576 - val_accuracy: 0.5034\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.7641e-06 - accuracy: 1.0000 - val_loss: 3.9925 - val_accuracy: 0.5034\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.1098e-06 - accuracy: 1.0000 - val_loss: 4.0218 - val_accuracy: 0.5034\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.5903e-06 - accuracy: 1.0000 - val_loss: 4.0566 - val_accuracy: 0.5034\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.0903e-06 - accuracy: 1.0000 - val_loss: 4.0895 - val_accuracy: 0.5034\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.6795e-06 - accuracy: 1.0000 - val_loss: 4.1157 - val_accuracy: 0.5034\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 4.3009e-06 - accuracy: 1.0000 - val_loss: 4.1446 - val_accuracy: 0.5034\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.9998e-06 - accuracy: 1.0000 - val_loss: 4.1696 - val_accuracy: 0.5034\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.7226e-06 - accuracy: 1.0000 - val_loss: 4.1962 - val_accuracy: 0.5034\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.4509e-06 - accuracy: 1.0000 - val_loss: 4.2190 - val_accuracy: 0.5034\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.2235e-06 - accuracy: 1.0000 - val_loss: 4.2465 - val_accuracy: 0.5034\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.0224e-06 - accuracy: 1.0000 - val_loss: 4.2695 - val_accuracy: 0.4966\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.8597e-06 - accuracy: 1.0000 - val_loss: 4.2887 - val_accuracy: 0.5034\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.6995e-06 - accuracy: 1.0000 - val_loss: 4.3062 - val_accuracy: 0.5034\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.5547e-06 - accuracy: 1.0000 - val_loss: 4.3283 - val_accuracy: 0.5034\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.4256e-06 - accuracy: 1.0000 - val_loss: 4.3520 - val_accuracy: 0.4966\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3045e-06 - accuracy: 1.0000 - val_loss: 4.3668 - val_accuracy: 0.5034\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.1951e-06 - accuracy: 1.0000 - val_loss: 4.3817 - val_accuracy: 0.5034\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.0980e-06 - accuracy: 1.0000 - val_loss: 4.4056 - val_accuracy: 0.4966\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.0086e-06 - accuracy: 1.0000 - val_loss: 4.4180 - val_accuracy: 0.5034\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.9153e-06 - accuracy: 1.0000 - val_loss: 4.4343 - val_accuracy: 0.5034\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.8328e-06 - accuracy: 1.0000 - val_loss: 4.4517 - val_accuracy: 0.5034\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.7611e-06 - accuracy: 1.0000 - val_loss: 4.4684 - val_accuracy: 0.4966\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.6918e-06 - accuracy: 1.0000 - val_loss: 4.4875 - val_accuracy: 0.4966\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.6291e-06 - accuracy: 1.0000 - val_loss: 4.4993 - val_accuracy: 0.4966\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.5643e-06 - accuracy: 1.0000 - val_loss: 4.5127 - val_accuracy: 0.4966\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.5096e-06 - accuracy: 1.0000 - val_loss: 4.5269 - val_accuracy: 0.4966\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.4552e-06 - accuracy: 1.0000 - val_loss: 4.5399 - val_accuracy: 0.4966\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.4066e-06 - accuracy: 1.0000 - val_loss: 4.5520 - val_accuracy: 0.5034\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.3571e-06 - accuracy: 1.0000 - val_loss: 4.5674 - val_accuracy: 0.4966\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.3140e-06 - accuracy: 1.0000 - val_loss: 4.5848 - val_accuracy: 0.4966\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.2724e-06 - accuracy: 1.0000 - val_loss: 4.5945 - val_accuracy: 0.4966\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.2298e-06 - accuracy: 1.0000 - val_loss: 4.6068 - val_accuracy: 0.4966\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.1924e-06 - accuracy: 1.0000 - val_loss: 4.6209 - val_accuracy: 0.4966\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 1.1557e-06 - accuracy: 1.0000 - val_loss: 4.6313 - val_accuracy: 0.4966\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 1.1206e-06 - accuracy: 1.0000 - val_loss: 4.6424 - val_accuracy: 0.4966\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 1.0938e-06 - accuracy: 1.0000 - val_loss: 4.6489 - val_accuracy: 0.5034\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 1.0584e-06 - accuracy: 1.0000 - val_loss: 4.6665 - val_accuracy: 0.4966\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 1.0245e-06 - accuracy: 1.0000 - val_loss: 4.6798 - val_accuracy: 0.4966\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.9581e-07 - accuracy: 1.0000 - val_loss: 4.6935 - val_accuracy: 0.4966\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.6837e-07 - accuracy: 1.0000 - val_loss: 4.7040 - val_accuracy: 0.4966\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.4091e-07 - accuracy: 1.0000 - val_loss: 4.7140 - val_accuracy: 0.4966\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.1625e-07 - accuracy: 1.0000 - val_loss: 4.7252 - val_accuracy: 0.4966\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 8.9253e-07 - accuracy: 1.0000 - val_loss: 4.7335 - val_accuracy: 0.4966\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 8.7040e-07 - accuracy: 1.0000 - val_loss: 4.7442 - val_accuracy: 0.4966\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.4926e-07 - accuracy: 1.0000 - val_loss: 4.7545 - val_accuracy: 0.4966\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 8.2851e-07 - accuracy: 1.0000 - val_loss: 4.7637 - val_accuracy: 0.4966\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0896e-07 - accuracy: 1.0000 - val_loss: 4.7744 - val_accuracy: 0.4966\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.8983e-07 - accuracy: 1.0000 - val_loss: 4.7836 - val_accuracy: 0.4966\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.7164e-07 - accuracy: 1.0000 - val_loss: 4.7934 - val_accuracy: 0.4966\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.5505e-07 - accuracy: 1.0000 - val_loss: 4.8053 - val_accuracy: 0.4897\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.3788e-07 - accuracy: 1.0000 - val_loss: 4.8117 - val_accuracy: 0.4966\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 7.1781e-07 - accuracy: 1.0000 - val_loss: 4.8231 - val_accuracy: 0.4966\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 7.0187e-07 - accuracy: 1.0000 - val_loss: 4.8316 - val_accuracy: 0.4966\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.8734e-07 - accuracy: 1.0000 - val_loss: 4.8416 - val_accuracy: 0.4897\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.7107e-07 - accuracy: 1.0000 - val_loss: 4.8507 - val_accuracy: 0.4897\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.5757e-07 - accuracy: 1.0000 - val_loss: 4.8588 - val_accuracy: 0.4897\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.4387e-07 - accuracy: 1.0000 - val_loss: 4.8682 - val_accuracy: 0.4897\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 6.3117e-07 - accuracy: 1.0000 - val_loss: 4.8823 - val_accuracy: 0.4897\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 6.1892e-07 - accuracy: 1.0000 - val_loss: 4.8885 - val_accuracy: 0.4897\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 6.0399e-07 - accuracy: 1.0000 - val_loss: 4.8944 - val_accuracy: 0.4897\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.9103e-07 - accuracy: 1.0000 - val_loss: 4.9011 - val_accuracy: 0.4897\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.7938e-07 - accuracy: 1.0000 - val_loss: 4.9085 - val_accuracy: 0.4897\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.6827e-07 - accuracy: 1.0000 - val_loss: 4.9174 - val_accuracy: 0.4897\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.5809e-07 - accuracy: 1.0000 - val_loss: 4.9280 - val_accuracy: 0.4897\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 5.4652e-07 - accuracy: 1.0000 - val_loss: 4.9346 - val_accuracy: 0.4897\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 5.3560e-07 - accuracy: 1.0000 - val_loss: 4.9417 - val_accuracy: 0.4897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f896e910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_1_5 = Sequential()\n",
    "model_0_1_5.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_1_5.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_1_5.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_1_5.add(Flatten())\n",
    "model_0_1_5.add(Dense(50, activation='relu')) \n",
    "model_0_1_5.add(Dense(1, activation='sigmoid')) \n",
    "model_0_1_5.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_1_5.fit(train_X_0_1,train_f0t_tc,epochs=200,  validation_data=(val_X_0_1, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa754d7",
   "metadata": {},
   "source": [
    "### Model_0_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda6210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_0_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_0_2, _ = df_to_X_y2(train_f0[feats],train_f0t)\n",
    "val_X_0_2, _ = df_to_X_y2(val_f0[feats], val_f0t)\n",
    "test_X_0_2, _ = df_to_X_y2(test_f0[feats],test_f0t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bcfded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 0.6965 - accuracy: 0.5039 - val_loss: 0.7021 - val_accuracy: 0.4897\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.5135 - val_loss: 0.7018 - val_accuracy: 0.4552\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.5058 - val_loss: 0.6994 - val_accuracy: 0.4621\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6996 - val_accuracy: 0.4759\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.5386 - val_loss: 0.6994 - val_accuracy: 0.4759\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6882 - accuracy: 0.5541 - val_loss: 0.7006 - val_accuracy: 0.4759\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.5598 - val_loss: 0.7001 - val_accuracy: 0.4828\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.5753 - val_loss: 0.7017 - val_accuracy: 0.4690\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5695 - val_loss: 0.7016 - val_accuracy: 0.4966\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6831 - accuracy: 0.5502 - val_loss: 0.7068 - val_accuracy: 0.4828\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.5753 - val_loss: 0.7023 - val_accuracy: 0.4828\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.5772 - val_loss: 0.7062 - val_accuracy: 0.4897\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.5811 - val_loss: 0.7076 - val_accuracy: 0.4828\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.6042 - val_loss: 0.7076 - val_accuracy: 0.4897\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6746 - accuracy: 0.5965 - val_loss: 0.7089 - val_accuracy: 0.5172\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6731 - accuracy: 0.5927 - val_loss: 0.7129 - val_accuracy: 0.4690\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.5830 - val_loss: 0.7141 - val_accuracy: 0.4828\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5753 - val_loss: 0.7132 - val_accuracy: 0.4483\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.6023 - val_loss: 0.7173 - val_accuracy: 0.4897\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.5830 - val_loss: 0.7157 - val_accuracy: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f8c6f8b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_2.shape[1]\n",
    "n_features = train_X_0_2.shape[2]\n",
    "\n",
    "model_0_2_1 = Sequential()\n",
    "model_0_2_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_2_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_1.add(Flatten())\n",
    "model_0_2_1.add(Dense(50, activation='relu')) \n",
    "model_0_2_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_2_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_1.fit(train_X_0_2, train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4bf6fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 21ms/step - loss: 0.7036 - accuracy: 0.4575 - val_loss: 0.6997 - val_accuracy: 0.4621\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.5251 - val_loss: 0.7030 - val_accuracy: 0.4759\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.5907 - val_loss: 0.7022 - val_accuracy: 0.4966\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6737 - accuracy: 0.5888 - val_loss: 0.7039 - val_accuracy: 0.4897\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.5907 - val_loss: 0.7054 - val_accuracy: 0.5172\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6626 - accuracy: 0.6158 - val_loss: 0.7108 - val_accuracy: 0.4552\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.5869 - val_loss: 0.7055 - val_accuracy: 0.5172\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6565 - accuracy: 0.6236 - val_loss: 0.7103 - val_accuracy: 0.4897\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6420 - accuracy: 0.6236 - val_loss: 0.7107 - val_accuracy: 0.4690\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6556 - accuracy: 0.6139 - val_loss: 0.7203 - val_accuracy: 0.4345\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6410 - accuracy: 0.6486 - val_loss: 0.7293 - val_accuracy: 0.4759\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6168 - accuracy: 0.6641 - val_loss: 0.7236 - val_accuracy: 0.4966\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6084 - accuracy: 0.7046 - val_loss: 0.7295 - val_accuracy: 0.4759\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.7085 - val_loss: 0.7695 - val_accuracy: 0.4759\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5969 - accuracy: 0.6853 - val_loss: 0.7422 - val_accuracy: 0.4345\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5769 - accuracy: 0.7259 - val_loss: 0.7514 - val_accuracy: 0.4552\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5591 - accuracy: 0.7394 - val_loss: 0.7674 - val_accuracy: 0.4414\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5518 - accuracy: 0.7529 - val_loss: 0.7873 - val_accuracy: 0.4207\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5334 - accuracy: 0.7896 - val_loss: 0.7890 - val_accuracy: 0.3862\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7683 - val_loss: 0.7933 - val_accuracy: 0.4138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f96625b0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_2 = Sequential()\n",
    "model_0_2_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_2_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_2.add(Flatten())\n",
    "model_0_2_2.add(Dense(50, activation='relu')) \n",
    "model_0_2_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_2_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_2.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41c7a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 12ms/step - loss: 0.6964 - accuracy: 0.5135 - val_loss: 0.6990 - val_accuracy: 0.4828\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5463 - val_loss: 0.7183 - val_accuracy: 0.4759\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.5734 - val_loss: 0.7082 - val_accuracy: 0.4483\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6811 - accuracy: 0.5463 - val_loss: 0.7257 - val_accuracy: 0.4621\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6720 - accuracy: 0.6100 - val_loss: 0.7198 - val_accuracy: 0.4276\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6629 - accuracy: 0.6197 - val_loss: 0.7164 - val_accuracy: 0.4483\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6644 - accuracy: 0.6042 - val_loss: 0.7226 - val_accuracy: 0.4069\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6451 - accuracy: 0.6680 - val_loss: 0.7246 - val_accuracy: 0.4276\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6346 - accuracy: 0.6641 - val_loss: 0.7382 - val_accuracy: 0.4483\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6209 - accuracy: 0.6815 - val_loss: 0.7898 - val_accuracy: 0.4690\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6111 - accuracy: 0.6931 - val_loss: 0.8144 - val_accuracy: 0.4828\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6009 - accuracy: 0.7162 - val_loss: 0.8150 - val_accuracy: 0.4828\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5884 - accuracy: 0.7066 - val_loss: 0.7860 - val_accuracy: 0.4414\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7548 - val_loss: 0.7933 - val_accuracy: 0.3862\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5583 - accuracy: 0.7124 - val_loss: 0.8163 - val_accuracy: 0.3862\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.7780 - val_loss: 1.0257 - val_accuracy: 0.4690\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5354 - accuracy: 0.7413 - val_loss: 0.8501 - val_accuracy: 0.4207\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.7973 - val_loss: 1.0284 - val_accuracy: 0.4690\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5051 - accuracy: 0.7683 - val_loss: 0.8524 - val_accuracy: 0.4069\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4996 - accuracy: 0.7452 - val_loss: 1.0533 - val_accuracy: 0.4483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f7287130>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_3 = Sequential()\n",
    "model_0_2_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_2_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_3.add(Flatten())\n",
    "model_0_2_3.add(Dense(50, activation='relu')) \n",
    "model_0_2_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_2_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_3.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d0874fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 0.6963 - accuracy: 0.4749 - val_loss: 0.6913 - val_accuracy: 0.5310\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.4961 - val_loss: 0.6917 - val_accuracy: 0.5034\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5676 - val_loss: 0.6929 - val_accuracy: 0.5310\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5637 - val_loss: 0.6938 - val_accuracy: 0.5172\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5734 - val_loss: 0.7017 - val_accuracy: 0.4759\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5907 - val_loss: 0.7174 - val_accuracy: 0.4759\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.5849 - val_loss: 0.7040 - val_accuracy: 0.4897\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6714 - accuracy: 0.5811 - val_loss: 0.7252 - val_accuracy: 0.4828\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.6351 - val_loss: 0.7114 - val_accuracy: 0.4897\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6405 - accuracy: 0.6564 - val_loss: 0.7937 - val_accuracy: 0.4828\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6353 - accuracy: 0.6351 - val_loss: 0.7523 - val_accuracy: 0.5172\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.6873 - val_loss: 0.7320 - val_accuracy: 0.4897\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5892 - accuracy: 0.7162 - val_loss: 0.7430 - val_accuracy: 0.5172\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5519 - accuracy: 0.7490 - val_loss: 0.8136 - val_accuracy: 0.4828\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.7625 - val_loss: 0.8016 - val_accuracy: 0.4759\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4960 - accuracy: 0.7722 - val_loss: 0.8371 - val_accuracy: 0.4552\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4692 - accuracy: 0.7992 - val_loss: 0.8741 - val_accuracy: 0.4483\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8263 - val_loss: 0.8695 - val_accuracy: 0.4897\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8475 - val_loss: 0.9822 - val_accuracy: 0.4690\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3483 - accuracy: 0.8726 - val_loss: 1.0707 - val_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f2eb8fd0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_2_4 = Sequential()\n",
    "model_0_2_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_2_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_2_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_2_4.add(Flatten())\n",
    "model_0_2_4.add(Dense(50, activation='relu')) \n",
    "model_0_2_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_2_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_2_4.fit(train_X_0_2,train_f0t_tc,epochs=20,  validation_data=(val_X_0_2, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb55cb",
   "metadata": {},
   "source": [
    "Model_0_2_4 best, and most consistent so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5aada2",
   "metadata": {},
   "source": [
    "### Model_0_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b367f40",
   "metadata": {},
   "source": [
    "# Model_0_1 final data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e15e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA with 23 components to explain 85% of variance\n",
    "sklearn_pca = PCA(n_components=8)\n",
    "train_X_0_3 = pd.DataFrame(sklearn_pca.fit_transform(train_f0[feats]))\n",
    "val_X_0_3 = pd.DataFrame(sklearn_pca.transform(val_f0[feats]))\n",
    "test_X_0_3 = pd.DataFrame(sklearn_pca.transform(test_f0[feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab93c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_0_3, _ = df_to_X_np(train_X_0_3)\n",
    "val_X_0_3, _ = df_to_X_np(val_X_0_3)\n",
    "test_X_0_3, _ = df_to_X_np(test_X_0_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "252b2c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 11ms/step - loss: 0.6944 - accuracy: 0.5193 - val_loss: 0.7121 - val_accuracy: 0.4345\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5019 - val_loss: 0.7088 - val_accuracy: 0.4345\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5174 - val_loss: 0.7105 - val_accuracy: 0.4345\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.5444 - val_loss: 0.7123 - val_accuracy: 0.4345\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5444 - val_loss: 0.7093 - val_accuracy: 0.4552\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6852 - accuracy: 0.5444 - val_loss: 0.7103 - val_accuracy: 0.4621\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.5560 - val_loss: 0.7113 - val_accuracy: 0.4552\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6832 - accuracy: 0.5598 - val_loss: 0.7093 - val_accuracy: 0.4552\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6823 - accuracy: 0.5483 - val_loss: 0.7109 - val_accuracy: 0.4690\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5637 - val_loss: 0.7110 - val_accuracy: 0.4690\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6806 - accuracy: 0.5560 - val_loss: 0.7115 - val_accuracy: 0.4759\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.5618 - val_loss: 0.7124 - val_accuracy: 0.4828\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6792 - accuracy: 0.5753 - val_loss: 0.7149 - val_accuracy: 0.4966\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6780 - accuracy: 0.5849 - val_loss: 0.7144 - val_accuracy: 0.4828\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.5811 - val_loss: 0.7140 - val_accuracy: 0.4966\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6787 - accuracy: 0.5560 - val_loss: 0.7112 - val_accuracy: 0.4483\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6765 - accuracy: 0.5792 - val_loss: 0.7159 - val_accuracy: 0.4897\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6745 - accuracy: 0.5869 - val_loss: 0.7179 - val_accuracy: 0.4759\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6733 - accuracy: 0.5927 - val_loss: 0.7168 - val_accuracy: 0.4759\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.5753 - val_loss: 0.7154 - val_accuracy: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f824db50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = train_X_0_3.shape[1]\n",
    "n_features = train_X_0_3.shape[2]\n",
    "\n",
    "model_0_3_1 = Sequential()\n",
    "model_0_3_1.add(Conv1D(filters=5, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_3_1.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_1.add(Flatten())\n",
    "model_0_3_1.add(Dense(50, activation='relu')) \n",
    "model_0_3_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_3_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_1.fit(train_X_0_3, train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7327a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 0.7011 - accuracy: 0.5019 - val_loss: 0.6933 - val_accuracy: 0.5103\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5714 - val_loss: 0.6914 - val_accuracy: 0.5034\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6743 - accuracy: 0.6120 - val_loss: 0.6971 - val_accuracy: 0.5034\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.6023 - val_loss: 0.7046 - val_accuracy: 0.4828\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6571 - accuracy: 0.6892 - val_loss: 0.7018 - val_accuracy: 0.4483\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6500 - accuracy: 0.6873 - val_loss: 0.7033 - val_accuracy: 0.4966\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6398 - accuracy: 0.7124 - val_loss: 0.7075 - val_accuracy: 0.4897\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6302 - accuracy: 0.7220 - val_loss: 0.7051 - val_accuracy: 0.4621\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6239 - accuracy: 0.7452 - val_loss: 0.7232 - val_accuracy: 0.4690\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6092 - accuracy: 0.7471 - val_loss: 0.7100 - val_accuracy: 0.4828\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.5998 - accuracy: 0.7490 - val_loss: 0.7287 - val_accuracy: 0.4207\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5881 - accuracy: 0.7645 - val_loss: 0.7260 - val_accuracy: 0.4207\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5723 - accuracy: 0.7683 - val_loss: 0.7381 - val_accuracy: 0.4345\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5582 - accuracy: 0.7896 - val_loss: 0.7337 - val_accuracy: 0.4276\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.7915 - val_loss: 0.7599 - val_accuracy: 0.4414\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5307 - accuracy: 0.8050 - val_loss: 0.7517 - val_accuracy: 0.4138\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.5167 - accuracy: 0.7992 - val_loss: 0.7724 - val_accuracy: 0.4207\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5052 - accuracy: 0.8166 - val_loss: 0.7749 - val_accuracy: 0.4276\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.8127 - val_loss: 0.7740 - val_accuracy: 0.4207\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4763 - accuracy: 0.8301 - val_loss: 0.7816 - val_accuracy: 0.4345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0f948ee80>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_2 = Sequential()\n",
    "model_0_3_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_0_3_2.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_2.add(Flatten())\n",
    "model_0_3_2.add(Dense(50, activation='relu')) \n",
    "model_0_3_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_0_3_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_2.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ca8a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 12ms/step - loss: 0.6961 - accuracy: 0.5116 - val_loss: 0.6971 - val_accuracy: 0.4552\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.5753 - val_loss: 0.7022 - val_accuracy: 0.4483\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6725 - accuracy: 0.6409 - val_loss: 0.6988 - val_accuracy: 0.5172\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6661 - accuracy: 0.6023 - val_loss: 0.7051 - val_accuracy: 0.4552\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6615 - accuracy: 0.6158 - val_loss: 0.7122 - val_accuracy: 0.4207\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6447 - accuracy: 0.6448 - val_loss: 0.7115 - val_accuracy: 0.4966\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6330 - accuracy: 0.6737 - val_loss: 0.7280 - val_accuracy: 0.4345\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6209 - accuracy: 0.6815 - val_loss: 0.7326 - val_accuracy: 0.4483\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6007 - accuracy: 0.7239 - val_loss: 0.7332 - val_accuracy: 0.4759\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5806 - accuracy: 0.7471 - val_loss: 0.7489 - val_accuracy: 0.4276\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5561 - accuracy: 0.7683 - val_loss: 0.7596 - val_accuracy: 0.4414\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5311 - accuracy: 0.7876 - val_loss: 0.7763 - val_accuracy: 0.4276\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7819 - val_loss: 0.7944 - val_accuracy: 0.4414\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4878 - accuracy: 0.8012 - val_loss: 0.7990 - val_accuracy: 0.4138\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.8127 - val_loss: 0.8126 - val_accuracy: 0.4552\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4312 - accuracy: 0.8378 - val_loss: 0.8224 - val_accuracy: 0.4966\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4073 - accuracy: 0.8533 - val_loss: 0.8996 - val_accuracy: 0.4345\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3843 - accuracy: 0.8494 - val_loss: 0.9220 - val_accuracy: 0.4552\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8571 - val_loss: 0.9034 - val_accuracy: 0.4897\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.8938 - val_loss: 0.9376 - val_accuracy: 0.4897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0fa216790>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_3 = Sequential()\n",
    "model_0_3_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_3_3.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_3.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_3.add(Flatten())\n",
    "model_0_3_3.add(Dense(50, activation='relu')) \n",
    "model_0_3_3.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model_0_3_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_3.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b778221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 1s 14ms/step - loss: 0.6942 - accuracy: 0.4981 - val_loss: 0.6936 - val_accuracy: 0.5034\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6882 - accuracy: 0.5888 - val_loss: 0.6940 - val_accuracy: 0.4966\n",
      "Epoch 3/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.6699 - val_loss: 0.6958 - val_accuracy: 0.4552\n",
      "Epoch 4/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6712 - accuracy: 0.6486 - val_loss: 0.7008 - val_accuracy: 0.4759\n",
      "Epoch 5/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6514 - accuracy: 0.7104 - val_loss: 0.7065 - val_accuracy: 0.5172\n",
      "Epoch 6/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6204 - accuracy: 0.7259 - val_loss: 0.7343 - val_accuracy: 0.4345\n",
      "Epoch 7/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5795 - accuracy: 0.7355 - val_loss: 0.7550 - val_accuracy: 0.4483\n",
      "Epoch 8/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.5359 - accuracy: 0.7568 - val_loss: 0.7574 - val_accuracy: 0.4690\n",
      "Epoch 9/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.4817 - accuracy: 0.7915 - val_loss: 0.8518 - val_accuracy: 0.4759\n",
      "Epoch 10/20\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.4376 - accuracy: 0.8263 - val_loss: 0.8494 - val_accuracy: 0.4138\n",
      "Epoch 11/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3836 - accuracy: 0.8417 - val_loss: 0.8910 - val_accuracy: 0.4483\n",
      "Epoch 12/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.3315 - accuracy: 0.8938 - val_loss: 0.9019 - val_accuracy: 0.4828\n",
      "Epoch 13/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2939 - accuracy: 0.8803 - val_loss: 1.0115 - val_accuracy: 0.4276\n",
      "Epoch 14/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2562 - accuracy: 0.9208 - val_loss: 1.1480 - val_accuracy: 0.4276\n",
      "Epoch 15/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.2279 - accuracy: 0.9189 - val_loss: 1.0975 - val_accuracy: 0.4414\n",
      "Epoch 16/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1831 - accuracy: 0.9498 - val_loss: 1.2120 - val_accuracy: 0.4414\n",
      "Epoch 17/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9768 - val_loss: 1.2990 - val_accuracy: 0.4276\n",
      "Epoch 18/20\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1323 - accuracy: 0.9672 - val_loss: 1.3755 - val_accuracy: 0.4483\n",
      "Epoch 19/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9846 - val_loss: 1.4960 - val_accuracy: 0.4828\n",
      "Epoch 20/20\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.9942 - val_loss: 1.4936 - val_accuracy: 0.4690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff0fa6e4f40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_3_4 = Sequential()\n",
    "model_0_3_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features)))\n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=2, activation='relu')) \n",
    "model_0_3_4.add(MaxPooling1D(pool_size=2)) \n",
    "model_0_3_4.add(Conv1D(filters=32, kernel_size=1, activation='relu')) \n",
    "model_0_3_4.add(Flatten())\n",
    "model_0_3_4.add(Dense(50, activation='relu')) \n",
    "model_0_3_4.add(Dense(1, activation='sigmoid')) \n",
    "model_0_3_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "model_0_3_4.fit(train_X_0_3,train_f0t_tc,epochs=20,  validation_data=(val_X_0_3, val_f0t_tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd18708c",
   "metadata": {},
   "source": [
    "Generally the data from Model_0_3 has been the best. Keeping High NaN variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fc2b0",
   "metadata": {},
   "source": [
    "## Modeling with df_1, high NaN Vairables Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24d92000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Features       Score\n",
      "160                      target_3  314.255281\n",
      "50                   Change_Close   96.833439\n",
      "105                Stock_ROC_Move    4.523183\n",
      "79             Google_MAvg_s_Move    3.892640\n",
      "109        Stock_Disparity_s_Move    3.827079\n",
      "73                    Google_Rocp    3.765494\n",
      "127                      Nas_Move    3.191416\n",
      "98                     Stock_Rocp    3.098736\n",
      "130                  Nas_ROC_Move    2.572110\n",
      "134          Nas_Disparity_s_Move    2.293653\n",
      "63              Google_Moment_2_s    2.074318\n",
      "155                  Dow_ROC_Move    1.813696\n",
      "15         Banking in Switzerland    1.691470\n",
      "128                 Nas_MAvg_Move    1.631182\n",
      "129               Nas_MAvg_s_Move    1.593830\n",
      "158            Dow_Disparity_Move    1.582640\n",
      "102                    Stock_Move    1.572636\n",
      "72                   Google_ROC_s    1.463655\n",
      "62              Google_Moment_1_s    1.463655\n",
      "51                  Change_Google    1.434364\n",
      "108          Stock_Disparity_Move    1.373022\n",
      "81                Google_EMA_Move    1.207079\n",
      "82              Google_EMA_Move_5    1.207079\n",
      "59          Wiki_Disparity_s_Move    1.175917\n",
      "53                 Wiki_MAvg_Move    1.157403\n",
      "78               Google_MAvg_Move    1.154027\n",
      "123                      Nas_Rocp    1.133135\n",
      "3    UBS Global Wealth Management    1.011342\n",
      "148                      Dow_Rocp    1.004264\n",
      "133            Nas_Disparity_Move    1.003724\n",
      "103               Stock_MAvg_Move    0.973835\n",
      "154               Dow_MAvg_s_Move    0.973763\n",
      "9                          Volume    0.936322\n",
      "49            Stoch_Oscillator_14    0.840817\n",
      "68               Google_Disparity    0.825429\n",
      "70           Google_Disparity_s_5    0.825429\n",
      "55                  Wiki_ROC_Move    0.822638\n",
      "64                    Google_MAvg    0.757872\n",
      "66                Google_MAvg_s_5    0.757872\n",
      "159          Dow_Disparity_s_Move    0.680343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    }
   ],
   "source": [
    "# apply SelectKBest class to extract top 40 best features\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k=40)\n",
    "best_fit = bestfeatures.fit(train_f1, train_f1t)\n",
    "best_scores = pd.DataFrame(best_fit.scores_)\n",
    "best_columns = pd.DataFrame(df_1.columns)\n",
    "\n",
    "# concatenate the dataframes for better visualization\n",
    "features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "print(features_score.nlargest(40, 'Score'))  # print the top 40 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "683d2281",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_1 = list(features_score.nlargest(40, 'Score')['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "048d7967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 17.22805243,  30.00751293,  41.28594082,  48.36032554,\n",
       "        53.89597061,  58.71527909,  62.41786881,  65.14986416,\n",
       "        67.56707279,  69.77552726,  71.90459631,  73.97764483,\n",
       "        75.84133656,  77.46534958,  79.03644132,  80.45652409,\n",
       "        81.76844004,  83.00447846,  84.1671976 ,  85.26028694,\n",
       "        86.27028551,  87.2067805 ,  88.0994489 ,  88.91808055,\n",
       "        89.68791942,  90.43553876,  91.13618374,  91.82407212,\n",
       "        92.4383362 ,  93.00734853,  93.54375614,  94.05515473,\n",
       "        94.55482226,  95.0180511 ,  95.4595628 ,  95.8659553 ,\n",
       "        96.25872755,  96.59895077,  96.93344089,  97.25904455,\n",
       "        97.56066866,  97.82738803,  98.06931441,  98.29141443,\n",
       "        98.47721441,  98.62573747,  98.76994076,  98.88539118,\n",
       "        98.99146262,  99.09199367,  99.17607947,  99.2372616 ,\n",
       "        99.29523866,  99.35220075,  99.40534853,  99.45102161,\n",
       "        99.49351408,  99.53222451,  99.56902954,  99.60251861,\n",
       "        99.63204934,  99.65933412,  99.68461448,  99.70847664,\n",
       "        99.73102782,  99.75299727,  99.77429525,  99.79364466,\n",
       "        99.81145881,  99.82738813,  99.84276456,  99.85701339,\n",
       "        99.8705081 ,  99.88346523,  99.89566708,  99.90719926,\n",
       "        99.91853319,  99.92924302,  99.93740786,  99.94526475,\n",
       "        99.95280575,  99.95875505,  99.96349667,  99.96770468,\n",
       "        99.97125546,  99.97419526,  99.97678764,  99.97935542,\n",
       "        99.98161806,  99.98359605,  99.98541093,  99.98716913,\n",
       "        99.98861013,  99.98997621,  99.99126252,  99.99241641,\n",
       "        99.99348209,  99.99433463,  99.99506791,  99.99574921,\n",
       "        99.99637304,  99.99687958,  99.99731414,  99.99771042,\n",
       "        99.99803159,  99.99833438,  99.99857257,  99.99877214,\n",
       "        99.99896545,  99.99913292,  99.99928305,  99.99940971,\n",
       "        99.99952946,  99.99962061,  99.99968326,  99.99974187,\n",
       "        99.9997948 ,  99.99983826,  99.99987404,  99.99990656,\n",
       "        99.99993622,  99.99995298,  99.99996692,  99.99997809,\n",
       "        99.99998697,  99.99999139,  99.99999545,  99.99999883,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ,\n",
       "       100.        ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA().fit(train_f1)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "# reach 85% variance explained with 20 principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e186f113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 22.18407368,  34.83097467,  43.35818379,  50.87117424,\n",
       "        57.12776456,  62.31840641,  66.73121161,  70.55121833,\n",
       "        73.98398423,  77.11813048,  80.17588291,  82.56954178,\n",
       "        84.88335785,  86.95894505,  88.95433653,  90.60912771,\n",
       "        92.0293286 ,  93.3143592 ,  94.42454912,  95.4347556 ,\n",
       "        96.38595113,  97.2415363 ,  98.0226068 ,  98.6858616 ,\n",
       "        99.09547952,  99.41188414,  99.55311373,  99.65494261,\n",
       "        99.73385849,  99.80477162,  99.86351295,  99.90882006,\n",
       "        99.94005425,  99.96900372,  99.99497902, 100.        ,\n",
       "       100.        , 100.        , 100.        , 100.        ])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA with 40 best components\n",
    "pca_1 = PCA().fit(train_f1[feats_1])\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_1.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') # for each component\n",
    "plt.show()\n",
    "np.cumsum(pca_1.explained_variance_ratio_) * 100\n",
    "# can use 14 principle compents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b2cc0",
   "metadata": {},
   "source": [
    "Keeping score: Will test 4 models with df_1 data variations,\n",
    "- Model_1_0 = All varaibles,\n",
    "- Model_1_1 = PCA All varaibles\n",
    "- Model_1_2 = 40 best k score\n",
    "- Model_1_3 = PCA of 40 Best K Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f78ca9",
   "metadata": {},
   "source": [
    "### Model_1_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6208918c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 63 is out of bounds for axis 0 with size 63",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-dd6f46253be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1t_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1t_tc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_X_1_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1t_tc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_X_y2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_f1t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-62e459483688>\u001b[0m in \u001b[0;36mdf_to_X_y2\u001b[0;34m(df, target, window_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_as_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# grabs row i and all rows above within the window size length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creates 3 dimentional array, (# obseravtions, # rows in window, # features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# pulls the target variable after the window, target varible needs to be column zero in this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns (N,) martix of targets i+window_length time periods away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 63 is out of bounds for axis 0 with size 63"
     ]
    }
   ],
   "source": [
    "# Model_1_0 final data prep\n",
    "\n",
    "# converting to window format, in this case 5 periods\n",
    "train_X_1_0, train_f1t_tc = df_to_X_y2(train_f1,train_f1t)\n",
    "val_X_1_0, val_f1t_tc= df_to_X_y2(val_f1, val_f1t)\n",
    "test_X_1_0, test_f1t_tc = df_to_X_y2(test_f1,test_f1t) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31582b9feba862c420bc95ad7fac43fb721c474490d1710b4e50ac63470f9531"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dba26a6-1827-4e08-ba17-ddefa8156a12",
   "metadata": {},
   "source": [
    "# CNN UBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd012b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5000c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Normalization\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, InputLayer, SimpleRNN\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import (\n",
    "    linear_model, metrics, neural_network, pipeline, model_selection\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "# %pip install -q -U keras-tuner\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f13dc36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UBS_x</th>\n",
       "      <th>UBS Financial Services Inc.</th>\n",
       "      <th>UBS Investment Bank</th>\n",
       "      <th>UBS Global Wealth Management</th>\n",
       "      <th>UBS Asset Management</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Dow_EMA_Move</th>\n",
       "      <th>Dow_EMA_Move_5</th>\n",
       "      <th>Dow_Disparity_Move</th>\n",
       "      <th>Dow_Disparity_s_Move</th>\n",
       "      <th>Dow_RSI_Move</th>\n",
       "      <th>target_1</th>\n",
       "      <th>target_2</th>\n",
       "      <th>target_3</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UBS_x  UBS Financial Services Inc.  UBS Investment Bank  \\\n",
       "0     18                           18                   18   \n",
       "\n",
       "   UBS Global Wealth Management  UBS Asset Management  Open  High  Low  Close  \\\n",
       "0                            18                    18     0     0    0      0   \n",
       "\n",
       "   Volume  ...  Dow_EMA_Move  Dow_EMA_Move_5  Dow_Disparity_Move  \\\n",
       "0       0  ...             0               0                   0   \n",
       "\n",
       "   Dow_Disparity_s_Move  Dow_RSI_Move  target_1  target_2  target_3  target_4  \\\n",
       "0                     0             0         0         0         0         0   \n",
       "\n",
       "   target_5  \n",
       "0         0  \n",
       "\n",
       "[1 rows x 197 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"UBS_Cleaned_Date.csv\") #.iloc[2:, :].reset_index(drop = True)Netflix.date = pd.to_datetime(Netflix.date)\n",
    "df = df.set_index(\"date\")\n",
    "df = df.drop([\"Nas_total\", 'Stock_total', 'Dow_total'],axis=1) # to remove duplicated columns\n",
    "pd.DataFrame(df.isna().sum()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c88f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df[~(df.isin([np.inf, -np.inf]).any(axis=1))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c9fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['target_1', 'target_2', 'target_4', 'target_5'], axis=1)\n",
    "target_3 = df[\"target_3\"]\n",
    "\n",
    "#splitting into training sets \n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "X_train = df[0:int(n*0.7)]\n",
    "X_val = df[int(n*0.7):int(n*0.9)]\n",
    "X_test = df[int(n*0.9):]\n",
    "\n",
    "y_train = target_3[0:int(n*0.7)]\n",
    "y_val = target_3[int(n*0.7):int(n*0.9)]\n",
    "y_test = target_3[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37634713",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mscaler = MinMaxScaler() # keeps binarys at zero and 1 :)\n",
    "\n",
    "X_train = pd.DataFrame(Mscaler.fit_transform(X_train), columns = df.columns)\n",
    "X_val = pd.DataFrame(Mscaler.fit_transform(X_val), columns = df.columns)\n",
    "X_test = pd.DataFrame(Mscaler.fit_transform(X_test), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f734d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kbest_creator(k, df, df_val, df_test):\n",
    "    \"\"\"\n",
    "    returns data frame of principle componets of # of k best features \n",
    "    \"\"\"\n",
    "    # apply SelectKBest class to extract top 40 best features\n",
    "    bestfeatures = SelectKBest(score_func=f_regression, k=k)\n",
    "    best_fit = bestfeatures.fit(df, y_train)\n",
    "    best_scores = pd.DataFrame(best_fit.scores_)\n",
    "    best_columns = pd.DataFrame(df.columns)\n",
    "    \n",
    "    # concatenate the dataframes for better visualization\n",
    "    features_score = pd.concat([best_columns, best_scores], axis=1)\n",
    "    features_score.columns = ['Features', 'Score']  # naming the dataframe columns\n",
    "    feats_kb = list(features_score.nlargest(k, 'Score').iloc[1:k]['Features'])\n",
    "\n",
    "    pca = PCA().fit(df[feats_kb])\n",
    "    pca_scores = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "    res = next(x for x, val in enumerate(pca_scores) if val > 85)\n",
    "    res = res + 1\n",
    "    res\n",
    "    \n",
    "    pca_kb_1 = PCA(n_components = res).fit(df[feats_kb].to_numpy())\n",
    "    df = pca_kb_1.transform(df[feats_kb].to_numpy())\n",
    "    df_val = pca_kb_1.transform(df_val[feats_kb].to_numpy())\n",
    "    df_test = pca_kb_1.transform(df_test[feats_kb].to_numpy())\n",
    "    return df, df_val, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df7665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    }
   ],
   "source": [
    "X_train_kb_10, X_val_kb_10, X_test_kb_10 = kbest_creator(10, X_train, X_val, X_test) \n",
    "X_train_kb_25, X_val_kb_25, X_test_kb_25 = kbest_creator(25, X_train, X_val, X_test) \n",
    "X_train_kb_40, X_val_kb_40, X_test_kb_40 = kbest_creator(40, X_train, X_val, X_test) \n",
    "X_train_kb_55, X_val_kb_55, X_test_kb_55 = kbest_creator(55, X_train, X_val, X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1f947",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802d7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c8d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y2(df, target, window_size=5):\n",
    "  # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "    label = target[i+window_size] # pulls the target variable after the window, target varible needs to be column zero in this \n",
    "    y.append(label) # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a67462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to window format, in this case 5 periods\n",
    "X_train_kb_40, train_5w = df_to_X_y2(X_train_kb_40,y_train)\n",
    "X_val_kb_40, val_5w = df_to_X_y2(X_val_kb_40, y_val)\n",
    "X_test_kb_40, test_5w = df_to_X_y2(X_test_kb_40,y_test) \n",
    "\n",
    "X_train_kb_10, _ = df_to_X_y2(X_train_kb_10,y_train)\n",
    "X_val_kb_10, _ = df_to_X_y2(X_val_kb_10, y_val)\n",
    "X_test_kb_10, _ = df_to_X_y2(X_test_kb_10,y_test) \n",
    "\n",
    "X_train_kb_25, _ = df_to_X_y2(X_train_kb_25,y_train)\n",
    "X_val_kb_25, _ = df_to_X_y2(X_val_kb_25, y_val)\n",
    "X_test_kb_25, _ = df_to_X_y2(X_test_kb_25,y_test) \n",
    "\n",
    "X_train_kb_55, _ = df_to_X_y2(X_train_kb_55,y_train)\n",
    "X_val_kb_55, _ = df_to_X_y2(X_val_kb_55, y_val)\n",
    "X_test_kb_55, _ = df_to_X_y2(X_test_kb_55,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa349c0e",
   "metadata": {},
   "source": [
    "### Model Format 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7206c",
   "metadata": {},
   "source": [
    "## Hyper Parameter tuned Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a6a4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 00m 02s]\n",
      "val_binary_accuracy: 0.5517241358757019\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.5862069129943848\n",
      "Total elapsed time: 00h 01m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Model 1_1 - 1_4, using differnt k best pca variables\n",
    "\n",
    "def model_builder_1_1(hp):\n",
    "    n_steps = X_train_kb_10.shape[1]\n",
    "    n_features = X_train_kb_10.shape[2]\n",
    "\n",
    "    model1_1 = Sequential()\n",
    "\n",
    "    hp_filters = hp.Int('units', min_value=4, max_value=128, step=4)\n",
    "    hp_units = hp.Int('units', min_value=5, max_value=100, step=10)\n",
    "    model1_1.add(Conv1D(filters=hp_filters, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "    model1_1.add(Flatten())\n",
    "    model1_1.add(Dense(units = hp_units, activation='relu')) \n",
    "    model1_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model1_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "    return model1_1\n",
    "\n",
    "tuner = kt.Hyperband(model_builder_1_1,\n",
    "                     objective='val_binary_accuracy',\n",
    "                     max_epochs=30,\n",
    "                     factor=3,\n",
    "                     # directory='my_dir',    don't know what to do with this for now\n",
    "                     # project_name='intro_to_kt' # sasme with this\n",
    "                     )\n",
    "\n",
    "tuner.search(X_train_kb_10, train_5w, epochs=50, validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])\n",
    "\n",
    "#model1_1.fit(X_train_kb_10, train_5w,epochs=30,  validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7daaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_tuner.engine.hyperparameters.HyperParameters at 0x7f79857007c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e006937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 1s 23ms/step - loss: 0.6986 - binary_accuracy: 0.5058 - val_loss: 0.6851 - val_binary_accuracy: 0.5724\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.6765 - binary_accuracy: 0.5849 - val_loss: 0.6904 - val_binary_accuracy: 0.5034\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6647 - binary_accuracy: 0.6390 - val_loss: 0.6961 - val_binary_accuracy: 0.5034\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6542 - binary_accuracy: 0.6409 - val_loss: 0.6983 - val_binary_accuracy: 0.4966\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6435 - binary_accuracy: 0.6564 - val_loss: 0.7070 - val_binary_accuracy: 0.5034\n"
     ]
    }
   ],
   "source": [
    "model_1_1 = tuner.hypermodel.build(best_hps)\n",
    "history = model_1_1.fit(X_train_kb_10, train_5w, epochs=30, validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7994a832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857142"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1_1 = model_1_1.predict(X_test_kb_10)\n",
    "y_hat1_1 = y_hat1_1 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e3ae82",
   "metadata": {},
   "source": [
    "## Normal Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc525f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 21ms/step - loss: 0.6952 - binary_accuracy: 0.5077 - val_loss: 0.6925 - val_binary_accuracy: 0.5310\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6865 - binary_accuracy: 0.5463 - val_loss: 0.6891 - val_binary_accuracy: 0.5517\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6812 - binary_accuracy: 0.5714 - val_loss: 0.6896 - val_binary_accuracy: 0.5034\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6768 - binary_accuracy: 0.5830 - val_loss: 0.6895 - val_binary_accuracy: 0.5034\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6718 - binary_accuracy: 0.6004 - val_loss: 0.6898 - val_binary_accuracy: 0.5172\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6679 - binary_accuracy: 0.6004 - val_loss: 0.6886 - val_binary_accuracy: 0.5517\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6640 - binary_accuracy: 0.6216 - val_loss: 0.6892 - val_binary_accuracy: 0.5448\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6601 - binary_accuracy: 0.6641 - val_loss: 0.6914 - val_binary_accuracy: 0.5310\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6555 - binary_accuracy: 0.6757 - val_loss: 0.6924 - val_binary_accuracy: 0.5379\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6520 - binary_accuracy: 0.6795 - val_loss: 0.6909 - val_binary_accuracy: 0.5241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79860f52e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_10.shape[1]\n",
    "n_features = X_train_kb_10.shape[2]\n",
    "\n",
    "model1_1 = Sequential()\n",
    "\n",
    "\n",
    "model1_1.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model1_1.add(Flatten())\n",
    "model1_1.add(Dense(units = 15, activation='relu')) \n",
    "model1_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model1_1.fit(X_train_kb_10, train_5w,epochs=30,  validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540bf1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44285714285714284"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1_1 = model1_1.predict(X_test_kb_10)\n",
    "y_hat1_1 = y_hat1_1 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c962f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 33ms/step - loss: 0.6970 - binary_accuracy: 0.4884 - val_loss: 0.6934 - val_binary_accuracy: 0.5379\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6803 - binary_accuracy: 0.5734 - val_loss: 0.6992 - val_binary_accuracy: 0.4966\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6703 - binary_accuracy: 0.6120 - val_loss: 0.7029 - val_binary_accuracy: 0.4828\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6630 - binary_accuracy: 0.6371 - val_loss: 0.7022 - val_binary_accuracy: 0.5103\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6541 - binary_accuracy: 0.6795 - val_loss: 0.7055 - val_binary_accuracy: 0.4966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79865d2b20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_25.shape[1]\n",
    "n_features = X_train_kb_25.shape[2]\n",
    "\n",
    "model1_2 = Sequential()\n",
    "model1_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model1_2.add(Flatten())\n",
    "model1_2.add(Dense(25, activation='relu')) \n",
    "model1_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model1_2.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa3d1fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857142"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1_2 = model1_2.predict(X_test_kb_25)\n",
    "y_hat1_2 = y_hat1_2 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e0ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 2s 50ms/step - loss: 0.7077 - binary_accuracy: 0.4749 - val_loss: 0.7041 - val_binary_accuracy: 0.4828\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6900 - binary_accuracy: 0.5328 - val_loss: 0.6985 - val_binary_accuracy: 0.4621\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6744 - binary_accuracy: 0.5830 - val_loss: 0.6988 - val_binary_accuracy: 0.4828\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6636 - binary_accuracy: 0.6139 - val_loss: 0.6969 - val_binary_accuracy: 0.5310\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6523 - binary_accuracy: 0.6467 - val_loss: 0.6983 - val_binary_accuracy: 0.5034\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6396 - binary_accuracy: 0.6931 - val_loss: 0.7034 - val_binary_accuracy: 0.4897\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6314 - binary_accuracy: 0.6950 - val_loss: 0.7071 - val_binary_accuracy: 0.4759\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6177 - binary_accuracy: 0.7124 - val_loss: 0.7076 - val_binary_accuracy: 0.4690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798686cc70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_40.shape[1]\n",
    "n_features = X_train_kb_40.shape[2]\n",
    "\n",
    "model1_3 = Sequential()\n",
    "model1_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model1_3.add(Flatten())\n",
    "model1_3.add(Dense(25, activation='relu')) \n",
    "model1_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model1_3.fit(X_train_kb_40, train_5w,epochs=30,  validation_data=(X_val_kb_40, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd22db0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1_3 = model1_3.predict(X_test_kb_40)\n",
    "y_hat1_3 = y_hat1_3 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 14ms/step - loss: 0.7076 - binary_accuracy: 0.4768 - val_loss: 0.7057 - val_binary_accuracy: 0.4552\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6807 - binary_accuracy: 0.5502 - val_loss: 0.7010 - val_binary_accuracy: 0.4897\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6657 - binary_accuracy: 0.6120 - val_loss: 0.6996 - val_binary_accuracy: 0.5034\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6502 - binary_accuracy: 0.6544 - val_loss: 0.7020 - val_binary_accuracy: 0.5172\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 21ms/step - loss: 0.6343 - binary_accuracy: 0.7259 - val_loss: 0.7035 - val_binary_accuracy: 0.5172\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6189 - binary_accuracy: 0.7259 - val_loss: 0.7058 - val_binary_accuracy: 0.5310\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6005 - binary_accuracy: 0.7645 - val_loss: 0.7132 - val_binary_accuracy: 0.5655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7986847c70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_55.shape[1]\n",
    "n_features = X_train_kb_55.shape[2]\n",
    "\n",
    "model1_4 = Sequential()\n",
    "model1_4.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model1_4.add(Flatten())\n",
    "model1_4.add(Dense(25, activation='relu')) \n",
    "model1_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model1_4.fit(X_train_kb_55, train_5w,epochs=30,  validation_data=(X_val_kb_55, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c8dd064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f798682f160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4857142857142857"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1_4 = model1_4.predict(X_test_kb_55)\n",
    "y_hat1_4 = y_hat1_4 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80642814",
   "metadata": {},
   "source": [
    "### Model 2, Less Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a6a4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 12ms/step - loss: 0.7219 - binary_accuracy: 0.5135 - val_loss: 0.7187 - val_binary_accuracy: 0.4621\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.7030 - binary_accuracy: 0.5154 - val_loss: 0.7119 - val_binary_accuracy: 0.5034\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6973 - binary_accuracy: 0.5000 - val_loss: 0.7101 - val_binary_accuracy: 0.4621\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6939 - binary_accuracy: 0.5039 - val_loss: 0.7079 - val_binary_accuracy: 0.4759\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6913 - binary_accuracy: 0.5135 - val_loss: 0.7062 - val_binary_accuracy: 0.4690\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6892 - binary_accuracy: 0.5193 - val_loss: 0.7044 - val_binary_accuracy: 0.4552\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6871 - binary_accuracy: 0.5251 - val_loss: 0.7036 - val_binary_accuracy: 0.4345\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6852 - binary_accuracy: 0.5386 - val_loss: 0.7037 - val_binary_accuracy: 0.4552\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6835 - binary_accuracy: 0.5483 - val_loss: 0.7024 - val_binary_accuracy: 0.4828\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6819 - binary_accuracy: 0.5598 - val_loss: 0.7018 - val_binary_accuracy: 0.4828\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6816 - binary_accuracy: 0.5463 - val_loss: 0.7015 - val_binary_accuracy: 0.4828\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6784 - binary_accuracy: 0.5849 - val_loss: 0.7001 - val_binary_accuracy: 0.4966\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6780 - binary_accuracy: 0.5965 - val_loss: 0.6993 - val_binary_accuracy: 0.4897\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6759 - binary_accuracy: 0.6023 - val_loss: 0.7000 - val_binary_accuracy: 0.4690\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6738 - binary_accuracy: 0.6004 - val_loss: 0.7005 - val_binary_accuracy: 0.4690\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6725 - binary_accuracy: 0.6004 - val_loss: 0.7013 - val_binary_accuracy: 0.4621\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6711 - binary_accuracy: 0.6178 - val_loss: 0.7007 - val_binary_accuracy: 0.4690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7987e50d60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2_1 - 2_4, using differnt k best pca variables. reduced filters\n",
    "n_steps = X_train_kb_10.shape[1]\n",
    "n_features = X_train_kb_10.shape[2]\n",
    "\n",
    "model2_1 = Sequential()\n",
    "model2_1.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_1.add(Flatten())\n",
    "model2_1.add(Dense(25, activation='relu')) \n",
    "model2_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_1.fit(X_train_kb_10, train_5w,epochs=30,  validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "929becef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7987f833a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_1 = model2_1.predict(X_test_kb_10)\n",
    "y_hat2_1 = y_hat2_1 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c962f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 13ms/step - loss: 0.7078 - binary_accuracy: 0.4826 - val_loss: 0.6880 - val_binary_accuracy: 0.5241\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6975 - binary_accuracy: 0.4923 - val_loss: 0.6864 - val_binary_accuracy: 0.5310\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6935 - binary_accuracy: 0.5174 - val_loss: 0.6855 - val_binary_accuracy: 0.5448\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6905 - binary_accuracy: 0.5367 - val_loss: 0.6863 - val_binary_accuracy: 0.5379\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6879 - binary_accuracy: 0.5386 - val_loss: 0.6872 - val_binary_accuracy: 0.5379\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6855 - binary_accuracy: 0.5541 - val_loss: 0.6893 - val_binary_accuracy: 0.5103\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6829 - binary_accuracy: 0.5541 - val_loss: 0.6896 - val_binary_accuracy: 0.5310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7987fbdfd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_25.shape[1]\n",
    "n_features = X_train_kb_25.shape[2]\n",
    "\n",
    "model2_2 = Sequential()\n",
    "model2_2.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_2.add(Flatten())\n",
    "model2_2.add(Dense(25, activation='relu')) \n",
    "model2_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_2.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4021da90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45714285714285713"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_2 = model2_2.predict(X_test_kb_25)\n",
    "y_hat2_2 = y_hat2_2 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e0ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 16ms/step - loss: 0.7110 - binary_accuracy: 0.5039 - val_loss: 0.7005 - val_binary_accuracy: 0.4966\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.7019 - binary_accuracy: 0.4981 - val_loss: 0.7009 - val_binary_accuracy: 0.4759\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6978 - binary_accuracy: 0.5154 - val_loss: 0.7016 - val_binary_accuracy: 0.4759\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6929 - binary_accuracy: 0.5328 - val_loss: 0.7022 - val_binary_accuracy: 0.4690\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6892 - binary_accuracy: 0.5367 - val_loss: 0.7036 - val_binary_accuracy: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7988cc1dc0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_40.shape[1]\n",
    "n_features = X_train_kb_40.shape[2]\n",
    "\n",
    "model2_3 = Sequential()\n",
    "model2_3.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_3.add(Flatten())\n",
    "model2_3.add(Dense(25, activation='relu')) \n",
    "model2_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_3.fit(X_train_kb_40, train_5w,epochs=30,  validation_data=(X_val_kb_40, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e994ea9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_3 = model2_3.predict(X_test_kb_40)\n",
    "y_hat2_3 = y_hat2_3 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 14ms/step - loss: 0.7490 - binary_accuracy: 0.5000 - val_loss: 0.7178 - val_binary_accuracy: 0.4690\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.7160 - binary_accuracy: 0.4942 - val_loss: 0.7006 - val_binary_accuracy: 0.5172\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7057 - binary_accuracy: 0.5019 - val_loss: 0.6973 - val_binary_accuracy: 0.5172\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6995 - binary_accuracy: 0.5193 - val_loss: 0.6952 - val_binary_accuracy: 0.5103\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6939 - binary_accuracy: 0.5270 - val_loss: 0.6938 - val_binary_accuracy: 0.5103\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6892 - binary_accuracy: 0.5444 - val_loss: 0.6925 - val_binary_accuracy: 0.5172\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6846 - binary_accuracy: 0.5405 - val_loss: 0.6935 - val_binary_accuracy: 0.4621\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6795 - binary_accuracy: 0.5676 - val_loss: 0.6902 - val_binary_accuracy: 0.5241\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6758 - binary_accuracy: 0.5869 - val_loss: 0.6890 - val_binary_accuracy: 0.5241\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6715 - binary_accuracy: 0.5888 - val_loss: 0.6891 - val_binary_accuracy: 0.5103\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6678 - binary_accuracy: 0.5985 - val_loss: 0.6902 - val_binary_accuracy: 0.5310\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6639 - binary_accuracy: 0.5907 - val_loss: 0.6891 - val_binary_accuracy: 0.5310\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6610 - binary_accuracy: 0.6062 - val_loss: 0.6884 - val_binary_accuracy: 0.5448\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6575 - binary_accuracy: 0.6139 - val_loss: 0.6898 - val_binary_accuracy: 0.5517\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6538 - binary_accuracy: 0.6274 - val_loss: 0.6886 - val_binary_accuracy: 0.5586\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6506 - binary_accuracy: 0.6351 - val_loss: 0.6887 - val_binary_accuracy: 0.5655\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6469 - binary_accuracy: 0.6448 - val_loss: 0.6886 - val_binary_accuracy: 0.5793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7985717190>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_55.shape[1]\n",
    "n_features = X_train_kb_55.shape[2]\n",
    "\n",
    "model2_4 = Sequential()\n",
    "model2_4.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_4.add(Flatten())\n",
    "model2_4.add(Dense(25, activation='relu')) \n",
    "model2_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_4.fit(X_train_kb_55, train_5w,epochs=30,  validation_data=(X_val_kb_55, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68c28a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_4 = model2_4.predict(X_test_kb_55)\n",
    "y_hat2_4 = y_hat2_4 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a946d9",
   "metadata": {},
   "source": [
    "### Model 3, Adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a6a4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 20ms/step - loss: 0.6925 - binary_accuracy: 0.4961 - val_loss: 0.6957 - val_binary_accuracy: 0.4690\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6914 - binary_accuracy: 0.4981 - val_loss: 0.6949 - val_binary_accuracy: 0.4828\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6906 - binary_accuracy: 0.5174 - val_loss: 0.6952 - val_binary_accuracy: 0.4897\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6901 - binary_accuracy: 0.5251 - val_loss: 0.6954 - val_binary_accuracy: 0.4897\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6894 - binary_accuracy: 0.5309 - val_loss: 0.6952 - val_binary_accuracy: 0.5034\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6889 - binary_accuracy: 0.5290 - val_loss: 0.6955 - val_binary_accuracy: 0.4759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79865a19a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3_1 - 3_4, adding layers\n",
    "n_steps = X_train_kb_10.shape[1]\n",
    "n_features = X_train_kb_10.shape[2]\n",
    "\n",
    "model3_1 = Sequential()\n",
    "model3_1.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_1.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_1.add(MaxPooling1D(pool_size=2))\n",
    "model3_1.add(Flatten())\n",
    "model3_1.add(Dense(8, activation='relu')) \n",
    "model3_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_1.fit(X_train_kb_10, train_5w,epochs=30,  validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9c077a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat3_1 = model3_1.predict(X_test_kb_10)\n",
    "y_hat3_1 = y_hat3_1 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c962f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 15ms/step - loss: 0.6929 - binary_accuracy: 0.5116 - val_loss: 0.6957 - val_binary_accuracy: 0.4690\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6882 - binary_accuracy: 0.5695 - val_loss: 0.6984 - val_binary_accuracy: 0.4207\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6855 - binary_accuracy: 0.5830 - val_loss: 0.6993 - val_binary_accuracy: 0.4276\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6804 - binary_accuracy: 0.6139 - val_loss: 0.7005 - val_binary_accuracy: 0.4414\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6758 - binary_accuracy: 0.6467 - val_loss: 0.7020 - val_binary_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7985c6ed00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_25.shape[1]\n",
    "n_features = X_train_kb_25.shape[2]\n",
    "\n",
    "model3_2 = Sequential()\n",
    "model3_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_2.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_2.add(MaxPooling1D(pool_size=2))\n",
    "model3_2.add(Flatten())\n",
    "model3_2.add(Dense(25, activation='relu')) \n",
    "model3_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_2.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94fe03bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857142"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat3_2 = model3_2.predict(X_test_kb_25)\n",
    "y_hat3_2 = y_hat3_2 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24e0ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 45ms/step - loss: 0.6956 - binary_accuracy: 0.5077 - val_loss: 0.6934 - val_binary_accuracy: 0.5241\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6913 - binary_accuracy: 0.5309 - val_loss: 0.6939 - val_binary_accuracy: 0.4897\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6891 - binary_accuracy: 0.5598 - val_loss: 0.6947 - val_binary_accuracy: 0.4897\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6865 - binary_accuracy: 0.5637 - val_loss: 0.6954 - val_binary_accuracy: 0.4966\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6835 - binary_accuracy: 0.5985 - val_loss: 0.6970 - val_binary_accuracy: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7985f15850>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_40.shape[1]\n",
    "n_features = X_train_kb_40.shape[2]\n",
    "\n",
    "model3_3 = Sequential()\n",
    "model3_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_3.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_3.add(MaxPooling1D(pool_size=2))\n",
    "model3_3.add(Flatten())\n",
    "model3_3.add(Dense(25, activation='relu')) \n",
    "model3_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_3.fit(X_train_kb_40, train_5w,epochs=30,  validation_data=(X_val_kb_40, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8a26636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat3_3 = model3_3.predict(X_test_kb_40)\n",
    "y_hat3_3 = y_hat3_3 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 12ms/step - loss: 0.6964 - binary_accuracy: 0.4923 - val_loss: 0.6971 - val_binary_accuracy: 0.5103\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6932 - binary_accuracy: 0.5251 - val_loss: 0.6974 - val_binary_accuracy: 0.4759\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6916 - binary_accuracy: 0.5290 - val_loss: 0.6975 - val_binary_accuracy: 0.4759\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6904 - binary_accuracy: 0.5386 - val_loss: 0.6967 - val_binary_accuracy: 0.4897\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6893 - binary_accuracy: 0.5579 - val_loss: 0.6973 - val_binary_accuracy: 0.4828\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6878 - binary_accuracy: 0.5734 - val_loss: 0.6975 - val_binary_accuracy: 0.4966\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6867 - binary_accuracy: 0.5869 - val_loss: 0.6978 - val_binary_accuracy: 0.4966\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6855 - binary_accuracy: 0.5869 - val_loss: 0.6985 - val_binary_accuracy: 0.4828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7987d91ac0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_55.shape[1]\n",
    "n_features = X_train_kb_55.shape[2]\n",
    "\n",
    "model3_4 = Sequential()\n",
    "model3_4.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_4.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_4.add(MaxPooling1D(pool_size=2))\n",
    "model3_4.add(Flatten())\n",
    "model3_4.add(Dense(25, activation='relu')) \n",
    "model3_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_4.fit(X_train_kb_55, train_5w,epochs=30,  validation_data=(X_val_kb_55, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7941f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat3_4 = model3_4.predict(X_test_kb_55)\n",
    "y_hat3_4 = y_hat3_4 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a38693",
   "metadata": {},
   "source": [
    "### Model 4, adding conv layer after pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a6a4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 18ms/step - loss: 0.6943 - binary_accuracy: 0.4961 - val_loss: 0.6886 - val_binary_accuracy: 0.5448\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6932 - binary_accuracy: 0.4961 - val_loss: 0.6896 - val_binary_accuracy: 0.5793\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6921 - binary_accuracy: 0.4942 - val_loss: 0.6896 - val_binary_accuracy: 0.5586\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6916 - binary_accuracy: 0.5154 - val_loss: 0.6894 - val_binary_accuracy: 0.5517\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6912 - binary_accuracy: 0.5135 - val_loss: 0.6895 - val_binary_accuracy: 0.5517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798ac38f70>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 4_1 - 4_4, adding conv layer after pooling\n",
    "n_steps = X_train_kb_10.shape[1]\n",
    "n_features = X_train_kb_10.shape[2]\n",
    "\n",
    "model4_1 = Sequential()\n",
    "model4_1.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model4_1.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model4_1.add(MaxPooling1D(pool_size=2))\n",
    "model4_1.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model4_1.add(Flatten())\n",
    "model4_1.add(Dense(8, activation='relu')) \n",
    "model4_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4_1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model4_1.fit(X_train_kb_10, train_5w,epochs=30,  validation_data=(X_val_kb_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ea7e586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat4_1 = model4_1.predict(X_test_kb_10)\n",
    "y_hat4_1 = y_hat4_1 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c962f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 16ms/step - loss: 0.6965 - binary_accuracy: 0.4884 - val_loss: 0.7006 - val_binary_accuracy: 0.4138\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6914 - binary_accuracy: 0.5251 - val_loss: 0.6990 - val_binary_accuracy: 0.4138\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6892 - binary_accuracy: 0.5849 - val_loss: 0.7011 - val_binary_accuracy: 0.4069\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6870 - binary_accuracy: 0.5637 - val_loss: 0.7022 - val_binary_accuracy: 0.3931\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6846 - binary_accuracy: 0.6139 - val_loss: 0.7018 - val_binary_accuracy: 0.4069\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6806 - binary_accuracy: 0.6448 - val_loss: 0.7041 - val_binary_accuracy: 0.4207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798cc4cfa0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_25.shape[1]\n",
    "n_features = X_train_kb_25.shape[2]\n",
    "\n",
    "model4_2 = Sequential()\n",
    "model4_2.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model4_2.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model4_2.add(MaxPooling1D(pool_size=2))\n",
    "model4_2.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model4_2.add(Flatten())\n",
    "model4_2.add(Dense(25, activation='relu')) \n",
    "model4_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4_2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model4_2.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7443231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat4_2 = model4_2.predict(X_test_kb_25)\n",
    "y_hat4_2 = y_hat4_2 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat4_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24e0ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 16ms/step - loss: 0.6942 - binary_accuracy: 0.4768 - val_loss: 0.6941 - val_binary_accuracy: 0.4759\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6907 - binary_accuracy: 0.5444 - val_loss: 0.6952 - val_binary_accuracy: 0.4276\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6884 - binary_accuracy: 0.5849 - val_loss: 0.6960 - val_binary_accuracy: 0.4621\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6849 - binary_accuracy: 0.6023 - val_loss: 0.6974 - val_binary_accuracy: 0.4345\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6814 - binary_accuracy: 0.5946 - val_loss: 0.6984 - val_binary_accuracy: 0.4966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798ccd5f70>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_40.shape[1]\n",
    "n_features = X_train_kb_40.shape[2]\n",
    "\n",
    "model4_3 = Sequential()\n",
    "model4_3.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model4_3.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model4_3.add(MaxPooling1D(pool_size=2))\n",
    "model4_3.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model4_3.add(Flatten())\n",
    "model4_3.add(Dense(25, activation='relu')) \n",
    "model4_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4_3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model4_3.fit(X_train_kb_40, train_5w,epochs=30,  validation_data=(X_val_kb_40, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a97295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45714285714285713"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat4_3 = model4_3.predict(X_test_kb_40)\n",
    "y_hat4_3 = y_hat4_3 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat4_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 12ms/step - loss: 0.7004 - binary_accuracy: 0.5019 - val_loss: 0.6954 - val_binary_accuracy: 0.5103\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6923 - binary_accuracy: 0.5174 - val_loss: 0.6943 - val_binary_accuracy: 0.4207\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6900 - binary_accuracy: 0.5502 - val_loss: 0.6942 - val_binary_accuracy: 0.4621\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6890 - binary_accuracy: 0.5386 - val_loss: 0.6949 - val_binary_accuracy: 0.4483\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6871 - binary_accuracy: 0.5618 - val_loss: 0.6950 - val_binary_accuracy: 0.4759\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6859 - binary_accuracy: 0.5618 - val_loss: 0.6952 - val_binary_accuracy: 0.4552\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6844 - binary_accuracy: 0.5811 - val_loss: 0.6953 - val_binary_accuracy: 0.4621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7987d98460>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_55.shape[1]\n",
    "n_features = X_train_kb_55.shape[2]\n",
    "\n",
    "model4_4 = Sequential()\n",
    "model4_4.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model4_4.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model4_4.add(MaxPooling1D(pool_size=2))\n",
    "model4_4.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model4_4.add(Flatten())\n",
    "model4_4.add(Dense(25, activation='relu')) \n",
    "model4_4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4_4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model4_4.fit(X_train_kb_55, train_5w,epochs=30,  validation_data=(X_val_kb_55, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "263a0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat4_4 = model4_4.predict(X_test_kb_55)\n",
    "y_hat4_4 = y_hat4_4 > .5\n",
    "metrics.accuracy_score(test_5w,y_hat4_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c637d4e3",
   "metadata": {},
   "source": [
    "# Trying the Correlation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76a38693",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_train.corr()\n",
    "corr_matrix\n",
    "\n",
    "feats_corr_10 = list(pd.DataFrame(corr_matrix['target_3'].sort_values(ascending=False).iloc[1:11]).reset_index()['index'])\n",
    "feats_corr_25 = list(pd.DataFrame(corr_matrix['target_3'].sort_values(ascending=False).iloc[1:26]).reset_index()['index'])\n",
    "feats_corr_40 = list(pd.DataFrame(corr_matrix['target_3'].sort_values(ascending=False).iloc[1:41]).reset_index()['index'])\n",
    "feats_corr_55 = list(pd.DataFrame(corr_matrix['target_3'].sort_values(ascending=False).iloc[1:56]).reset_index()['index'])\n",
    "\n",
    "X_train_cr_10 = X_train[feats_corr_10]\n",
    "X_test_cr_10 = X_test[feats_corr_10]\n",
    "X_val_cr_10 = X_val[feats_corr_10]\n",
    "X_train_cr_25 = X_train[feats_corr_25]\n",
    "X_test_cr_25 = X_test[feats_corr_25]\n",
    "X_val_cr_25 = X_val[feats_corr_25]\n",
    "X_train_cr_40 = X_train[feats_corr_40]\n",
    "X_test_cr_40 = X_test[feats_corr_40]\n",
    "X_val_cr_40 = X_val[feats_corr_40]\n",
    "X_train_cr_55 = X_train[feats_corr_55]\n",
    "X_test_cr_55 = X_test[feats_corr_55]\n",
    "X_val_cr_55 = X_val[feats_corr_55]\n",
    "\n",
    "def pca_finder(df, df_val, df_test):\n",
    "    \n",
    "    pca = PCA().fit(df)\n",
    "    pca_scores = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "    res = next(x for x, val in enumerate(pca_scores) if val > 85)\n",
    "    res = res + 1\n",
    "    res\n",
    "    \n",
    "    pca_cr_1 = PCA(n_components = res).fit(df.to_numpy())\n",
    "    df = pca_cr_1.transform(df.to_numpy())\n",
    "    df_val = pca_cr_1.transform(df_val.to_numpy())\n",
    "    df_test = pca_cr_1.transform(df_test.to_numpy())\n",
    "    return df, df_val, df_test\n",
    "\n",
    "X_train_cr_10, X_val_cr_10, X_test_cr_10 = pca_finder(X_train_cr_10, X_val_cr_10, X_test_cr_10)\n",
    "X_train_cr_25, X_val_cr_25, X_test_cr_25 = pca_finder(X_train_cr_25, X_val_cr_25, X_test_cr_25)\n",
    "X_train_cr_40, X_val_cr_40, X_test_cr_40 = pca_finder(X_train_cr_40, X_val_cr_40, X_test_cr_40)\n",
    "X_train_cr_55, X_val_cr_55, X_test_cr_55 = pca_finder(X_train_cr_55, X_val_cr_55, X_test_cr_55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06fc0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cr_40, train_5w = df_to_X_y2(X_train_cr_40,y_train)\n",
    "X_val_cr_40, val_5w = df_to_X_y2(X_val_cr_40, y_val)\n",
    "X_test_cr_40, test_5w = df_to_X_y2(X_test_cr_40,y_test) \n",
    "\n",
    "X_train_cr_10, _ = df_to_X_y2(X_train_cr_10,y_train)\n",
    "X_val_cr_10, _ = df_to_X_y2(X_val_cr_10, y_val)\n",
    "X_test_cr_10, _ = df_to_X_y2(X_test_cr_10,y_test) \n",
    "\n",
    "X_train_cr_25, _ = df_to_X_y2(X_train_cr_25,y_train)\n",
    "X_val_cr_25, _ = df_to_X_y2(X_val_cr_25, y_val)\n",
    "X_test_cr_25, _ = df_to_X_y2(X_test_cr_25,y_test) \n",
    "\n",
    "X_train_cr_55, _ = df_to_X_y2(X_train_cr_55,y_train)\n",
    "X_val_cr_55, _ = df_to_X_y2(X_val_cr_55, y_val)\n",
    "X_test_cr_55, _ = df_to_X_y2(X_test_cr_55,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a6a4c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 15ms/step - loss: 0.7032 - binary_accuracy: 0.5019 - val_loss: 0.6796 - val_binary_accuracy: 0.5724\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6976 - binary_accuracy: 0.5077 - val_loss: 0.6811 - val_binary_accuracy: 0.5586\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6948 - binary_accuracy: 0.5077 - val_loss: 0.6809 - val_binary_accuracy: 0.5724\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6923 - binary_accuracy: 0.5290 - val_loss: 0.6822 - val_binary_accuracy: 0.5655\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6905 - binary_accuracy: 0.5290 - val_loss: 0.6828 - val_binary_accuracy: 0.5379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7988e10df0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2_1 - 2_4, using differnt k best pca variables. reduced filters\n",
    "n_steps = X_train_cr_10.shape[1]\n",
    "n_features = X_train_cr_10.shape[2]\n",
    "\n",
    "model2_1_cr = Sequential()\n",
    "model2_1_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_1_cr.add(Flatten())\n",
    "model2_1_cr.add(Dense(25, activation='relu')) \n",
    "model2_1_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_1_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_1_cr.fit(X_train_cr_10, train_5w,epochs=30,  validation_data=(X_val_cr_10, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "929becef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44285714285714284"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_1_cr = model2_1_cr.predict(X_test_cr_10)\n",
    "y_hat2_1_cr = y_hat2_1_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_1_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c962f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 9ms/step - loss: 0.7300 - binary_accuracy: 0.4633 - val_loss: 0.7023 - val_binary_accuracy: 0.4897\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7121 - binary_accuracy: 0.5039 - val_loss: 0.6966 - val_binary_accuracy: 0.5103\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7037 - binary_accuracy: 0.5019 - val_loss: 0.6946 - val_binary_accuracy: 0.5241\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6986 - binary_accuracy: 0.5097 - val_loss: 0.6930 - val_binary_accuracy: 0.5448\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6940 - binary_accuracy: 0.5154 - val_loss: 0.6936 - val_binary_accuracy: 0.5586\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6904 - binary_accuracy: 0.5444 - val_loss: 0.6925 - val_binary_accuracy: 0.5586\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6873 - binary_accuracy: 0.5444 - val_loss: 0.6926 - val_binary_accuracy: 0.5241\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6843 - binary_accuracy: 0.5598 - val_loss: 0.6912 - val_binary_accuracy: 0.5448\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6810 - binary_accuracy: 0.5656 - val_loss: 0.6912 - val_binary_accuracy: 0.5448\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6783 - binary_accuracy: 0.5656 - val_loss: 0.6909 - val_binary_accuracy: 0.5448\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6761 - binary_accuracy: 0.5792 - val_loss: 0.6925 - val_binary_accuracy: 0.5655\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6732 - binary_accuracy: 0.5985 - val_loss: 0.6921 - val_binary_accuracy: 0.5517\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6702 - binary_accuracy: 0.6062 - val_loss: 0.6914 - val_binary_accuracy: 0.5448\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6680 - binary_accuracy: 0.6004 - val_loss: 0.6901 - val_binary_accuracy: 0.5517\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6639 - binary_accuracy: 0.6216 - val_loss: 0.6902 - val_binary_accuracy: 0.5448\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6624 - binary_accuracy: 0.6197 - val_loss: 0.6896 - val_binary_accuracy: 0.5379\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6595 - binary_accuracy: 0.6293 - val_loss: 0.6887 - val_binary_accuracy: 0.5517\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6559 - binary_accuracy: 0.6274 - val_loss: 0.6864 - val_binary_accuracy: 0.5448\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6529 - binary_accuracy: 0.6216 - val_loss: 0.6855 - val_binary_accuracy: 0.5517\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6493 - binary_accuracy: 0.6409 - val_loss: 0.6864 - val_binary_accuracy: 0.5241\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6467 - binary_accuracy: 0.6506 - val_loss: 0.6858 - val_binary_accuracy: 0.5448\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6437 - binary_accuracy: 0.6602 - val_loss: 0.6844 - val_binary_accuracy: 0.5379\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6398 - binary_accuracy: 0.6680 - val_loss: 0.6855 - val_binary_accuracy: 0.5310\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6363 - binary_accuracy: 0.6757 - val_loss: 0.6870 - val_binary_accuracy: 0.5172\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6333 - binary_accuracy: 0.6699 - val_loss: 0.6837 - val_binary_accuracy: 0.5517\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6281 - binary_accuracy: 0.6834 - val_loss: 0.6842 - val_binary_accuracy: 0.5172\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6255 - binary_accuracy: 0.6931 - val_loss: 0.6859 - val_binary_accuracy: 0.5172\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6202 - binary_accuracy: 0.7027 - val_loss: 0.6851 - val_binary_accuracy: 0.5379\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6162 - binary_accuracy: 0.6931 - val_loss: 0.6835 - val_binary_accuracy: 0.5379\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6130 - binary_accuracy: 0.6988 - val_loss: 0.6841 - val_binary_accuracy: 0.5379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798876dac0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_cr_25.shape[1]\n",
    "n_features = X_train_cr_25.shape[2]\n",
    "\n",
    "model2_2_cr = Sequential()\n",
    "model2_2_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_2_cr.add(Flatten())\n",
    "model2_2_cr.add(Dense(25, activation='relu')) \n",
    "model2_2_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_2_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_2_cr.fit(X_train_cr_25, train_5w,epochs=30,  validation_data=(X_val_cr_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4021da90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_2_cr = model2_2_cr.predict(X_test_cr_25)\n",
    "y_hat2_2_cr = y_hat2_2_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_2_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24e0ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 9ms/step - loss: 0.7067 - binary_accuracy: 0.5039 - val_loss: 0.7291 - val_binary_accuracy: 0.4483\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6935 - binary_accuracy: 0.5290 - val_loss: 0.7245 - val_binary_accuracy: 0.4759\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6872 - binary_accuracy: 0.5386 - val_loss: 0.7213 - val_binary_accuracy: 0.4690\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6820 - binary_accuracy: 0.5541 - val_loss: 0.7179 - val_binary_accuracy: 0.4828\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6766 - binary_accuracy: 0.5734 - val_loss: 0.7147 - val_binary_accuracy: 0.4690\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6727 - binary_accuracy: 0.5907 - val_loss: 0.7162 - val_binary_accuracy: 0.4897\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6679 - binary_accuracy: 0.6081 - val_loss: 0.7134 - val_binary_accuracy: 0.4690\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6643 - binary_accuracy: 0.6120 - val_loss: 0.7137 - val_binary_accuracy: 0.4828\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6601 - binary_accuracy: 0.6216 - val_loss: 0.7159 - val_binary_accuracy: 0.4966\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6569 - binary_accuracy: 0.6236 - val_loss: 0.7166 - val_binary_accuracy: 0.4759\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6527 - binary_accuracy: 0.6371 - val_loss: 0.7146 - val_binary_accuracy: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79888d7370>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_cr_40.shape[1]\n",
    "n_features = X_train_cr_40.shape[2]\n",
    "\n",
    "model2_3_cr = Sequential()\n",
    "model2_3_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_3_cr.add(Flatten())\n",
    "model2_3_cr.add(Dense(25, activation='relu')) \n",
    "model2_3_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_3_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_3_cr.fit(X_train_cr_40, train_5w,epochs=30,  validation_data=(X_val_cr_40, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e994ea9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4142857142857143"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_3_cr = model2_3_cr.predict(X_test_cr_40)\n",
    "y_hat2_3_cr = y_hat2_3_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_3_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "26ddd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 10ms/step - loss: 0.7641 - binary_accuracy: 0.5019 - val_loss: 0.7109 - val_binary_accuracy: 0.5379\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7194 - binary_accuracy: 0.5077 - val_loss: 0.6942 - val_binary_accuracy: 0.4966\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7054 - binary_accuracy: 0.4923 - val_loss: 0.6933 - val_binary_accuracy: 0.4759\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6989 - binary_accuracy: 0.4865 - val_loss: 0.6938 - val_binary_accuracy: 0.4897\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6951 - binary_accuracy: 0.4942 - val_loss: 0.6928 - val_binary_accuracy: 0.4483\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6907 - binary_accuracy: 0.5116 - val_loss: 0.6923 - val_binary_accuracy: 0.4483\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6869 - binary_accuracy: 0.5232 - val_loss: 0.6913 - val_binary_accuracy: 0.4483\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6836 - binary_accuracy: 0.5521 - val_loss: 0.6899 - val_binary_accuracy: 0.4621\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6804 - binary_accuracy: 0.5405 - val_loss: 0.6903 - val_binary_accuracy: 0.5034\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6771 - binary_accuracy: 0.5541 - val_loss: 0.6903 - val_binary_accuracy: 0.4483\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6740 - binary_accuracy: 0.5676 - val_loss: 0.6900 - val_binary_accuracy: 0.4690\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6710 - binary_accuracy: 0.5753 - val_loss: 0.6896 - val_binary_accuracy: 0.4828\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6679 - binary_accuracy: 0.5849 - val_loss: 0.6898 - val_binary_accuracy: 0.5034\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6650 - binary_accuracy: 0.6023 - val_loss: 0.6904 - val_binary_accuracy: 0.5241\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6616 - binary_accuracy: 0.6120 - val_loss: 0.6907 - val_binary_accuracy: 0.5103\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6584 - binary_accuracy: 0.6236 - val_loss: 0.6912 - val_binary_accuracy: 0.5241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7988bcd190>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_cr_55.shape[1]\n",
    "n_features = X_train_cr_55.shape[2]\n",
    "\n",
    "model2_4_cr = Sequential()\n",
    "model2_4_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "\n",
    "model2_4_cr.add(Flatten())\n",
    "model2_4_cr.add(Dense(25, activation='relu')) \n",
    "model2_4_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2_4_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model2_4_cr.fit(X_train_cr_55, train_5w,epochs=30,  validation_data=(X_val_cr_55, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "68c28a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45714285714285713"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat2_4_cr = model2_4_cr.predict(X_test_cr_55)\n",
    "y_hat2_4_cr = y_hat2_4_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat2_4_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "84c605eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 17ms/step - loss: 0.6948 - binary_accuracy: 0.4595 - val_loss: 0.6968 - val_binary_accuracy: 0.5034\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6936 - binary_accuracy: 0.4826 - val_loss: 0.6971 - val_binary_accuracy: 0.4414\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6921 - binary_accuracy: 0.5328 - val_loss: 0.6961 - val_binary_accuracy: 0.4690\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6912 - binary_accuracy: 0.5347 - val_loss: 0.6958 - val_binary_accuracy: 0.5034\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.6903 - binary_accuracy: 0.5444 - val_loss: 0.6959 - val_binary_accuracy: 0.4897\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6901 - binary_accuracy: 0.5502 - val_loss: 0.6964 - val_binary_accuracy: 0.4828\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6893 - binary_accuracy: 0.5386 - val_loss: 0.6959 - val_binary_accuracy: 0.5034\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6884 - binary_accuracy: 0.5714 - val_loss: 0.6952 - val_binary_accuracy: 0.5241\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6881 - binary_accuracy: 0.5792 - val_loss: 0.6947 - val_binary_accuracy: 0.5310\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6876 - binary_accuracy: 0.5695 - val_loss: 0.6943 - val_binary_accuracy: 0.5379\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6869 - binary_accuracy: 0.5811 - val_loss: 0.6947 - val_binary_accuracy: 0.5310\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6869 - binary_accuracy: 0.5811 - val_loss: 0.6953 - val_binary_accuracy: 0.5241\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6859 - binary_accuracy: 0.5869 - val_loss: 0.6941 - val_binary_accuracy: 0.5586\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6853 - binary_accuracy: 0.5753 - val_loss: 0.6945 - val_binary_accuracy: 0.5310\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6848 - binary_accuracy: 0.5849 - val_loss: 0.6935 - val_binary_accuracy: 0.5310\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6838 - binary_accuracy: 0.5907 - val_loss: 0.6939 - val_binary_accuracy: 0.5448\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6832 - binary_accuracy: 0.5946 - val_loss: 0.6943 - val_binary_accuracy: 0.5241\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6826 - binary_accuracy: 0.5927 - val_loss: 0.6949 - val_binary_accuracy: 0.4966\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6826 - binary_accuracy: 0.6004 - val_loss: 0.6932 - val_binary_accuracy: 0.5379\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6810 - binary_accuracy: 0.6062 - val_loss: 0.6932 - val_binary_accuracy: 0.5379\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6807 - binary_accuracy: 0.5907 - val_loss: 0.6950 - val_binary_accuracy: 0.4897\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6798 - binary_accuracy: 0.5965 - val_loss: 0.6954 - val_binary_accuracy: 0.4966\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6790 - binary_accuracy: 0.5946 - val_loss: 0.6954 - val_binary_accuracy: 0.4966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3_1 - 3_4, adding layers\n",
    "n_steps = X_train_cr_10.shape[1]\n",
    "n_features = X_train_cr_10.shape[2]\n",
    "\n",
    "model3_1_cr = Sequential()\n",
    "model3_1_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_1_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_1_cr.add(MaxPooling1D(pool_size=2))\n",
    "model3_1_cr.add(Flatten())\n",
    "model3_1_cr.add(Dense(8, activation='relu')) \n",
    "model3_1_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_1_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_1_cr.fit(X_train_cr_10, train_5w,epochs=30,  validation_data=(X_val_cr_10, val_5w), callbacks = [early_stopping_monitor])\n",
    "y_hat3_1_cr = model3_1_cr.predict(X_test_cr_10)\n",
    "y_hat3_1_cr = y_hat3_1_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_1_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4e97456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 10ms/step - loss: 0.6939 - binary_accuracy: 0.4942 - val_loss: 0.6952 - val_binary_accuracy: 0.4345\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6897 - binary_accuracy: 0.5772 - val_loss: 0.6961 - val_binary_accuracy: 0.4276\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6872 - binary_accuracy: 0.6158 - val_loss: 0.6951 - val_binary_accuracy: 0.4621\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6844 - binary_accuracy: 0.6236 - val_loss: 0.6946 - val_binary_accuracy: 0.4966\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6822 - binary_accuracy: 0.6409 - val_loss: 0.6948 - val_binary_accuracy: 0.4966\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6791 - binary_accuracy: 0.6583 - val_loss: 0.6954 - val_binary_accuracy: 0.4966\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6756 - binary_accuracy: 0.6718 - val_loss: 0.6948 - val_binary_accuracy: 0.4966\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6712 - binary_accuracy: 0.6795 - val_loss: 0.6952 - val_binary_accuracy: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5428571428571428"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_cr_25.shape[1]\n",
    "n_features = X_train_cr_25.shape[2]\n",
    "\n",
    "model3_2_cr = Sequential()\n",
    "model3_2_cr.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_2_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_2_cr.add(MaxPooling1D(pool_size=2))\n",
    "model3_2_cr.add(Flatten())\n",
    "model3_2_cr.add(Dense(25, activation='relu')) \n",
    "model3_2_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_2_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_2_cr.fit(X_train_cr_25, train_5w,epochs=30,  validation_data=(X_val_cr_25, val_5w), callbacks = [early_stopping_monitor])\n",
    "y_hat3_2_cr = model3_2_cr.predict(X_test_cr_25)\n",
    "y_hat3_2_cr = y_hat3_2_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_2_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2679fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 11ms/step - loss: 0.7003 - binary_accuracy: 0.4846 - val_loss: 0.6944 - val_binary_accuracy: 0.5034\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6924 - binary_accuracy: 0.5309 - val_loss: 0.6936 - val_binary_accuracy: 0.5172\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6885 - binary_accuracy: 0.5618 - val_loss: 0.6927 - val_binary_accuracy: 0.5310\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6846 - binary_accuracy: 0.6004 - val_loss: 0.6908 - val_binary_accuracy: 0.5517\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6815 - binary_accuracy: 0.6216 - val_loss: 0.6901 - val_binary_accuracy: 0.5655\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6769 - binary_accuracy: 0.6216 - val_loss: 0.6912 - val_binary_accuracy: 0.5862\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6703 - binary_accuracy: 0.6351 - val_loss: 0.6919 - val_binary_accuracy: 0.5931\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6628 - binary_accuracy: 0.6544 - val_loss: 0.6929 - val_binary_accuracy: 0.5517\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6548 - binary_accuracy: 0.6583 - val_loss: 0.6922 - val_binary_accuracy: 0.5724\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_steps = X_train_cr_40.shape[1]\n",
    "n_features = X_train_cr_40.shape[2]\n",
    "\n",
    "model3_3_cr = Sequential()\n",
    "model3_3_cr.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_3_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_3_cr.add(MaxPooling1D(pool_size=2))\n",
    "model3_3_cr.add(Flatten())\n",
    "model3_3_cr.add(Dense(25, activation='relu')) \n",
    "model3_3_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_3_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_3_cr.fit(X_train_cr_40, train_5w,epochs=30,  validation_data=(X_val_cr_40, val_5w), callbacks = [early_stopping_monitor])\n",
    "y_hat3_3_cr = model3_3_cr.predict(X_test_cr_40)\n",
    "y_hat3_3_cr = y_hat3_3_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_3_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "839a97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 14ms/step - loss: 0.7292 - binary_accuracy: 0.4961 - val_loss: 0.7089 - val_binary_accuracy: 0.5310\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7060 - binary_accuracy: 0.4768 - val_loss: 0.6995 - val_binary_accuracy: 0.4759\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6987 - binary_accuracy: 0.4865 - val_loss: 0.6986 - val_binary_accuracy: 0.4690\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6962 - binary_accuracy: 0.5019 - val_loss: 0.6986 - val_binary_accuracy: 0.4552\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6948 - binary_accuracy: 0.5058 - val_loss: 0.6981 - val_binary_accuracy: 0.4552\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6930 - binary_accuracy: 0.5116 - val_loss: 0.6975 - val_binary_accuracy: 0.4414\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6915 - binary_accuracy: 0.5290 - val_loss: 0.6974 - val_binary_accuracy: 0.4552\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6908 - binary_accuracy: 0.5270 - val_loss: 0.6983 - val_binary_accuracy: 0.4621\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6889 - binary_accuracy: 0.5405 - val_loss: 0.6980 - val_binary_accuracy: 0.4621\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6872 - binary_accuracy: 0.5483 - val_loss: 0.6973 - val_binary_accuracy: 0.4483\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6853 - binary_accuracy: 0.5618 - val_loss: 0.6967 - val_binary_accuracy: 0.4483\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6840 - binary_accuracy: 0.5598 - val_loss: 0.6968 - val_binary_accuracy: 0.4483\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6821 - binary_accuracy: 0.5714 - val_loss: 0.6968 - val_binary_accuracy: 0.4552\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6802 - binary_accuracy: 0.5734 - val_loss: 0.6971 - val_binary_accuracy: 0.4621\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6789 - binary_accuracy: 0.5888 - val_loss: 0.6976 - val_binary_accuracy: 0.5034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5428571428571428"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_cr_55.shape[1]\n",
    "n_features = X_train_cr_55.shape[2]\n",
    "\n",
    "model3_4_cr = Sequential()\n",
    "model3_4_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model3_4_cr.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model3_4_cr.add(MaxPooling1D(pool_size=2))\n",
    "model3_4_cr.add(Flatten())\n",
    "model3_4_cr.add(Dense(25, activation='relu')) \n",
    "model3_4_cr.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3_4_cr.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model3_4_cr.fit(X_train_cr_55, train_5w,epochs=30,  validation_data=(X_val_cr_55, val_5w), callbacks = [early_stopping_monitor])\n",
    "y_hat3_4_cr = model3_4_cr.predict(X_test_cr_55)\n",
    "y_hat3_4_cr = y_hat3_4_cr > .5\n",
    "metrics.accuracy_score(test_5w,y_hat3_4_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f9482",
   "metadata": {},
   "source": [
    "# Changes to the Target Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae54949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:301: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n"
     ]
    }
   ],
   "source": [
    "X_train_kb_10, X_val_kb_10, X_test_kb_10 = kbest_creator(10, X_train.drop('target_3', axis=1), X_val.drop('target_3', axis=1), X_test.drop('target_3', axis=1)) \n",
    "X_train_kb_25, X_val_kb_25, X_test_kb_25 = kbest_creator(25, X_train.drop('target_3', axis=1), X_val.drop('target_3', axis=1), X_test.drop('target_3', axis=1)) \n",
    "X_train_kb_40, X_val_kb_40, X_test_kb_40 = kbest_creator(40, X_train.drop('target_3', axis=1), X_val.drop('target_3', axis=1), X_test.drop('target_3', axis=1)) \n",
    "X_train_kb_55, X_val_kb_55, X_test_kb_55 = kbest_creator(55, X_train.drop('target_3', axis=1), X_val.drop('target_3', axis=1), X_test.drop('target_3', axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d12e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y_mod(df, target, window_size=5):\n",
    "  # converts to matrix of numpy arrays\n",
    "  X = []\n",
    "  y = []\n",
    "  for i in range(len(df)-window_size): # length of data frame - window_size so it does't take empty values at the end, \n",
    "    # does force you to loose the last 5 values, could fix with padding\n",
    "    row = [r for r in df[i:i+window_size]] # grabs row i and all rows above within the window size length\n",
    "    X.append(row) # creates 3 dimentional array, (# obseravtions, # rows in window, # features)\n",
    "    label = target[i+window_size-1] # pulls the target variable after the window, target varible needs to be column zero in this \n",
    "    y.append(label) # returns (N,) martix of targets i+window_length time periods away\n",
    "  return np.array(X), np.array(y)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72d79d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to window format, in this case 5 periods\n",
    "X_train_kb_40, train_5w = df_to_X_y2(X_train_kb_40,y_train)\n",
    "X_val_kb_40, val_5w = df_to_X_y2(X_val_kb_40, y_val)\n",
    "X_test_kb_40, test_5w = df_to_X_y2(X_test_kb_40,y_test) \n",
    "\n",
    "X_train_kb_10, _ = df_to_X_y2(X_train_kb_10,y_train)\n",
    "X_val_kb_10, _ = df_to_X_y2(X_val_kb_10, y_val)\n",
    "X_test_kb_10, _ = df_to_X_y2(X_test_kb_10,y_test) \n",
    "\n",
    "X_train_kb_25, _ = df_to_X_y2(X_train_kb_25,y_train)\n",
    "X_val_kb_25, _ = df_to_X_y2(X_val_kb_25, y_val)\n",
    "X_test_kb_25, _ = df_to_X_y2(X_test_kb_25,y_test) \n",
    "\n",
    "X_train_kb_55, _ = df_to_X_y2(X_train_kb_55,y_train)\n",
    "X_val_kb_55, _ = df_to_X_y2(X_val_kb_55, y_val)\n",
    "X_test_kb_55, _ = df_to_X_y2(X_test_kb_55,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10acaa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 19ms/step - loss: 0.6974 - binary_accuracy: 0.5000 - val_loss: 0.6860 - val_binary_accuracy: 0.5241\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6921 - binary_accuracy: 0.4942 - val_loss: 0.6868 - val_binary_accuracy: 0.5310\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6906 - binary_accuracy: 0.5309 - val_loss: 0.6870 - val_binary_accuracy: 0.5517\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6889 - binary_accuracy: 0.5367 - val_loss: 0.6861 - val_binary_accuracy: 0.5103\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6878 - binary_accuracy: 0.5463 - val_loss: 0.6861 - val_binary_accuracy: 0.5172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7989949520>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = X_train_kb_25.shape[1]\n",
    "n_features = X_train_kb_25.shape[2]\n",
    "\n",
    "model_mod4 = Sequential()\n",
    "model_mod4.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_mod4.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model_mod4.add(MaxPooling1D(pool_size=2))\n",
    "model_mod4.add(Conv1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model_mod4.add(Flatten())\n",
    "model_mod4.add(Dense(25, activation='relu')) \n",
    "model_mod4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_mod4.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model_mod4.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c39a79c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mod4_kb = model_mod4.predict(X_test_kb_25)\n",
    "y_hat_mod4_kb = y_hat_mod4_kb > .5\n",
    "metrics.accuracy_score(test_5w,y_hat_mod4_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3b1741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 12ms/step - loss: 0.6984 - binary_accuracy: 0.5058 - val_loss: 0.6932 - val_binary_accuracy: 0.4966\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6929 - binary_accuracy: 0.5386 - val_loss: 0.6934 - val_binary_accuracy: 0.4966\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6907 - binary_accuracy: 0.5541 - val_loss: 0.6931 - val_binary_accuracy: 0.5793\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6886 - binary_accuracy: 0.5483 - val_loss: 0.6954 - val_binary_accuracy: 0.5448\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6869 - binary_accuracy: 0.5618 - val_loss: 0.6950 - val_binary_accuracy: 0.5310\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6855 - binary_accuracy: 0.5695 - val_loss: 0.6964 - val_binary_accuracy: 0.5448\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6844 - binary_accuracy: 0.5579 - val_loss: 0.6961 - val_binary_accuracy: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7989dd9fa0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mod3 = Sequential()\n",
    "model_mod3.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_mod3.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model_mod3.add(MaxPooling1D(pool_size=2))\n",
    "model_mod3.add(Flatten())\n",
    "model_mod3.add(Dense(25, activation='relu')) \n",
    "model_mod3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_mod3.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model_mod3.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w), callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3532b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5571428571428572"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mod3_kb = model_mod3.predict(X_test_kb_25)\n",
    "y_hat_mod3_kb = y_hat_mod3_kb > .5\n",
    "metrics.accuracy_score(test_5w,y_hat_mod3_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "09950897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 15ms/step - loss: 0.6942 - binary_accuracy: 0.5000 - val_loss: 0.7006 - val_binary_accuracy: 0.5379\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6903 - binary_accuracy: 0.5174 - val_loss: 0.6993 - val_binary_accuracy: 0.5034\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6856 - binary_accuracy: 0.5541 - val_loss: 0.6998 - val_binary_accuracy: 0.4828\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6842 - binary_accuracy: 0.5579 - val_loss: 0.6999 - val_binary_accuracy: 0.4966\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6818 - binary_accuracy: 0.5598 - val_loss: 0.7002 - val_binary_accuracy: 0.4966\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6800 - binary_accuracy: 0.5714 - val_loss: 0.7000 - val_binary_accuracy: 0.5103\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6780 - binary_accuracy: 0.5830 - val_loss: 0.7006 - val_binary_accuracy: 0.5103\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6757 - binary_accuracy: 0.5888 - val_loss: 0.7005 - val_binary_accuracy: 0.5103\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6740 - binary_accuracy: 0.5907 - val_loss: 0.7008 - val_binary_accuracy: 0.4897\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6720 - binary_accuracy: 0.5985 - val_loss: 0.7016 - val_binary_accuracy: 0.4897\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6702 - binary_accuracy: 0.5985 - val_loss: 0.7027 - val_binary_accuracy: 0.4897\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6678 - binary_accuracy: 0.6062 - val_loss: 0.7023 - val_binary_accuracy: 0.5034\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6656 - binary_accuracy: 0.6100 - val_loss: 0.7024 - val_binary_accuracy: 0.5103\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6631 - binary_accuracy: 0.6178 - val_loss: 0.7008 - val_binary_accuracy: 0.5103\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6612 - binary_accuracy: 0.6197 - val_loss: 0.7002 - val_binary_accuracy: 0.5172\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6593 - binary_accuracy: 0.6158 - val_loss: 0.7014 - val_binary_accuracy: 0.5310\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6560 - binary_accuracy: 0.6236 - val_loss: 0.7014 - val_binary_accuracy: 0.5241\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6540 - binary_accuracy: 0.6332 - val_loss: 0.7016 - val_binary_accuracy: 0.5172\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6519 - binary_accuracy: 0.6332 - val_loss: 0.7029 - val_binary_accuracy: 0.5241\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6482 - binary_accuracy: 0.6371 - val_loss: 0.7015 - val_binary_accuracy: 0.5379\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6458 - binary_accuracy: 0.6293 - val_loss: 0.7019 - val_binary_accuracy: 0.5379\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6472 - binary_accuracy: 0.6274 - val_loss: 0.7009 - val_binary_accuracy: 0.5517\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6412 - binary_accuracy: 0.6486 - val_loss: 0.7039 - val_binary_accuracy: 0.5172\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6384 - binary_accuracy: 0.6564 - val_loss: 0.7034 - val_binary_accuracy: 0.5379\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6362 - binary_accuracy: 0.6390 - val_loss: 0.7050 - val_binary_accuracy: 0.5517\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6333 - binary_accuracy: 0.6448 - val_loss: 0.7072 - val_binary_accuracy: 0.5172\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6283 - binary_accuracy: 0.6486 - val_loss: 0.7095 - val_binary_accuracy: 0.5448\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6259 - binary_accuracy: 0.6544 - val_loss: 0.7137 - val_binary_accuracy: 0.5310\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6230 - binary_accuracy: 0.6564 - val_loss: 0.7139 - val_binary_accuracy: 0.5310\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6189 - binary_accuracy: 0.6544 - val_loss: 0.7126 - val_binary_accuracy: 0.5517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f798ccb6fa0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mod2 = Sequential()\n",
    "model_mod2.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_mod2.add(Conv1D(filters=8, kernel_size=2, activation='relu')) \n",
    "model_mod2.add(Flatten())\n",
    "model_mod2.add(Dense(25, activation='relu')) \n",
    "model_mod2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_mod2.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model_mod2.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88ba84ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142857142857142"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mod2_kb = model_mod2.predict(X_test_kb_25)\n",
    "y_hat_mod2_kb = y_hat_mod2_kb > .5\n",
    "metrics.accuracy_score(test_5w,y_hat_mod2_kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ba039765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabrizio/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 9ms/step - loss: 0.7142 - binary_accuracy: 0.4575 - val_loss: 0.6890 - val_binary_accuracy: 0.5034\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7077 - binary_accuracy: 0.4826 - val_loss: 0.6881 - val_binary_accuracy: 0.5034\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.7025 - binary_accuracy: 0.4942 - val_loss: 0.6865 - val_binary_accuracy: 0.5379\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6987 - binary_accuracy: 0.4961 - val_loss: 0.6860 - val_binary_accuracy: 0.5448\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6954 - binary_accuracy: 0.5154 - val_loss: 0.6860 - val_binary_accuracy: 0.5379\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6927 - binary_accuracy: 0.5405 - val_loss: 0.6844 - val_binary_accuracy: 0.5655\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6898 - binary_accuracy: 0.5328 - val_loss: 0.6869 - val_binary_accuracy: 0.5310\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6875 - binary_accuracy: 0.5425 - val_loss: 0.6863 - val_binary_accuracy: 0.5172\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6853 - binary_accuracy: 0.5695 - val_loss: 0.6872 - val_binary_accuracy: 0.5379\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6832 - binary_accuracy: 0.5772 - val_loss: 0.6874 - val_binary_accuracy: 0.5241\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6811 - binary_accuracy: 0.5927 - val_loss: 0.6859 - val_binary_accuracy: 0.5793\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6788 - binary_accuracy: 0.5965 - val_loss: 0.6865 - val_binary_accuracy: 0.5724\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6767 - binary_accuracy: 0.6100 - val_loss: 0.6861 - val_binary_accuracy: 0.6000\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6747 - binary_accuracy: 0.6139 - val_loss: 0.6853 - val_binary_accuracy: 0.5793\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6724 - binary_accuracy: 0.6100 - val_loss: 0.6863 - val_binary_accuracy: 0.5724\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6709 - binary_accuracy: 0.6100 - val_loss: 0.6877 - val_binary_accuracy: 0.5655\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6690 - binary_accuracy: 0.6120 - val_loss: 0.6878 - val_binary_accuracy: 0.5310\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6668 - binary_accuracy: 0.6236 - val_loss: 0.6891 - val_binary_accuracy: 0.5379\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6643 - binary_accuracy: 0.6236 - val_loss: 0.6904 - val_binary_accuracy: 0.5241\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6621 - binary_accuracy: 0.6236 - val_loss: 0.6914 - val_binary_accuracy: 0.5241\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6614 - binary_accuracy: 0.6409 - val_loss: 0.6939 - val_binary_accuracy: 0.5034\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6583 - binary_accuracy: 0.6409 - val_loss: 0.6922 - val_binary_accuracy: 0.5103\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6562 - binary_accuracy: 0.6332 - val_loss: 0.6933 - val_binary_accuracy: 0.5103\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6541 - binary_accuracy: 0.6467 - val_loss: 0.6940 - val_binary_accuracy: 0.5241\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6520 - binary_accuracy: 0.6467 - val_loss: 0.6934 - val_binary_accuracy: 0.5172\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6500 - binary_accuracy: 0.6448 - val_loss: 0.6937 - val_binary_accuracy: 0.5034\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.6473 - binary_accuracy: 0.6448 - val_loss: 0.6947 - val_binary_accuracy: 0.5103\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6457 - binary_accuracy: 0.6506 - val_loss: 0.6968 - val_binary_accuracy: 0.5172\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.6432 - binary_accuracy: 0.6660 - val_loss: 0.6979 - val_binary_accuracy: 0.4966\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.6409 - binary_accuracy: 0.6718 - val_loss: 0.6998 - val_binary_accuracy: 0.4897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7989d3de80>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mod1 = Sequential()\n",
    "model_mod1.add(Conv1D(filters=8, kernel_size=2, activation='relu', input_shape=(n_steps,n_features))) \n",
    "model_mod1.add(Flatten())\n",
    "model_mod1.add(Dense(25, activation='relu')) \n",
    "model_mod1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_mod1.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                   metrics=[\"BinaryAccuracy\"])\n",
    "\n",
    "model_mod1.fit(X_train_kb_25, train_5w,epochs=30,  validation_data=(X_val_kb_25, val_5w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89178642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5285714285714286"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_mod1_kb = model_mod1.predict(X_test_kb_25)\n",
    "y_hat_mod1_kb = y_hat_mod1_kb > .5\n",
    "metrics.accuracy_score(test_5w,y_hat_mod1_kb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31582b9feba862c420bc95ad7fac43fb721c474490d1710b4e50ac63470f9531"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

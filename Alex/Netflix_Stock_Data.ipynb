{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection -  Netflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#%pip install pytrends\n",
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "#pip install pageviewapi\n",
    "import pageviewapi\n",
    "#%pip install yfinance\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Big_scraper(kw_list_1, kw_list_2, ticker, start,end):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    \n",
    "    The function initially grabs historical, indexed, hourly data for when the keyword \n",
    "    was searched most as shown on Google Trends' Interest Over Time section.\n",
    "    It then cleans the data to show daily hits on the keyword in Google news.\n",
    "\n",
    "    \n",
    "    input:\n",
    "    -----\n",
    "    kw_list_1: List of up to 5 key words that will be scraped from google trends for the dates given.\n",
    "             Here, the scraping will pull the total posted items in google news that contains\n",
    "             one of the key words.\n",
    "    \n",
    "    kw_list_2: List of wikipedia article titles (unlimited length) that will pull the amount of\n",
    "            views the article recieved each day. \n",
    "\n",
    "    ticker: the ticker abriviation of the desired stock. Must be netered in as an all capitalized string \n",
    "    example Apple Inc. woud be \"AAPL\"\n",
    "             \n",
    "    start: the start of the desired timeline you want scrape. Date Must be entered in as \"YYYYMMDD\"\n",
    "    \n",
    "    end: the end of the desired timeline you want scrape. Date Must be entered in as \"YYYYMMDD\"\n",
    "             \n",
    "    return:\n",
    "    -------\n",
    "    \n",
    "    combined: a dataframe containing the sum of the daily keyword hits in google news (key words labeled _x),\n",
    "    \n",
    "    data frame cointaing stock info including open, close, high, low prices of the stock,\n",
    "    as well as the stocks daily trading volume and the amount if there was a split or dividend \n",
    "    preformed on the stock that day,\n",
    "    \n",
    "    and the sum of how many times key word wikipedia pages were viewed in a day (key words labeled _y)\n",
    "    \"\"\"\n",
    "    \n",
    "    year_s = int(start[0:4])\n",
    "    month_s = int(start[4:6])\n",
    "    day_s = int(start[6:8])\n",
    "    year_e = int(end[0:4])\n",
    "    month_e = int(end[4:6])\n",
    "    day_e = int(end[6:8])\n",
    "    \n",
    "    starter = pd.to_datetime(f\"'{year_s}-{month_s}-{day_s}'\")\n",
    "    ender = pd.to_datetime(f\"'{year_e}-{month_e}-{day_e}'\")\n",
    "    \n",
    "    \n",
    "    pytrends = TrendReq(hl='en-US', tz=360, retries=10)\n",
    "    jeff = pytrends.get_historical_interest(kw_list_1, \\\n",
    "                                 year_start = year_s, month_start = month_s, day_start = day_s, hour_start = 1, \\\n",
    "                                 year_end = year_e, month_end = month_e, day_end = day_e, hour_end = 23, \\\n",
    "                                 cat = 0, geo = '', gprop = 'news', sleep = 60)\n",
    "    \n",
    "    jeff = jeff.iloc[:, 0:-1] # eliminates the isPartial Column\n",
    "    jeff = jeff.reset_index().drop_duplicates(subset = \"date\") #removing duplicates from the index\n",
    "    jeff = jeff.groupby(pd.Grouper(key=\"date\", freq=\"D\")).mean() # coverts to the mean of daily scores\n",
    "\n",
    "    dow = yf.Ticker(\"^DJI\")\n",
    "    dow_h = dow.history(start=starter, end=ender)\n",
    "    dow_h = pd.DataFrame(dow_h)\n",
    "    dow_names = {\"Open\":\"dow_open\",\"Close\":\"dow_close\",\"Low\": \"dow_low\",\n",
    "    'High':'dow_high','Volume':'dow_vol'}\n",
    "    dow_h=dow_h.rename(dow_names, axis=1).drop([\"Dividends\",\"Stock Splits\"], axis=1)\n",
    "\n",
    "    nas = yf.Ticker(\"^IXIC\")\n",
    "    nas_h = nas.history(start=starter, end=ender)\n",
    "    nas_h = pd.DataFrame(nas_h)\n",
    "    nas_names = {\"Open\":\"nas_open\", \"Close\":\"nas_close\", \"Low\": \"nas_low\",\n",
    "    'High':'nas_high','Volume':'nas_vol'}\n",
    "    nas_h=nas_h.rename(nas_names, axis=1).drop([\"Dividends\",\"Stock Splits\"], axis=1)\n",
    "\n",
    "    market = dow_h.merge(nas_h,left_index=True, right_index=True, how=\"left\")\n",
    "    \n",
    "    tick = yf.Ticker(ticker)\n",
    "    hist = tick.history(start=starter, end=ender)\n",
    "    hist = pd.DataFrame(hist)\n",
    "    \n",
    "    combined = jeff.merge(hist, left_index=True, right_index=True, how=\"left\")\n",
    "    combined = combined.merge(market, left_index=True, right_index=True, how=\"left\")  \n",
    "    \n",
    "    d = pd.DataFrame()\n",
    "    for key_word in kw_list_2:\n",
    "        geoff = pageviewapi.per_article('en.wikipedia', key_word, start, end,\n",
    "                                    access='all-access', agent='all-agents', granularity='daily')\n",
    "        dicty = dict(geoff)\n",
    "        views = pd.DataFrame(dicty[\"items\"])\n",
    "        views[\"timestamp\"] = pd.to_datetime((views[\"timestamp\"]), format=\"%Y%m%d%H\")\n",
    "        views = views.set_index(\"timestamp\")\n",
    "        page = pd.Series(views[\"views\"])\n",
    "        d[key_word] = page\n",
    "        \n",
    "    combined = combined.merge(d, left_index=True, right_index=True, how=\"left\") \n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pull for Netflix\n",
    "kw_list_1 = [\"Netflix. Inc\", \"Netflix\", \"Netflix Stock\", \"Streaming media\", \"Reed Hastings\"]\n",
    "kw_list_2 = [\"Netflix\", \"Reed Hastings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull neflix data for 2019\n",
    "neflix = Big_scraper(kw_list_1, kw_list_2, \"NFLX\", \"20190101\", \"20191231\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-lable variable name for future use\n",
    "netflix19 = neflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull neflix data for 2020\n",
    "netflix20 = Big_scraper(kw_list_1, kw_list_2, \"NFLX\", \"20200101\", \"20201231\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull neflix data for 2021\n",
    "netflix21 = Big_scraper(kw_list_1, kw_list_2, \"NFLX\", \"20210101\", \"20211231\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull neflix data for 2022\n",
    "netflix22 = Big_scraper(kw_list_1, kw_list_2, \"NFLX\", \"20220101\", \"20220331\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all the dataframes (2019 - 2022)\n",
    "netflix = pd.concat([netflix19,netflix20,netflix21,netflix22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating base varibles to be used in variable creator functions\n",
    "netflix[\"Wiki_total\"] = (netflix[\"Netflix_y\"] + netflix[\"Reed Hastings_y\"]) \n",
    "\n",
    "netflix[\"Google_total\"] = (netflix[\"Netflix. Inc\"] + netflix[\"Netflix_x\"] +\n",
    "                           netflix[\"Netflix Stock\"] + netflix[\"Streaming media\"] + \n",
    "                           netflix[\"Reed Hastings_x\"])   \n",
    "\n",
    "netflix[\"Stock_total\"] = netflix[\"Close\"]\n",
    "netflix[\"Nas_total\"] = netflix[\"nas_close\"]\n",
    "netflix[\"Dow_total\"] = netflix[\"dow_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Netflix. Inc</th>\n",
       "      <th>Netflix_x</th>\n",
       "      <th>Netflix Stock</th>\n",
       "      <th>Streaming media</th>\n",
       "      <th>Reed Hastings_x</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>nas_low</th>\n",
       "      <th>nas_close</th>\n",
       "      <th>nas_vol</th>\n",
       "      <th>Netflix_y</th>\n",
       "      <th>Reed Hastings_y</th>\n",
       "      <th>Wiki_total</th>\n",
       "      <th>Google_total</th>\n",
       "      <th>Stock_total</th>\n",
       "      <th>Nas_total</th>\n",
       "      <th>Dow_total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18853</td>\n",
       "      <td>2310</td>\n",
       "      <td>21163</td>\n",
       "      <td>29.086957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>26.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>259.279999</td>\n",
       "      <td>269.750000</td>\n",
       "      <td>256.579987</td>\n",
       "      <td>267.660004</td>\n",
       "      <td>11679500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6506.879883</td>\n",
       "      <td>6665.939941</td>\n",
       "      <td>2.261800e+09</td>\n",
       "      <td>22905</td>\n",
       "      <td>3146</td>\n",
       "      <td>26051</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>267.660004</td>\n",
       "      <td>6665.939941</td>\n",
       "      <td>23346.240234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>270.200012</td>\n",
       "      <td>275.790009</td>\n",
       "      <td>264.429993</td>\n",
       "      <td>271.200012</td>\n",
       "      <td>14969600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6457.129883</td>\n",
       "      <td>6463.500000</td>\n",
       "      <td>2.607290e+09</td>\n",
       "      <td>20646</td>\n",
       "      <td>2666</td>\n",
       "      <td>23312</td>\n",
       "      <td>32.875000</td>\n",
       "      <td>271.200012</td>\n",
       "      <td>6463.500000</td>\n",
       "      <td>22686.220703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.708333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>281.880005</td>\n",
       "      <td>297.799988</td>\n",
       "      <td>278.540009</td>\n",
       "      <td>297.570007</td>\n",
       "      <td>19330100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6554.240234</td>\n",
       "      <td>6738.859863</td>\n",
       "      <td>2.579550e+09</td>\n",
       "      <td>19040</td>\n",
       "      <td>2307</td>\n",
       "      <td>21347</td>\n",
       "      <td>30.083333</td>\n",
       "      <td>297.570007</td>\n",
       "      <td>6738.859863</td>\n",
       "      <td>23433.160156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19227</td>\n",
       "      <td>2021</td>\n",
       "      <td>21248</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Netflix. Inc  Netflix_x  Netflix Stock  Streaming media  \\\n",
       "date                                                                  \n",
       "2019-01-01           0.0  29.086957       0.000000              0.0   \n",
       "2019-01-02           0.0  26.375000       0.500000              0.0   \n",
       "2019-01-03           0.0  32.541667       0.333333              0.0   \n",
       "2019-01-04           0.0  29.708333       0.000000              0.0   \n",
       "2019-01-05           0.0  30.333333       0.000000              0.0   \n",
       "\n",
       "            Reed Hastings_x        Open        High         Low       Close  \\\n",
       "date                                                                          \n",
       "2019-01-01            0.000         NaN         NaN         NaN         NaN   \n",
       "2019-01-02            0.000  259.279999  269.750000  256.579987  267.660004   \n",
       "2019-01-03            0.000  270.200012  275.790009  264.429993  271.200012   \n",
       "2019-01-04            0.375  281.880005  297.799988  278.540009  297.570007   \n",
       "2019-01-05            0.000         NaN         NaN         NaN         NaN   \n",
       "\n",
       "                Volume  ...      nas_low    nas_close       nas_vol  \\\n",
       "date                    ...                                           \n",
       "2019-01-01         NaN  ...          NaN          NaN           NaN   \n",
       "2019-01-02  11679500.0  ...  6506.879883  6665.939941  2.261800e+09   \n",
       "2019-01-03  14969600.0  ...  6457.129883  6463.500000  2.607290e+09   \n",
       "2019-01-04  19330100.0  ...  6554.240234  6738.859863  2.579550e+09   \n",
       "2019-01-05         NaN  ...          NaN          NaN           NaN   \n",
       "\n",
       "            Netflix_y  Reed Hastings_y  Wiki_total  Google_total  Stock_total  \\\n",
       "date                                                                            \n",
       "2019-01-01      18853             2310       21163     29.086957          NaN   \n",
       "2019-01-02      22905             3146       26051     26.875000   267.660004   \n",
       "2019-01-03      20646             2666       23312     32.875000   271.200012   \n",
       "2019-01-04      19040             2307       21347     30.083333   297.570007   \n",
       "2019-01-05      19227             2021       21248     30.333333          NaN   \n",
       "\n",
       "              Nas_total     Dow_total  \n",
       "date                                   \n",
       "2019-01-01          NaN           NaN  \n",
       "2019-01-02  6665.939941  23346.240234  \n",
       "2019-01-03  6463.500000  22686.220703  \n",
       "2019-01-04  6738.859863  23433.160156  \n",
       "2019-01-05          NaN           NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_creator(df, variable_list, w=7):\n",
    "    \n",
    "    '''\n",
    "    descrition:\n",
    "    -----------\n",
    "    takes dataframe and returns new varibles based on recommmended calcualtions, \n",
    "    it should be done seporately with internet based and stock based dataframes\n",
    "\n",
    "    Note: When using for aggreated varaibles, for example Wiki_total, the sum of all the wiki pages daily page view counts, \n",
    "    you MUST calculate Wiki_total in the dataframe seperately BEFORE this function can be used. \n",
    "\n",
    "    For example if you had wiki page counts for Ford Bronco and Ford Ranger, Wiki_total would equal Ford Bronco + Ford ranger counts, \n",
    "    AGAIN Wiki_total must be calculated in the desired dateframe before using this function \n",
    "\n",
    "    input:\n",
    "    ------\n",
    "    df: dataframe containing the google trends, yahoo finance, and or wikipedia page count data\n",
    "\n",
    "    variable_list: list of strings to be added to the equations to calculate the new varaibles. \n",
    "    ex. insertting the string \"Wiki\" will add to df[f\"{}_total\"] to become \"Wiki_total\"\n",
    "\n",
    "    w: the window length for one period shift. Default is 7 providing 7 day moving averages for wiki and google data, \n",
    "        FOR STOCK DATA THIS WILL NEED TO BE CHANGED TO 5.\n",
    "\n",
    "    output:\n",
    "    -------\n",
    "    df: the same dataframe as was inputted but now containing variables for \n",
    "    Momemtum, Disparity, Moving Average, Exponential Moving Aerage, Rator Change, and RSI index score.\n",
    "    Also containg are moving variables, which are boolean with 1 indicating an increase in the above variables\n",
    "    '''   \n",
    "    \n",
    "    for i in variable_list:\n",
    "        # Momentum_1\n",
    "        df[f\"{i}_Moment_1\"] =  (df[f\"{i}_total\"] / df[f\"{i}_total\"].shift(w)) * 100\n",
    "        # Momentum_2\n",
    "        df[f\"{i}_Moment_2\"] =  (df[f\"{i}_total\"] - df[f\"{i}_total\"].shift(w)) * 100\n",
    "        # Momentum_1_s three day shift (instead of w)\n",
    "        df[f\"{i}_Moment_1_s\"] =  (df[f\"{i}_total\"] / df[f\"{i}_total\"].shift(3)) * 100\n",
    "        # Momentum_2_s\n",
    "        df[f\"{i}_Moment_2_s\"] =  (df[f\"{i}_total\"] - df[f\"{i}_total\"].shift(3)) * 100\n",
    "        # Moving average\n",
    "        df[f\"{i}_MAvg\"] = df[f\"{i}_total\"].rolling(f\"{w}d\").mean()\n",
    "        # Moving average 3 day\n",
    "        df[f\"{i}_MAvg_s\"] = df[f\"{i}_total\"].rolling(\"3d\").mean()\n",
    "        # Disparity\n",
    "        df[f\"{i}_Disparity\"] = (df[f\"{i}_total\"]/df[f\"{i}_MAvg\"]) * 100\n",
    "        # Disparity 3 day\n",
    "        df[f\"{i}_Disparity_s\"] = (df[f\"{i}_total\"]/df[f\"{i}_MAvg_s\"]) * 100\n",
    "        # Rate of Change Normal Way\n",
    "        df[f\"{i}_ROC\"] = (df[f\"{i}_total\"]-df[f\"{i}_total\"].shift(w))/(df[f\"{i}_total\"].shift(w)) *100\n",
    "        df[f\"{i}_ROC_s\"] = (df[f\"{i}_total\"]-df[f\"{i}_total\"].shift(3))/(df[f\"{i}_total\"].shift(3)) *100\n",
    "        #Rate of Change Paper Way (doesn't make sense but just in case)\n",
    "        df[f'{i}_Rocp'] = (df[f\"{i}_total\"]/df[f\"{i}_Moment_2\"]) *100\n",
    "        # Exponential Moving Average\n",
    "        df[f\"{i}_EMA\"] = (df[f\"{i}_total\"]-df[f\"{i}_MAvg\"].shift(1))*(2/(w+1))+df[f\"{i}_MAvg\"].shift(1)\n",
    "\n",
    "        # calculating the Relative Strength Index, based on 14 day window\n",
    "        df[f\"{i}_diff\"] = df[f\"{i}_total\"].diff(1)\n",
    "        df[f\"{i}_gain\"] = df[f\"{i}_diff\"].clip(lower=0).round(2) #keeps all values above or below a given threshold, lower=lower bound\n",
    "        df[f\"{i}_loss\"] = df[f\"{i}_diff\"].clip(upper=0).round(2)\n",
    "        df[f'{i}_avg_gain'] = df[f'{i}_gain'].rolling(14).mean()\n",
    "        df[f'{i}_avg_loss'] = df[f'{i}_loss'].rolling(14).mean()\n",
    "        df[f'{i}_rs'] = df[f'{i}_avg_gain'] / df[f'{i}_avg_loss']\n",
    "        df[f'{i}_RSI'] = 100 - (100 / (1.0 + df[f'{i}_rs']))\n",
    "\n",
    "        # Calculatiing the Move Variables \n",
    "        df[f\"{i}_Move\"] = df[f\"{i}_total\"] > df[f\"{i}_total\"].shift(1) \n",
    "        df[f\"{i}_Move\"] = df[f\"{i}_Move\"].replace({True:1,False: 0})\n",
    "    \n",
    "      \n",
    "        df[f\"{i}_MAvg_Move\"] = df[f\"{i}_MAvg\"] > df[f\"{i}_MAvg\"].shift(1) \n",
    "        df[f\"{i}_MAvg_Move\"] = df[f\"{i}_MAvg_Move\"].replace({True:1,False: 0})\n",
    "        df[f\"{i}_MAvg_s_Move\"] = df[f\"{i}_MAvg_s\"] > df[f\"{i}_MAvg_s\"].shift(1) \n",
    "        df[f\"{i}_MAvg_s_Move\"] = df[f\"{i}_MAvg_s_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_EMA_Move\"] = df[f\"{i}_EMA\"] > df[f\"{i}_EMA\"].shift(1) \n",
    "        df[f\"{i}_EMA_Move\"] = df[f\"{i}_EMA_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_Disparity_Move\"] = df[f\"{i}_Disparity\"] > df[f\"{i}_Disparity\"].shift(1) \n",
    "        df[f\"{i}_Disparity_Move\"] = df[f\"{i}_Disparity_Move\"].replace({True:1,False: 0})\n",
    "        df[f\"{i}_Disparity_s_Move\"] = df[f\"{i}_Disparity_s\"] > df[f\"{i}_Disparity_s\"].shift(1) \n",
    "        df[f\"{i}_Disparity_s_Move\"] = df[f\"{i}_Disparity_s_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_RSI_Move\"] = df[f\"{i}_RSI\"] > df[f\"{i}_RSI\"].shift(1) \n",
    "        df[f\"{i}_RSI_Move\"] = df[f\"{i}_RSI_Move\"].replace({True:1,False: 0})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix = variables_creator(netflix, [\"Wiki\",\"Google\", \"Stock\", \"Nas\", \"Dow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_creator(df):\n",
    "    '''\n",
    "    description: creates the differnt types of target variables based on tomorrow minus today,\n",
    "    '''\n",
    "        \n",
    "    # target 1, Open(t+1) - Close(t)\n",
    "    df[\"target_1\"] = (df[\"Open\"].shift(-1) - df[\"Close\"]) > 0\n",
    "    df[\"target_1\"] = df[\"target_1\"].replace({True:1,False: 0})\n",
    "    # target 2\n",
    "    df[\"target_2\"] = (df[\"Open\"].shift(-1) - df[\"Open\"]) > 0\n",
    "    df[\"target_2\"] = df[\"target_2\"].replace({True:1,False: 0})\n",
    "    # target 3\n",
    "    df[\"target_3\"] = (df[\"Close\"].shift(-1) - df[\"Close\"]) > 0\n",
    "    df[\"target_3\"] = df[\"target_3\"].replace({True:1,False: 0})\n",
    "    # target 4\n",
    "    df[\"target_4\"] = (df[\"Close\"].shift(-1) - df[\"Open\"]) > 0\n",
    "    df[\"target_4\"] = df[\"target_4\"].replace({True:1,False: 0})\n",
    "    # target 5\n",
    "    df[\"target_5\"] = (df[\"Volume\"].shift(-1) - df[\"Volume\"]) > 0\n",
    "    df[\"target_5\"] = df[\"target_5\"].replace({True:1,False: 0})\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\AppData\\Local\\Temp/ipykernel_36404/27534496.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"target_1\"] = (df[\"Open\"].shift(-1) - df[\"Close\"]) > 0\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp/ipykernel_36404/27534496.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"target_2\"] = (df[\"Open\"].shift(-1) - df[\"Open\"]) > 0\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp/ipykernel_36404/27534496.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"target_3\"] = (df[\"Close\"].shift(-1) - df[\"Close\"]) > 0\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp/ipykernel_36404/27534496.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"target_4\"] = (df[\"Close\"].shift(-1) - df[\"Open\"]) > 0\n",
      "C:\\Users\\alexa\\AppData\\Local\\Temp/ipykernel_36404/27534496.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"target_5\"] = (df[\"Volume\"].shift(-1) - df[\"Volume\"]) > 0\n"
     ]
    }
   ],
   "source": [
    "netflix = target_creator(netflix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix.to_csv(\"netflix_full_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was prepared by:\n",
    "\n",
    "Jack Brennen (jpb9088@nyu.edu)\n",
    "\n",
    "Fabrizio Cabrera (fc2250@nyu.edu)\n",
    "\n",
    "Alex Ferrante (af3913@nyu.edu)\n",
    "\n",
    "Graduate School of Arts and Sciences (GSAS) at New York University (NYU)\n",
    "\n",
    "May 2022\n",
    "\n",
    "# Predicting Daily Stock Outcomes - An Application of Machine Learning Binary Classifiers  (can change to something sexier)\n",
    "\n",
    "The ability to predict the movement of a stock was once far fetched, however, thanks to advancements in machine learning techniques, there is reason to\n",
    "believe short-run predictions are possible, within reason. So what makes stock movement prediction tricky? Well, several factors, but one which is most obscure is market noise. With most persons' having access to the internet, news spreads quickly. This means consumers' sentiments are subject to changing quickly, resulting in short-run market fluctuations. Exogenous shocks to the market come in many forms and all of which are reported on. With the rise of internet news platforms these sources server as venues for market particpants to consume stock information which subsequetly informs short-run behavior. \n",
    "\n",
    "Since we believe that capturing trends in agents news consumption is valuable in explaining short-run movements in stocks, this project will include data from Google Trends and Wikipedia API's to serve as indexes to inform the frequency and popularity of stock news. In addition to data collected from Google Trends and Wikipedia API's we also collect general stock data (open, close, volumem ect.). Through the use of random forrests, convolutional neural networks (CNN), and recursive neural networks (RNN) we will contruct a binary-classification task and see how accurately we can model short-run stock movement. While there are an array of targets we could test, the target for the procedding models will be whether the stock closing price will increase tomorrow (1) or whether it will decrease (0). \n",
    "\n",
    "Given constraints in time and computing power, we will collect data from 01/01/2019 to 03/31/2022 for Ford, Netflix, and UBS. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pulling Together the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the saying goes, garbage in, garbage out. Thus, we gave some serious thought on how to best collect our data. This section reviews the necessary steps taken to aquire the stock and news data from their respective API's and the subsequent data cleaning procedures. Given that the web scraping process was computationally expensive, this section will display the code in its raw form. At the end of section two the cleaned and transformed CSV data files will be made available. Your computer will thank us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import pandas as pd\n",
    "import os\n",
    "#%pip install pytrends\n",
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "#pip install pageviewapi\n",
    "import pageviewapi\n",
    "#%pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function used to web scrape the data from Yahoo Finance, Google Trends, and Wikipedia. We will walk through the data collection process and will provide an example using Ford. Note, Netflix and UBS followed an identical processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Big_scraper(kw_list_1, kw_list_2, ticker, start,end):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "    \n",
    "    The function initially grabs historical, indexed, hourly data for when the keyword \n",
    "    was searched most as shown on Google Trends' Interest Over Time section.\n",
    "    It then cleans the data to show daily hits on the keyword in Google news.\n",
    "\n",
    "    \n",
    "    input:\n",
    "    -----\n",
    "    kw_list_1: List of up to 5 key words that will be scraped from google trends for the dates given.\n",
    "             Here, the scraping will pull the total posted items in google news that contains\n",
    "             one of the key words.\n",
    "    \n",
    "    kw_list_2: List of wikipedia article titles (unlimited length) that will pull the amount of\n",
    "            views the article recieved each day. \n",
    "\n",
    "    ticker: the ticker abriviation of the desired stock. Must be netered in as an all capitalized string \n",
    "    example Apple Inc. woud be \"AAPL\"\n",
    "             \n",
    "    start: the start of the desired timeline you want scrape. Date Must be entered in as \"YYYYMMDD\"\n",
    "    \n",
    "    end: the end of the desired timeline you want scrape. Date Must be entered in as \"YYYYMMDD\"\n",
    "             \n",
    "    return:\n",
    "    -------\n",
    "    \n",
    "    combined: a dataframe containing the sum of the daily keyword hits in google news (key words labeled _x),\n",
    "    \n",
    "    data frame cointaing stock info including open, close, high, low prices of the stock,\n",
    "    as well as the stocks daily trading volume and the amount if there was a split or dividend \n",
    "    preformed on the stock that day,\n",
    "    \n",
    "    and the sum of how many times key word wikipedia pages were viewed in a day (key words labeled _y)\n",
    "    \"\"\"\n",
    "    \n",
    "    year_s = int(start[0:4])\n",
    "    month_s = int(start[4:6])\n",
    "    day_s = int(start[6:8])\n",
    "    year_e = int(end[0:4])\n",
    "    month_e = int(end[4:6])\n",
    "    day_e = int(end[6:8])\n",
    "    \n",
    "    starter = pd.to_datetime(f\"'{year_s}-{month_s}-{day_s}'\")\n",
    "    ender = pd.to_datetime(f\"'{year_e}-{month_e}-{day_e}'\")\n",
    "    \n",
    "    \n",
    "    pytrends = TrendReq(hl='en-US', tz=360, retries=10)\n",
    "    jeff = pytrends.get_historical_interest(kw_list_1, \\\n",
    "                                 year_start = year_s, month_start = month_s, day_start = day_s, hour_start = 1, \\\n",
    "                                 year_end = year_e, month_end = month_e, day_end = day_e, hour_end = 23, \\\n",
    "                                 cat = 0, geo = '', gprop = 'news', sleep = 60)\n",
    "    \n",
    "    jeff = jeff.iloc[:, 0:-1] # eliminates the isPartial Column\n",
    "    jeff = jeff.reset_index().drop_duplicates(subset = \"date\") #removing duplicates from the index\n",
    "    jeff = jeff.groupby(pd.Grouper(key=\"date\", freq=\"D\")).mean() # coverts to the mean of daily scores\n",
    "\n",
    "    dow = yf.Ticker(\"^DJI\")\n",
    "    dow_h = dow.history(start=starter, end=ender)\n",
    "    dow_h = pd.DataFrame(dow_h)\n",
    "    dow_names = {\"Open\":\"dow_open\",\"Close\":\"dow_close\",\"Low\": \"dow_low\",\n",
    "    'High':'dow_high','Volume':'dow_vol'}\n",
    "    dow_h=dow_h.rename(dow_names, axis=1).drop([\"Dividends\",\"Stock Splits\"], axis=1)\n",
    "\n",
    "    nas = yf.Ticker(\"^IXIC\")\n",
    "    nas_h = nas.history(start=starter, end=ender)\n",
    "    nas_h = pd.DataFrame(nas_h)\n",
    "    nas_names = {\"Open\":\"nas_open\", \"Close\":\"nas_close\", \"Low\": \"nas_low\",\n",
    "    'High':'nas_high','Volume':'nas_vol'}\n",
    "    nas_h=nas_h.rename(nas_names, axis=1).drop([\"Dividends\",\"Stock Splits\"], axis=1)\n",
    "\n",
    "    market = dow_h.merge(nas_h,left_index=True, right_index=True, how=\"left\")\n",
    "    \n",
    "    tick = yf.Ticker(ticker)\n",
    "    hist = tick.history(start=starter, end=ender)\n",
    "    hist = pd.DataFrame(hist)\n",
    "    \n",
    "    combined = jeff.merge(hist, left_index=True, right_index=True, how=\"left\")\n",
    "    combined = combined.merge(market, left_index=True, right_index=True, how=\"left\")  \n",
    "    \n",
    "    d = pd.DataFrame()\n",
    "    for key_word in kw_list_2:\n",
    "        geoff = pageviewapi.per_article('en.wikipedia', key_word, start, end,\n",
    "                                    access='all-access', agent='all-agents', granularity='daily')\n",
    "        dicty = dict(geoff)\n",
    "        views = pd.DataFrame(dicty[\"items\"])\n",
    "        views[\"timestamp\"] = pd.to_datetime((views[\"timestamp\"]), format=\"%Y%m%d%H\")\n",
    "        views = views.set_index(\"timestamp\")\n",
    "        page = pd.Series(views[\"views\"])\n",
    "        d[key_word] = page\n",
    "        \n",
    "    combined = combined.merge(d, left_index=True, right_index=True, how=\"left\") \n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to collecting data on daily stock performance indicators (i.e Open, close, volume, ect.) the Yahoo Finance API works wonders. With respect to our decision to incorporate news data, the task becomes a little more involved. Within the funtion above you will notice the kw_list_1 and kw_list_2 parameters. These are necessary requisits for the Google News and Wikipedia API's. The keywords selected in kw_list_1 were arbitrarily determined, but choosen in the context of capturing relevant stock news. The keywords in kw_list_2 were also a bit arbitrary, however, in order for the Wikipedia API to work, the list of strings must correspond to exisiting Wikipedia webpages. Below are the keywords lists implemented for Ford. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list_1 = [\"Ford\", \"F-150\", \"Ford Bronco\", \"Ford Mustang\", \"Ford Stock\"]\n",
    "kw_list_2 = [\"Ford Motor Company\", \"Ford Mustang\", \"Ford F Series\", \"Ford Bronco\", \"Lincoln Navigator\", \"Lincoln Aviator\", \"Ford GT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the key words were determined, we then implemented the big_scraper function and web scraped data for the specified time-frames. The scraper was ran four times, creating seperate data files for each year in the sake of cutting down run times and ensuring safer data pulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate each of the annual datafiles to create one main\n",
    "Ford_19 = Big_scraper(kw_list_1, kw_list_2,\"F\", \"20190101\", \"20191231\")\n",
    "Ford_20 = Big_scraper(kw_list_1, kw_list_2,\"F\", \"20200101\", \"20201231\")\n",
    "Ford_21 = Big_scraper(kw_list_1, kw_list_2,\"F\", \"20210101\", \"20211231\")\n",
    "Ford_22 = Big_scraper(kw_list_1, kw_list_2,\"F\", \"20220101\", \"20220331\")\n",
    "\n",
    "ford = pd.concat([Ford_19, Ford_20, Ford_21, Ford_22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since stocks are traded only during weekdays and news data is reported daily, we decided to drop data collected for weekends and holidays. While this reduces the number of news observations, it was a step taken to reduce the amont of NaN values in our data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the date column back to datetime \n",
    "Ford.date = pd.to_datetime(Ford.date)\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "# create dateframe 2019-01-01 to 2022-03-31 which excludes weekends and holidays\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "dates = pd.date_range(start='2019-01-01', end='2022-03-31', freq=us_bd)\n",
    "dates = pd.DataFrame(dates)\n",
    "dates = dates.rename(columns={0: \"date\"})\n",
    "\n",
    "# Merge on new date range to remove weekends and holidays in ford dataset\n",
    "Ford = dates.merge(Ford, on=\"date\", how=\"left\")\n",
    "Ford = Ford.set_index(\"date\")\n",
    "Ford.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Processing - Feature and Target Creation\n",
    "\n",
    "In this next phase of data processing we focus on using the existing data pulled to generate additional features and targets. From the Google News and Wikipedia data collected, we sum across each of the keyword list results to create two new features noted as \"Wiki_total\" and \"Google_total\", respectively. These new \"total\" features will then be used as denominator values and assist in creating more features which with the hope of providing greater explanantory power to our future models.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating base varibles to be used in variable creator functions\n",
    "Ford[\"Wiki_total\"] = (Ford[\"Ford Motor Company\"] + \n",
    "    Ford[\"Ford Mustang_y\"] + Ford[\"Ford F Series\"] + \n",
    "    Ford[\"Ford Bronco_y\"] + Ford[\"Lincoln Navigator\"] + \n",
    "    Ford[\"Lincoln Aviator\"] + Ford[\"Ford GT\"])\n",
    "\n",
    "Ford[\"Google_total\"] = (Ford[\"Ford\"] +\n",
    "    Ford[\"F-150\"] + Ford[\"Ford Bronco_x\"] +\n",
    "    Ford[\"Ford Mustang_x\"] + Ford[\"Ford Stock\"])\n",
    "\n",
    "Ford[\"Stock_total\"] = Ford[\"Close\"]\n",
    "Ford[\"Nas_total\"] = Ford[\"nas_close\"]\n",
    "Ford[\"Dow_total\"] = Ford[\"dow_close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_creator(df, variable_list, w=7):\n",
    "    \n",
    "    '''\n",
    "    descrition:\n",
    "    -----------\n",
    "    takes dataframe and returns new varibles based on recommmended calcualtions, \n",
    "    it should be done seporately with internet based and stock based dataframes\n",
    "\n",
    "    Note: When using for aggreated varaibles, for example Wiki_total, the sum of all the wiki pages daily page view counts, \n",
    "    you MUST calculate Wiki_total in the dataframe seperately BEFORE this function can be used. \n",
    "\n",
    "    For example if you had wiki page counts for Ford Bronco and Ford Ranger, Wiki_total would equal Ford Bronco + Ford ranger counts, \n",
    "    AGAIN Wiki_total must be calculated in the desired dateframe before using this function \n",
    "\n",
    "    input:\n",
    "    ------\n",
    "    df: dataframe containing the google trends, yahoo finance, and or wikipedia page count data\n",
    "\n",
    "    variable_list: list of strings to be added to the equations to calculate the new varaibles. \n",
    "    ex. insertting the string \"Wiki\" will add to df[f\"{}_total\"] to become \"Wiki_total\"\n",
    "\n",
    "    w: the window length for one period shift. Default is 7 providing 7 day moving averages for wiki and google data, \n",
    "        FOR STOCK DATA THIS WILL NEED TO BE CHANGED TO 5.\n",
    "\n",
    "    output:\n",
    "    -------\n",
    "    df: the same dataframe as was inputted but now containing variables for \n",
    "    Momemtum, Disparity, Moving Average, Exponential Moving Aerage, Rator Change, and RSI index score.\n",
    "    Also containg are moving variables, which are boolean with 1 indicating an increase in the above variables\n",
    "    '''   \n",
    "    \n",
    "    for i in variable_list:\n",
    "        # Momentum_1\n",
    "        df[f\"{i}_Moment_1\"] =  (df[f\"{i}_total\"] / df[f\"{i}_total\"].shift(w)) * 100\n",
    "        # Momentum_2\n",
    "        df[f\"{i}_Moment_2\"] =  (df[f\"{i}_total\"] - df[f\"{i}_total\"].shift(w)) * 100\n",
    "        # Momentum_1_s three day shift (instead of w)\n",
    "        df[f\"{i}_Moment_1_s\"] =  (df[f\"{i}_total\"] / df[f\"{i}_total\"].shift(3)) * 100\n",
    "        # Momentum_2_s\n",
    "        df[f\"{i}_Moment_2_s\"] =  (df[f\"{i}_total\"] - df[f\"{i}_total\"].shift(3)) * 100\n",
    "        # Moving average\n",
    "        df[f\"{i}_MAvg\"] = df[f\"{i}_total\"].rolling(f\"{w}d\").mean()\n",
    "        # Moving average 3 day\n",
    "        df[f\"{i}_MAvg_s\"] = df[f\"{i}_total\"].rolling(\"3d\").mean()\n",
    "        # Disparity\n",
    "        df[f\"{i}_Disparity\"] = (df[f\"{i}_total\"]/df[f\"{i}_MAvg\"]) * 100\n",
    "        # Disparity 3 day\n",
    "        df[f\"{i}_Disparity_s\"] = (df[f\"{i}_total\"]/df[f\"{i}_MAvg_s\"]) * 100\n",
    "        # Rate of Change Normal Way\n",
    "        df[f\"{i}_ROC\"] = (df[f\"{i}_total\"]-df[f\"{i}_total\"].shift(w))/(df[f\"{i}_total\"].shift(w)) *100\n",
    "        df[f\"{i}_ROC_s\"] = (df[f\"{i}_total\"]-df[f\"{i}_total\"].shift(3))/(df[f\"{i}_total\"].shift(3)) *100\n",
    "        #Rate of Change Paper Way (doesn't make sense but just in case)\n",
    "        df[f'{i}_Rocp'] = (df[f\"{i}_total\"]/df[f\"{i}_Moment_2\"]) *100\n",
    "        # Exponential Moving Average\n",
    "        df[f\"{i}_EMA\"] = (df[f\"{i}_total\"]-df[f\"{i}_MAvg\"].shift(1))*(2/(w+1))+df[f\"{i}_MAvg\"].shift(1)\n",
    "\n",
    "        # calculating the Relative Strength Index, based on 14 day window\n",
    "        df[f\"{i}_diff\"] = df[f\"{i}_total\"].diff(1)\n",
    "        df[f\"{i}_gain\"] = df[f\"{i}_diff\"].clip(lower=0).round(2) #keeps all values above or below a given threshold, lower=lower bound\n",
    "        df[f\"{i}_loss\"] = df[f\"{i}_diff\"].clip(upper=0).round(2)\n",
    "        df[f'{i}_avg_gain'] = df[f'{i}_gain'].rolling(14).mean()\n",
    "        df[f'{i}_avg_loss'] = df[f'{i}_loss'].rolling(14).mean()\n",
    "        df[f'{i}_rs'] = df[f'{i}_avg_gain'] / df[f'{i}_avg_loss']\n",
    "        df[f'{i}_RSI'] = 100 - (100 / (1.0 + df[f'{i}_rs']))\n",
    "\n",
    "        # Calculatiing the Move Variables \n",
    "        df[f\"{i}_Move\"] = df[f\"{i}_total\"] > df[f\"{i}_total\"].shift(1) \n",
    "        df[f\"{i}_Move\"] = df[f\"{i}_Move\"].replace({True:1,False: 0})\n",
    "    \n",
    "      \n",
    "        df[f\"{i}_MAvg_Move\"] = df[f\"{i}_MAvg\"] > df[f\"{i}_MAvg\"].shift(1) \n",
    "        df[f\"{i}_MAvg_Move\"] = df[f\"{i}_MAvg_Move\"].replace({True:1,False: 0})\n",
    "        df[f\"{i}_MAvg_s_Move\"] = df[f\"{i}_MAvg_s\"] > df[f\"{i}_MAvg_s\"].shift(1) \n",
    "        df[f\"{i}_MAvg_s_Move\"] = df[f\"{i}_MAvg_s_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_EMA_Move\"] = df[f\"{i}_EMA\"] > df[f\"{i}_EMA\"].shift(1) \n",
    "        df[f\"{i}_EMA_Move\"] = df[f\"{i}_EMA_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_Disparity_Move\"] = df[f\"{i}_Disparity\"] > df[f\"{i}_Disparity\"].shift(1) \n",
    "        df[f\"{i}_Disparity_Move\"] = df[f\"{i}_Disparity_Move\"].replace({True:1,False: 0})\n",
    "        df[f\"{i}_Disparity_s_Move\"] = df[f\"{i}_Disparity_s\"] > df[f\"{i}_Disparity_s\"].shift(1) \n",
    "        df[f\"{i}_Disparity_s_Move\"] = df[f\"{i}_Disparity_s_Move\"].replace({True:1,False: 0})\n",
    "\n",
    "        df[f\"{i}_RSI_Move\"] = df[f\"{i}_RSI\"] > df[f\"{i}_RSI\"].shift(1) \n",
    "        df[f\"{i}_RSI_Move\"] = df[f\"{i}_RSI_Move\"].replace({True:1,False: 0})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ford = variables_creator(Ford, [\"Wiki\",\"Google\", \"Stock\", \"Nas\", \"Dow\"], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a comprehesive feature matrix of data, we turn to generating the targets. As mentioned, there are a variety of targets that could be tested to predict stock movement. Below we have generated five binary targets, each either related to the stock close price, open price, or volumne. For the purposes of the models developed in this project, we will focus on trying to accurately predict \"target_3\" - the difference in closing price. This target's result is binary where an increase in the closing price returns a value of 1 and a decrease in the closing price returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_creator(df):\n",
    "    '''\n",
    "    description: creates the differnt types of target variables based on tomorrow minus today,\n",
    "    '''\n",
    "        \n",
    "    # target 1, Open(t+1) - Close(t)\n",
    "    df[\"target_1\"] = (df[\"Open\"].shift(-1) - df[\"Close\"]) > 0\n",
    "    df[\"target_1\"] = df[\"target_1\"].replace({True:1,False: 0})\n",
    "    # target 2\n",
    "    df[\"target_2\"] = (df[\"Open\"].shift(-1) - df[\"Open\"]) > 0\n",
    "    df[\"target_2\"] = df[\"target_2\"].replace({True:1,False: 0})\n",
    "    # target 3\n",
    "    df[\"target_3\"] = (df[\"Close\"].shift(-1) - df[\"Close\"]) > 0\n",
    "    df[\"target_3\"] = df[\"target_3\"].replace({True:1,False: 0})\n",
    "    # target 4\n",
    "    df[\"target_4\"] = (df[\"Close\"].shift(-1) - df[\"Open\"]) > 0\n",
    "    df[\"target_4\"] = df[\"target_4\"].replace({True:1,False: 0})\n",
    "    # target 5\n",
    "    df[\"target_5\"] = (df[\"Volume\"].shift(-1) - df[\"Volume\"]) > 0\n",
    "    df[\"target_5\"] = df[\"target_5\"].replace({True:1,False: 0})\n",
    " \n",
    "    return df\n",
    "\n",
    "Ford = target_creator(Ford)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we've made it to a stage where out data set includes many features and targets to test them on. \n",
    "\n",
    "Below are the fully cleaned and model ready CSV files for Ford, Netflix, and UBS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ford, Neflix, and UBS data\n",
    "# Clarify with Fabrizio and Jack which are their final data files \n",
    "\n",
    "Ford = pd.read_csv(\"Ford_Cleaned_Date.csv\")\n",
    "netflix = pd. read_csv(\"Netflix_Cleaned_Data.csv\")\n",
    "ubs = pd.read_csv(\"UBS_Cleaned_Date.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
